{"docs/conf.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\nfrom importlib.metadata import version as get_version\n\nfrom packaging.version import parse\n\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx_autodoc_typehints\",\n]\n\ntemplates_path = [\"_templates\"]\nsource_suffix = \".rst\"\nmaster_doc = \"index\"\nproject = \"AnyIO\"\nauthor = \"Alex Gr\u00f6nholm\"\ncopyright = \"2018, \" + author\n\nv = parse(get_version(\"anyio\"))\nversion = v.base_version\nrelease = v.public\n\nlanguage = \"en\"\n\nexclude_patterns = [\"_build\"]\npygments_style = \"sphinx\"\nautodoc_default_options = {\"members\": True, \"show-inheritance\": True}\nautodoc_mock_imports = [\"_typeshed\"]\ntodo_include_todos = False\n\nhtml_theme = \"sphinx_rtd_theme\"\nhtmlhelp_basename = \"anyiodoc\"\n\nintersphinx_mapping = {\"python\": (\"https://docs.python.org/3/\", None)}\n", "tests/test_to_thread.py": "from __future__ import annotations\n\nimport asyncio\nimport threading\nimport time\nfrom concurrent.futures import Future, ThreadPoolExecutor\nfrom contextvars import ContextVar\nfrom functools import partial\nfrom typing import Any, NoReturn\n\nimport pytest\nimport sniffio\n\nimport anyio.to_thread\nfrom anyio import (\n    CapacityLimiter,\n    Event,\n    create_task_group,\n    from_thread,\n    sleep,\n    to_thread,\n    wait_all_tasks_blocked,\n)\nfrom anyio.from_thread import BlockingPortalProvider\n\npytestmark = pytest.mark.anyio\n\n\nasync def test_run_in_thread_cancelled() -> None:\n    state = 0\n\n    def thread_worker() -> None:\n        nonlocal state\n        state = 2\n\n    async def worker() -> None:\n        nonlocal state\n        state = 1\n        await to_thread.run_sync(thread_worker)\n        state = 3\n\n    async with create_task_group() as tg:\n        tg.start_soon(worker)\n        tg.cancel_scope.cancel()\n\n    assert state == 1\n\n\nasync def test_run_in_thread_exception() -> None:\n    def thread_worker() -> NoReturn:\n        raise ValueError(\"foo\")\n\n    with pytest.raises(ValueError) as exc:\n        await to_thread.run_sync(thread_worker)\n\n    exc.match(\"^foo$\")\n\n\nasync def test_run_in_custom_limiter() -> None:\n    max_active_threads = 0\n\n    def thread_worker() -> None:\n        nonlocal max_active_threads\n        active_threads.add(threading.current_thread())\n        max_active_threads = max(max_active_threads, len(active_threads))\n        event.wait(1)\n        active_threads.remove(threading.current_thread())\n\n    async def task_worker() -> None:\n        await to_thread.run_sync(thread_worker, limiter=limiter)\n\n    event = threading.Event()\n    limiter = CapacityLimiter(3)\n    active_threads: set[threading.Thread] = set()\n    async with create_task_group() as tg:\n        for _ in range(4):\n            tg.start_soon(task_worker)\n\n        await sleep(0.1)\n        assert len(active_threads) == 3\n        assert limiter.borrowed_tokens == 3\n        event.set()\n\n    assert len(active_threads) == 0\n    assert max_active_threads == 3\n\n\n@pytest.mark.parametrize(\n    \"abandon_on_cancel, expected_last_active\",\n    [\n        pytest.param(False, \"task\", id=\"noabandon\"),\n        pytest.param(True, \"thread\", id=\"abandon\"),\n    ],\n)\nasync def test_cancel_worker_thread(\n    abandon_on_cancel: bool, expected_last_active: str\n) -> None:\n    \"\"\"\n    Test that when a task running a worker thread is cancelled, the cancellation is not\n    acted on until the thread finishes.\n\n    \"\"\"\n    last_active: str | None = None\n\n    def thread_worker() -> None:\n        nonlocal last_active\n        from_thread.run_sync(sleep_event.set)\n        time.sleep(0.2)\n        last_active = \"thread\"\n        from_thread.run_sync(finish_event.set)\n\n    async def task_worker() -> None:\n        nonlocal last_active\n        try:\n            await to_thread.run_sync(thread_worker, abandon_on_cancel=abandon_on_cancel)\n        finally:\n            last_active = \"task\"\n\n    sleep_event = Event()\n    finish_event = Event()\n    async with create_task_group() as tg:\n        tg.start_soon(task_worker)\n        await sleep_event.wait()\n        tg.cancel_scope.cancel()\n\n    await finish_event.wait()\n    assert last_active == expected_last_active\n\n\nasync def test_cancel_wait_on_thread() -> None:\n    event = threading.Event()\n    future: Future[bool] = Future()\n\n    def wait_event() -> None:\n        future.set_result(event.wait(1))\n\n    async with create_task_group() as tg:\n        tg.start_soon(partial(to_thread.run_sync, abandon_on_cancel=True), wait_event)\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    await to_thread.run_sync(event.set)\n    assert future.result(1)\n\n\nasync def test_deprecated_cancellable_param() -> None:\n    with pytest.warns(DeprecationWarning, match=\"The `cancellable=`\"):\n        await to_thread.run_sync(bool, cancellable=True)\n\n\nasync def test_contextvar_propagation() -> None:\n    var = ContextVar(\"var\", default=1)\n    var.set(6)\n    assert await to_thread.run_sync(var.get) == 6\n\n\nasync def test_asynclib_detection() -> None:\n    with pytest.raises(sniffio.AsyncLibraryNotFoundError):\n        await to_thread.run_sync(sniffio.current_async_library)\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_asyncio_cancel_native_task() -> None:\n    task: asyncio.Task[None] | None = None\n\n    async def run_in_thread() -> None:\n        nonlocal task\n        task = asyncio.current_task()\n        await to_thread.run_sync(time.sleep, 0.2, abandon_on_cancel=True)\n\n    async with create_task_group() as tg:\n        tg.start_soon(run_in_thread)\n        await wait_all_tasks_blocked()\n        assert task is not None\n        task.cancel()\n\n\ndef test_asyncio_no_root_task(asyncio_event_loop: asyncio.AbstractEventLoop) -> None:\n    \"\"\"\n    Regression test for #264.\n\n    Ensures that to_thread.run_sync() does not raise an error when there is no root\n    task, but instead tries to find the top most parent task by traversing the cancel\n    scope tree, or failing that, uses the current task to set up a shutdown callback.\n\n    \"\"\"\n\n    async def run_in_thread() -> None:\n        try:\n            await to_thread.run_sync(time.sleep, 0)\n        finally:\n            asyncio_event_loop.call_soon(asyncio_event_loop.stop)\n\n    task = asyncio_event_loop.create_task(run_in_thread())\n    asyncio_event_loop.run_forever()\n    task.result()\n\n    # Wait for worker threads to exit\n    for t in threading.enumerate():\n        if t.name == \"AnyIO worker thread\":\n            t.join(2)\n            assert not t.is_alive()\n\n\ndef test_asyncio_future_callback_partial(\n    asyncio_event_loop: asyncio.AbstractEventLoop,\n) -> None:\n    \"\"\"\n    Regression test for #272.\n\n    Ensures that futures with partial callbacks are handled correctly when the root task\n    cannot be determined.\n    \"\"\"\n\n    def func(future: object) -> None:\n        pass\n\n    async def sleep_sync() -> None:\n        return await to_thread.run_sync(time.sleep, 0)\n\n    task = asyncio_event_loop.create_task(sleep_sync())\n    task.add_done_callback(partial(func))\n    asyncio_event_loop.run_until_complete(task)\n\n\ndef test_asyncio_run_sync_no_asyncio_run(\n    asyncio_event_loop: asyncio.AbstractEventLoop,\n) -> None:\n    \"\"\"Test that the thread pool shutdown callback does not raise an exception.\"\"\"\n\n    def exception_handler(loop: object, context: Any = None) -> None:\n        exceptions.append(context[\"exception\"])\n\n    exceptions: list[BaseException] = []\n    asyncio_event_loop.set_exception_handler(exception_handler)\n    asyncio_event_loop.run_until_complete(to_thread.run_sync(time.sleep, 0))\n    assert not exceptions\n\n\ndef test_asyncio_run_sync_multiple(\n    asyncio_event_loop: asyncio.AbstractEventLoop,\n) -> None:\n    \"\"\"Regression test for #304.\"\"\"\n    asyncio_event_loop.call_later(0.5, asyncio_event_loop.stop)\n    for _ in range(3):\n        asyncio_event_loop.run_until_complete(to_thread.run_sync(time.sleep, 0))\n\n    for t in threading.enumerate():\n        if t.name == \"AnyIO worker thread\":\n            t.join(2)\n            assert not t.is_alive()\n\n\ndef test_asyncio_no_recycle_stopping_worker(\n    asyncio_event_loop: asyncio.AbstractEventLoop,\n) -> None:\n    \"\"\"Regression test for #323.\"\"\"\n\n    async def taskfunc1() -> None:\n        await anyio.to_thread.run_sync(time.sleep, 0)\n        event1.set()\n        await event2.wait()\n\n    async def taskfunc2() -> None:\n        await event1.wait()\n        asyncio_event_loop.call_soon(event2.set)\n        await anyio.to_thread.run_sync(time.sleep, 0)\n        # At this point, the worker would be stopped but still in the idle workers pool,\n        # so the following would hang prior to the fix\n        await anyio.to_thread.run_sync(time.sleep, 0)\n\n    event1 = asyncio.Event()\n    event2 = asyncio.Event()\n    task1 = asyncio_event_loop.create_task(taskfunc1())\n    task2 = asyncio_event_loop.create_task(taskfunc2())\n    asyncio_event_loop.run_until_complete(asyncio.gather(task1, task2))\n\n\nasync def test_stopiteration() -> None:\n    \"\"\"\n    Test that raising StopIteration in a worker thread raises a RuntimeError on the\n    caller.\n\n    \"\"\"\n\n    def raise_stopiteration() -> NoReturn:\n        raise StopIteration\n\n    with pytest.raises(RuntimeError, match=\"coroutine raised StopIteration\"):\n        await to_thread.run_sync(raise_stopiteration)\n\n\nclass TestBlockingPortalProvider:\n    @pytest.fixture\n    def provider(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> BlockingPortalProvider:\n        return BlockingPortalProvider(\n            backend=anyio_backend_name, backend_options=anyio_backend_options\n        )\n\n    def test_single_thread(\n        self, provider: BlockingPortalProvider, anyio_backend_name: str\n    ) -> None:\n        threads: set[threading.Thread] = set()\n\n        async def check_thread() -> None:\n            assert sniffio.current_async_library() == anyio_backend_name\n            threads.add(threading.current_thread())\n\n        active_threads_before = threading.active_count()\n        for _ in range(3):\n            with provider as portal:\n                portal.call(check_thread)\n\n        assert len(threads) == 3\n        assert threading.active_count() == active_threads_before\n\n    def test_single_thread_overlapping(\n        self, provider: BlockingPortalProvider, anyio_backend_name: str\n    ) -> None:\n        threads: set[threading.Thread] = set()\n\n        async def check_thread() -> None:\n            assert sniffio.current_async_library() == anyio_backend_name\n            threads.add(threading.current_thread())\n\n        with provider as portal1:\n            with provider as portal2:\n                assert portal1 is portal2\n                portal2.call(check_thread)\n\n            portal1.call(check_thread)\n\n        assert len(threads) == 1\n\n    def test_multiple_threads(\n        self, provider: BlockingPortalProvider, anyio_backend_name: str\n    ) -> None:\n        threads: set[threading.Thread] = set()\n        event = Event()\n\n        async def check_thread() -> None:\n            assert sniffio.current_async_library() == anyio_backend_name\n            await event.wait()\n            threads.add(threading.current_thread())\n\n        def dummy() -> None:\n            with provider as portal:\n                portal.call(check_thread)\n\n        with ThreadPoolExecutor(max_workers=3) as pool:\n            for _ in range(3):\n                pool.submit(dummy)\n\n            with provider as portal:\n                portal.call(wait_all_tasks_blocked)\n                portal.call(event.set)\n\n        assert len(threads) == 1\n", "tests/test_signals.py": "from __future__ import annotations\n\nimport os\nimport signal\nimport sys\nfrom typing import AsyncIterable\n\nimport pytest\n\nfrom anyio import create_task_group, fail_after, open_signal_receiver, to_thread\n\npytestmark = [\n    pytest.mark.anyio,\n    pytest.mark.skipif(\n        sys.platform == \"win32\",\n        reason=\"Signal delivery cannot be tested on Windows\",\n    ),\n]\n\n\nasync def test_receive_signals() -> None:\n    with open_signal_receiver(signal.SIGUSR1, signal.SIGUSR2) as sigiter:\n        await to_thread.run_sync(os.kill, os.getpid(), signal.SIGUSR1)\n        await to_thread.run_sync(os.kill, os.getpid(), signal.SIGUSR2)\n        with fail_after(1):\n            sigusr1 = await sigiter.__anext__()\n            assert isinstance(sigusr1, signal.Signals)\n            assert sigusr1 == signal.Signals.SIGUSR1\n\n            sigusr2 = await sigiter.__anext__()\n            assert isinstance(sigusr2, signal.Signals)\n            assert sigusr2 == signal.Signals.SIGUSR2\n\n\nasync def test_task_group_cancellation_open() -> None:\n    async def signal_handler() -> None:\n        with open_signal_receiver(signal.SIGUSR1) as sigiter:\n            async for v in sigiter:\n                pytest.fail(\"SIGUSR1 should not be sent\")\n\n            pytest.fail(\"signal_handler should have been cancelled\")\n\n        pytest.fail(\"open_signal_receiver should not suppress cancellation\")\n\n    async with create_task_group() as tg:\n        tg.start_soon(signal_handler)\n        tg.cancel_scope.cancel()\n\n\nasync def test_task_group_cancellation_consume() -> None:\n    async def consume(sigiter: AsyncIterable[int]) -> None:\n        async for v in sigiter:\n            pytest.fail(\"SIGUSR1 should not be sent\")\n\n        pytest.fail(\"consume should have been cancelled\")\n\n    with open_signal_receiver(signal.SIGUSR1) as sigiter:\n        async with create_task_group() as tg:\n            tg.start_soon(consume, sigiter)\n            tg.cancel_scope.cancel()\n", "tests/test_sockets.py": "from __future__ import annotations\n\nimport array\nimport gc\nimport io\nimport os\nimport platform\nimport socket\nimport sys\nimport tempfile\nimport threading\nimport time\nfrom contextlib import suppress\nfrom pathlib import Path\nfrom socket import AddressFamily\nfrom ssl import SSLContext, SSLError\nfrom threading import Thread\nfrom typing import Any, Generator, Iterable, Iterator, NoReturn, TypeVar, cast\n\nimport psutil\nimport pytest\nfrom _pytest.fixtures import SubRequest\nfrom _pytest.logging import LogCaptureFixture\nfrom _pytest.monkeypatch import MonkeyPatch\nfrom _pytest.tmpdir import TempPathFactory\n\nfrom anyio import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    Event,\n    TypedAttributeLookupError,\n    connect_tcp,\n    connect_unix,\n    create_connected_udp_socket,\n    create_connected_unix_datagram_socket,\n    create_task_group,\n    create_tcp_listener,\n    create_udp_socket,\n    create_unix_datagram_socket,\n    create_unix_listener,\n    fail_after,\n    getaddrinfo,\n    getnameinfo,\n    move_on_after,\n    sleep,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import (\n    IPSockAddrType,\n    Listener,\n    SocketAttribute,\n    SocketListener,\n    SocketStream,\n)\nfrom anyio.streams.stapled import MultiListener\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import ExceptionGroup\n\nfrom typing import Literal\n\nAnyIPAddressFamily = Literal[\n    AddressFamily.AF_UNSPEC, AddressFamily.AF_INET, AddressFamily.AF_INET6\n]\n\npytestmark = pytest.mark.anyio\n\n# If a socket can bind to ::1, the current environment has IPv6 properly configured\nhas_ipv6 = False\nif socket.has_ipv6:\n    try:\n        s = socket.socket(AddressFamily.AF_INET6)\n        try:\n            s.bind((\"::1\", 0))\n        finally:\n            s.close()\n            del s\n    except OSError:\n        pass\n    else:\n        has_ipv6 = True\n\nskip_ipv6_mark = pytest.mark.skipif(not has_ipv6, reason=\"IPv6 is not available\")\n\n\n@pytest.fixture\ndef fake_localhost_dns(monkeypatch: MonkeyPatch) -> None:\n    def fake_getaddrinfo(*args: Any, **kwargs: Any) -> object:\n        # Make it return IPv4 addresses first so we can test the IPv6 preference\n        results = real_getaddrinfo(*args, **kwargs)\n        return sorted(results, key=lambda item: item[0])\n\n    real_getaddrinfo = socket.getaddrinfo\n    monkeypatch.setattr(\"socket.getaddrinfo\", fake_getaddrinfo)\n\n\n@pytest.fixture(\n    params=[\n        pytest.param(AddressFamily.AF_INET, id=\"ipv4\"),\n        pytest.param(AddressFamily.AF_INET6, id=\"ipv6\", marks=[skip_ipv6_mark]),\n    ]\n)\ndef family(request: SubRequest) -> AnyIPAddressFamily:\n    return request.param\n\n\n@pytest.fixture\ndef check_asyncio_bug(anyio_backend_name: str, family: AnyIPAddressFamily) -> None:\n    if (\n        anyio_backend_name == \"asyncio\"\n        and sys.platform == \"win32\"\n        and family == AddressFamily.AF_INET6\n    ):\n        import asyncio\n\n        policy = asyncio.get_event_loop_policy()\n        if policy.__class__.__name__ == \"WindowsProactorEventLoopPolicy\":\n            pytest.skip(\"Does not work due to a known bug (39148)\")\n\n\n_T = TypeVar(\"_T\")\n\n\ndef _identity(v: _T) -> _T:\n    return v\n\n\n#  _ProactorBasePipeTransport.abort() after _ProactorBasePipeTransport.close()\n# does not cancel writes: https://bugs.python.org/issue44428\n_ignore_win32_resource_warnings = (\n    pytest.mark.filterwarnings(\n        \"ignore:unclosed <socket.socket:ResourceWarning\",\n        \"ignore:unclosed transport <_ProactorSocketTransport closing:ResourceWarning\",\n    )\n    if sys.platform == \"win32\"\n    else _identity\n)\n\n\n@_ignore_win32_resource_warnings  # type: ignore[operator]\nclass TestTCPStream:\n    @pytest.fixture\n    def server_sock(self, family: AnyIPAddressFamily) -> Iterator[socket.socket]:\n        sock = socket.socket(family, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        sock.bind((\"localhost\", 0))\n        sock.listen()\n        yield sock\n        sock.close()\n\n    @pytest.fixture\n    def server_addr(self, server_sock: socket.socket) -> tuple[str, int]:\n        return server_sock.getsockname()[:2]\n\n    async def test_extra_attributes(\n        self,\n        server_sock: socket.socket,\n        server_addr: tuple[str, int],\n        family: AnyIPAddressFamily,\n    ) -> None:\n        async with await connect_tcp(*server_addr) as stream:\n            raw_socket = stream.extra(SocketAttribute.raw_socket)\n            assert stream.extra(SocketAttribute.family) == family\n            assert (\n                stream.extra(SocketAttribute.local_address)\n                == raw_socket.getsockname()[:2]\n            )\n            assert (\n                stream.extra(SocketAttribute.local_port) == raw_socket.getsockname()[1]\n            )\n            assert stream.extra(SocketAttribute.remote_address) == server_addr\n            assert stream.extra(SocketAttribute.remote_port) == server_addr[1]\n\n    async def test_send_receive(\n        self, server_sock: socket.socket, server_addr: tuple[str, int]\n    ) -> None:\n        async with await connect_tcp(*server_addr) as stream:\n            client, _ = server_sock.accept()\n            await stream.send(b\"blah\")\n            request = client.recv(100)\n            client.sendall(request[::-1])\n            response = await stream.receive()\n            client.close()\n\n        assert response == b\"halb\"\n\n    async def test_send_large_buffer(\n        self, server_sock: socket.socket, server_addr: tuple[str, int]\n    ) -> None:\n        def serve() -> None:\n            client, _ = server_sock.accept()\n            client.sendall(buffer)\n            client.close()\n\n        buffer = (\n            b\"\\xff\" * 1024 * 1024\n        )  # should exceed the maximum kernel send buffer size\n        async with await connect_tcp(*server_addr) as stream:\n            thread = Thread(target=serve, daemon=True)\n            thread.start()\n            response = b\"\"\n            while len(response) < len(buffer):\n                response += await stream.receive()\n\n        thread.join()\n        assert response == buffer\n\n    async def test_send_eof(\n        self, server_sock: socket.socket, server_addr: tuple[str, int]\n    ) -> None:\n        def serve() -> None:\n            client, _ = server_sock.accept()\n            request = b\"\"\n            while True:\n                data = client.recv(100)\n                request += data\n                if not data:\n                    break\n\n            client.sendall(request[::-1])\n            client.close()\n\n        async with await connect_tcp(*server_addr) as stream:\n            thread = Thread(target=serve, daemon=True)\n            thread.start()\n            await stream.send(b\"hello, \")\n            await stream.send(b\"world\\n\")\n            await stream.send_eof()\n            response = await stream.receive()\n\n        thread.join()\n        assert response == b\"\\ndlrow ,olleh\"\n\n    async def test_iterate(\n        self, server_sock: socket.socket, server_addr: tuple[str, int]\n    ) -> None:\n        def serve() -> None:\n            client, _ = server_sock.accept()\n            client.sendall(b\"bl\")\n            event.wait(1)\n            client.sendall(b\"ah\")\n            client.close()\n\n        event = threading.Event()\n        thread = Thread(target=serve, daemon=True)\n        thread.start()\n        chunks = []\n        async with await connect_tcp(*server_addr) as stream:\n            async for chunk in stream:\n                chunks.append(chunk)\n                event.set()\n\n        thread.join()\n        assert chunks == [b\"bl\", b\"ah\"]\n\n    async def test_socket_options(\n        self, family: AnyIPAddressFamily, server_addr: tuple[str, int]\n    ) -> None:\n        async with await connect_tcp(*server_addr) as stream:\n            raw_socket = stream.extra(SocketAttribute.raw_socket)\n            assert raw_socket.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) != 0\n\n    @skip_ipv6_mark\n    @pytest.mark.parametrize(\n        \"local_addr, expected_client_addr\",\n        [\n            pytest.param(\"\", \"::1\", id=\"dualstack\"),\n            pytest.param(\"127.0.0.1\", \"127.0.0.1\", id=\"ipv4\"),\n            pytest.param(\"::1\", \"::1\", id=\"ipv6\"),\n        ],\n    )\n    async def test_happy_eyeballs(\n        self, local_addr: str, expected_client_addr: str, fake_localhost_dns: None\n    ) -> None:\n        client_addr = None, None\n\n        def serve() -> None:\n            nonlocal client_addr\n            client, client_addr = server_sock.accept()\n            client.close()\n\n        family = (\n            AddressFamily.AF_INET\n            if local_addr == \"127.0.0.1\"\n            else AddressFamily.AF_INET6\n        )\n        server_sock = socket.socket(family)\n        server_sock.bind((local_addr, 0))\n        server_sock.listen()\n        port = server_sock.getsockname()[1]\n        thread = Thread(target=serve, daemon=True)\n        thread.start()\n\n        async with await connect_tcp(\"localhost\", port):\n            pass\n\n        thread.join()\n        server_sock.close()\n        assert client_addr[0] == expected_client_addr\n\n    @pytest.mark.parametrize(\n        \"target, exception_class\",\n        [\n            pytest.param(\n                \"localhost\", ExceptionGroup, id=\"multi\", marks=[skip_ipv6_mark]\n            ),\n            pytest.param(\"127.0.0.1\", ConnectionRefusedError, id=\"single\"),\n        ],\n    )\n    async def test_connection_refused(\n        self,\n        target: str,\n        exception_class: type[ExceptionGroup] | type[ConnectionRefusedError],\n        fake_localhost_dns: None,\n    ) -> None:\n        dummy_socket = socket.socket(AddressFamily.AF_INET6)\n        dummy_socket.bind((\"::\", 0))\n        free_port = dummy_socket.getsockname()[1]\n        dummy_socket.close()\n\n        with pytest.raises(OSError) as exc:\n            await connect_tcp(target, free_port)\n\n        assert exc.match(\"All connection attempts failed\")\n        assert isinstance(exc.value.__cause__, exception_class)\n        if isinstance(exc.value.__cause__, ExceptionGroup):\n            for exception in exc.value.__cause__.exceptions:\n                assert isinstance(exception, ConnectionRefusedError)\n\n    async def test_receive_timeout(\n        self, server_sock: socket.socket, server_addr: tuple[str, int]\n    ) -> None:\n        def serve() -> None:\n            conn, _ = server_sock.accept()\n            time.sleep(1)\n            conn.close()\n\n        thread = Thread(target=serve, daemon=True)\n        thread.start()\n        async with await connect_tcp(*server_addr) as stream:\n            start_time = time.monotonic()\n            with move_on_after(0.1):\n                while time.monotonic() - start_time < 0.3:\n                    await stream.receive(1)\n\n                pytest.fail(\"The timeout was not respected\")\n\n    async def test_concurrent_send(self, server_addr: tuple[str, int]) -> None:\n        async def send_data() -> NoReturn:\n            while True:\n                await stream.send(b\"\\x00\" * 4096)\n\n        async with await connect_tcp(*server_addr) as stream:\n            async with create_task_group() as tg:\n                tg.start_soon(send_data)\n                await wait_all_tasks_blocked()\n                with pytest.raises(BusyResourceError) as exc:\n                    await stream.send(b\"foo\")\n\n                exc.match(\"already writing to\")\n                tg.cancel_scope.cancel()\n\n    async def test_concurrent_receive(self, server_addr: tuple[str, int]) -> None:\n        async with await connect_tcp(*server_addr) as client:\n            async with create_task_group() as tg:\n                tg.start_soon(client.receive)\n                await wait_all_tasks_blocked()\n                try:\n                    with pytest.raises(BusyResourceError) as exc:\n                        await client.receive()\n\n                    exc.match(\"already reading from\")\n                finally:\n                    tg.cancel_scope.cancel()\n\n    async def test_close_during_receive(self, server_addr: tuple[str, int]) -> None:\n        async def interrupt() -> None:\n            await wait_all_tasks_blocked()\n            await stream.aclose()\n\n        async with await connect_tcp(*server_addr) as stream:\n            async with create_task_group() as tg:\n                tg.start_soon(interrupt)\n                with pytest.raises(ClosedResourceError):\n                    await stream.receive()\n\n    async def test_receive_after_close(self, server_addr: tuple[str, int]) -> None:\n        stream = await connect_tcp(*server_addr)\n        await stream.aclose()\n        with pytest.raises(ClosedResourceError):\n            await stream.receive()\n\n    async def test_send_after_close(self, server_addr: tuple[str, int]) -> None:\n        stream = await connect_tcp(*server_addr)\n        await stream.aclose()\n        with pytest.raises(ClosedResourceError):\n            await stream.send(b\"foo\")\n\n    async def test_send_after_peer_closed(self, family: AnyIPAddressFamily) -> None:\n        def serve_once() -> None:\n            client_sock, _ = server_sock.accept()\n            client_sock.close()\n            server_sock.close()\n\n        server_sock = socket.socket(family, socket.SOCK_STREAM)\n        server_sock.settimeout(1)\n        server_sock.bind((\"localhost\", 0))\n        server_addr = server_sock.getsockname()[:2]\n        server_sock.listen()\n        thread = Thread(target=serve_once, daemon=True)\n        thread.start()\n\n        with pytest.raises(BrokenResourceError):\n            async with await connect_tcp(*server_addr) as stream:\n                for _ in range(1000):\n                    await stream.send(b\"foo\")\n\n        thread.join()\n\n    async def test_connect_tcp_with_tls(\n        self,\n        server_context: SSLContext,\n        client_context: SSLContext,\n        server_sock: socket.socket,\n        server_addr: tuple[str, int],\n    ) -> None:\n        def serve() -> None:\n            with suppress(socket.timeout):\n                client, addr = server_sock.accept()\n                client.settimeout(1)\n                client = server_context.wrap_socket(client, server_side=True)\n                data = client.recv(100)\n                client.sendall(data[::-1])\n                client.unwrap()\n                client.close()\n\n        # The TLSStream tests are more comprehensive than this one!\n        thread = Thread(target=serve, daemon=True)\n        thread.start()\n        async with await connect_tcp(\n            *server_addr, tls_hostname=\"localhost\", ssl_context=client_context\n        ) as stream:\n            await stream.send(b\"hello\")\n            response = await stream.receive()\n\n        assert response == b\"olleh\"\n        thread.join()\n\n    async def test_connect_tcp_with_tls_cert_check_fail(\n        self,\n        server_context: SSLContext,\n        server_sock: socket.socket,\n        server_addr: tuple[str, int],\n    ) -> None:\n        thread_exception = None\n\n        def serve() -> None:\n            nonlocal thread_exception\n            client, addr = server_sock.accept()\n            with client:\n                client.settimeout(1)\n                try:\n                    server_context.wrap_socket(client, server_side=True)\n                except OSError:\n                    pass\n                except BaseException as exc:\n                    thread_exception = exc\n\n        thread = Thread(target=serve, daemon=True)\n        thread.start()\n        with pytest.raises(SSLError):\n            await connect_tcp(*server_addr, tls_hostname=\"localhost\")\n\n        thread.join()\n        assert thread_exception is None\n\n    @pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\n    async def test_unretrieved_future_exception_server_crash(\n        self, family: AnyIPAddressFamily, caplog: LogCaptureFixture\n    ) -> None:\n        \"\"\"\n        Test that there won't be any leftover Futures that don't get their exceptions\n        retrieved.\n\n        See https://github.com/encode/httpcore/issues/382 for details.\n\n        \"\"\"\n\n        def serve() -> None:\n            sock, addr = server_sock.accept()\n            event.wait(3)\n            sock.close()\n            del sock\n            gc.collect()\n\n        with socket.socket(family, socket.SOCK_STREAM) as server_sock:\n            server_sock.settimeout(1)\n            server_sock.bind((\"localhost\", 0))\n            server_sock.listen()\n            server_addr = server_sock.getsockname()[:2]\n            event = threading.Event()\n            thread = Thread(target=serve)\n            thread.start()\n            async with await connect_tcp(*server_addr) as stream:\n                await stream.send(b\"GET\")\n                event.set()\n                with pytest.raises(BrokenResourceError):\n                    await stream.receive()\n\n            thread.join()\n            gc.collect()\n            assert not caplog.text\n\n\n@pytest.mark.network\nclass TestTCPListener:\n    async def test_extra_attributes(self, family: AnyIPAddressFamily) -> None:\n        async with await create_tcp_listener(\n            local_host=\"localhost\", family=family\n        ) as multi:\n            assert multi.extra(SocketAttribute.family) == family\n            for listener in multi.listeners:\n                raw_socket = listener.extra(SocketAttribute.raw_socket)\n                assert listener.extra(SocketAttribute.family) == family\n                assert (\n                    listener.extra(SocketAttribute.local_address)\n                    == raw_socket.getsockname()[:2]\n                )\n                assert (\n                    listener.extra(SocketAttribute.local_port)\n                    == raw_socket.getsockname()[1]\n                )\n                pytest.raises(\n                    TypedAttributeLookupError,\n                    listener.extra,\n                    SocketAttribute.remote_address,\n                )\n                pytest.raises(\n                    TypedAttributeLookupError,\n                    listener.extra,\n                    SocketAttribute.remote_port,\n                )\n\n    @pytest.mark.parametrize(\n        \"family\",\n        [\n            pytest.param(AddressFamily.AF_INET, id=\"ipv4\"),\n            pytest.param(AddressFamily.AF_INET6, id=\"ipv6\", marks=[skip_ipv6_mark]),\n            pytest.param(socket.AF_UNSPEC, id=\"both\", marks=[skip_ipv6_mark]),\n        ],\n    )\n    async def test_accept(self, family: AnyIPAddressFamily) -> None:\n        async with await create_tcp_listener(\n            local_host=\"localhost\", family=family\n        ) as multi:\n            for listener in multi.listeners:\n                client = socket.socket(listener.extra(SocketAttribute.family))\n                client.settimeout(1)\n                client.connect(listener.extra(SocketAttribute.local_address))\n                assert isinstance(listener, SocketListener)\n                stream = await listener.accept()\n                client.sendall(b\"blah\")\n                request = await stream.receive()\n                await stream.send(request[::-1])\n                assert client.recv(100) == b\"halb\"\n                client.close()\n                await stream.aclose()\n\n    async def test_accept_after_close(self, family: AnyIPAddressFamily) -> None:\n        async with await create_tcp_listener(\n            local_host=\"localhost\", family=family\n        ) as multi:\n            for listener in multi.listeners:\n                await listener.aclose()\n                assert isinstance(listener, SocketListener)\n                with pytest.raises(ClosedResourceError):\n                    await listener.accept()\n\n    async def test_socket_options(self, family: AnyIPAddressFamily) -> None:\n        async with await create_tcp_listener(\n            local_host=\"localhost\", family=family\n        ) as multi:\n            for listener in multi.listeners:\n                raw_socket = listener.extra(SocketAttribute.raw_socket)\n                if sys.platform == \"win32\":\n                    assert (\n                        raw_socket.getsockopt(\n                            socket.SOL_SOCKET, socket.SO_EXCLUSIVEADDRUSE\n                        )\n                        != 0\n                    )\n                else:\n                    assert (\n                        raw_socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR)\n                        != 0\n                    )\n\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 80000)\n                assert raw_socket.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF) in (\n                    80000,\n                    160000,\n                )\n\n                client = socket.socket(raw_socket.family)\n                client.settimeout(1)\n                client.connect(raw_socket.getsockname())\n\n                assert isinstance(listener, SocketListener)\n                async with await listener.accept() as stream:\n                    raw_socket = stream.extra(SocketAttribute.raw_socket)\n                    assert raw_socket.gettimeout() == 0\n                    assert raw_socket.family == listener.extra(SocketAttribute.family)\n                    assert (\n                        raw_socket.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY)\n                        != 0\n                    )\n\n                client.close()\n\n    @pytest.mark.skipif(\n        not hasattr(socket, \"SO_REUSEPORT\"), reason=\"SO_REUSEPORT option not supported\"\n    )\n    async def test_reuse_port(self, family: AnyIPAddressFamily) -> None:\n        multi1 = await create_tcp_listener(\n            local_host=\"localhost\", family=family, reuse_port=True\n        )\n        assert len(multi1.listeners) == 1\n\n        multi2 = await create_tcp_listener(\n            local_host=\"localhost\",\n            local_port=multi1.listeners[0].extra(SocketAttribute.local_port),\n            family=family,\n            reuse_port=True,\n        )\n        assert len(multi2.listeners) == 1\n\n        assert multi1.listeners[0].extra(\n            SocketAttribute.local_address\n        ) == multi2.listeners[0].extra(SocketAttribute.local_address)\n        await multi1.aclose()\n        await multi2.aclose()\n\n    async def test_close_from_other_task(self, family: AnyIPAddressFamily) -> None:\n        listener = await create_tcp_listener(local_host=\"localhost\", family=family)\n        with pytest.raises(ExceptionGroup) as exc:\n            async with create_task_group() as tg:\n                tg.start_soon(listener.serve, lambda stream: None)\n                await wait_all_tasks_blocked()\n                await listener.aclose()\n                tg.cancel_scope.cancel()\n\n        assert len(exc.value.exceptions) == 1\n        assert isinstance(exc.value.exceptions[0], ExceptionGroup)\n        nested_grp = exc.value.exceptions[0]\n        assert len(nested_grp.exceptions) == 1\n        assert isinstance(nested_grp.exceptions[0], ExceptionGroup)\n\n    async def test_send_after_eof(self, family: AnyIPAddressFamily) -> None:\n        async def handle(stream: SocketStream) -> None:\n            async with stream:\n                await stream.send(b\"Hello\\n\")\n\n        multi = await create_tcp_listener(family=family, local_host=\"localhost\")\n        async with multi, create_task_group() as tg:\n            tg.start_soon(multi.serve, handle)\n            await wait_all_tasks_blocked()\n\n            with socket.socket(family) as client:\n                client.connect(multi.extra(SocketAttribute.local_address))\n                client.shutdown(socket.SHUT_WR)\n                client.setblocking(False)\n                with fail_after(1):\n                    while True:\n                        try:\n                            message = client.recv(10)\n                        except BlockingIOError:\n                            await sleep(0)\n                        else:\n                            assert message == b\"Hello\\n\"\n                            break\n\n            tg.cancel_scope.cancel()\n\n    async def test_eof_after_send(self, family: AnyIPAddressFamily) -> None:\n        \"\"\"Regression test for #701.\"\"\"\n        received_bytes = b\"\"\n\n        async def handle(stream: SocketStream) -> None:\n            nonlocal received_bytes\n            async with stream:\n                received_bytes = await stream.receive()\n                with pytest.raises(EndOfStream), fail_after(1):\n                    await stream.receive()\n\n            tg.cancel_scope.cancel()\n\n        multi = await create_tcp_listener(family=family, local_host=\"localhost\")\n        async with multi, create_task_group() as tg:\n            with socket.socket(family) as client:\n                client.connect(multi.extra(SocketAttribute.local_address))\n                client.send(b\"Hello\")\n                client.shutdown(socket.SHUT_WR)\n                await multi.serve(handle)\n\n        assert received_bytes == b\"Hello\"\n\n    @skip_ipv6_mark\n    @pytest.mark.skipif(\n        sys.platform == \"win32\",\n        reason=\"Windows does not support interface name suffixes\",\n    )\n    async def test_bind_link_local(self) -> None:\n        # Regression test for #554\n        link_local_ipv6_address = next(\n            (\n                addr.address\n                for addresses in psutil.net_if_addrs().values()\n                for addr in addresses\n                if addr.address.startswith(\"fe80::\") and \"%\" in addr.address\n            ),\n            None,\n        )\n        if link_local_ipv6_address is None:\n            pytest.fail(\"Could not find a link-local IPv6 interface\")\n\n        async with await create_tcp_listener(local_host=link_local_ipv6_address):\n            pass\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"UNIX sockets are not available on Windows\"\n)\nclass TestUNIXStream:\n    @pytest.fixture\n    def socket_path(self) -> Generator[Path, None, None]:\n        # Use stdlib tempdir generation\n        # Fixes `OSError: AF_UNIX path too long` from pytest generated temp_path\n        with tempfile.TemporaryDirectory() as path:\n            yield Path(path) / \"socket\"\n\n    @pytest.fixture(params=[False, True], ids=[\"str\", \"path\"])\n    def socket_path_or_str(self, request: SubRequest, socket_path: Path) -> Path | str:\n        return socket_path if request.param else str(socket_path)\n\n    @pytest.fixture\n    def server_sock(self, socket_path: Path) -> Iterable[socket.socket]:\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        sock.bind(str(socket_path))\n        sock.listen()\n        yield sock\n        sock.close()\n\n    async def test_extra_attributes(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async with await connect_unix(socket_path) as stream:\n            raw_socket = stream.extra(SocketAttribute.raw_socket)\n            assert stream.extra(SocketAttribute.family) == socket.AF_UNIX\n            assert (\n                stream.extra(SocketAttribute.local_address) == raw_socket.getsockname()\n            )\n            assert stream.extra(SocketAttribute.remote_address) == str(socket_path)\n            pytest.raises(\n                TypedAttributeLookupError, stream.extra, SocketAttribute.local_port\n            )\n            pytest.raises(\n                TypedAttributeLookupError, stream.extra, SocketAttribute.remote_port\n            )\n\n    async def test_send_receive(\n        self, server_sock: socket.socket, socket_path_or_str: Path | str\n    ) -> None:\n        async with await connect_unix(socket_path_or_str) as stream:\n            client, _ = server_sock.accept()\n            await stream.send(b\"blah\")\n            request = client.recv(100)\n            client.sendall(request[::-1])\n            response = await stream.receive()\n            client.close()\n\n        assert response == b\"halb\"\n\n    async def test_receive_large_buffer(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        def serve() -> None:\n            client, _ = server_sock.accept()\n            client.sendall(buffer)\n            client.close()\n\n        buffer = (\n            b\"\\xff\" * 1024 * 512 + b\"\\x00\" * 1024 * 512\n        )  # should exceed the maximum kernel send buffer size\n        async with await connect_unix(socket_path) as stream:\n            thread = Thread(target=serve, daemon=True)\n            thread.start()\n            response = b\"\"\n            while len(response) < len(buffer):\n                response += await stream.receive()\n\n        thread.join()\n        assert response == buffer\n\n    async def test_send_large_buffer(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        response = b\"\"\n\n        def serve() -> None:\n            nonlocal response\n            client, _ = server_sock.accept()\n            while True:\n                data = client.recv(1024)\n                if not data:\n                    break\n\n                response += data\n\n            client.close()\n\n        buffer = (\n            b\"\\xff\" * 1024 * 512 + b\"\\x00\" * 1024 * 512\n        )  # should exceed the maximum kernel send buffer size\n        async with await connect_unix(socket_path) as stream:\n            thread = Thread(target=serve, daemon=True)\n            thread.start()\n            await stream.send(buffer)\n\n        thread.join()\n        assert response == buffer\n\n    async def test_receive_fds(\n        self, server_sock: socket.socket, socket_path: Path, tmp_path: Path\n    ) -> None:\n        def serve() -> None:\n            path1 = tmp_path / \"file1\"\n            path2 = tmp_path / \"file2\"\n            path1.write_text(\"Hello, \")\n            path2.write_text(\"World!\")\n            with path1.open() as file1, path2.open() as file2:\n                fdarray = array.array(\"i\", [file1.fileno(), file2.fileno()])\n                client, _ = server_sock.accept()\n                cmsg = (socket.SOL_SOCKET, socket.SCM_RIGHTS, fdarray)\n                with client:\n                    client.sendmsg([b\"test\"], [cmsg])\n\n        async with await connect_unix(socket_path) as stream:\n            thread = Thread(target=serve, daemon=True)\n            thread.start()\n            message, fds = await stream.receive_fds(10, 2)\n            thread.join()\n\n        text = \"\"\n        for fd in fds:\n            with os.fdopen(fd) as file:\n                text += file.read()\n\n        assert message == b\"test\"\n        assert text == \"Hello, World!\"\n\n    async def test_receive_fds_bad_args(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async with await connect_unix(socket_path) as stream:\n            for msglen in (-1, \"foo\"):\n                with pytest.raises(\n                    ValueError, match=\"msglen must be a non-negative integer\"\n                ):\n                    await stream.receive_fds(msglen, 0)  # type: ignore[arg-type]\n\n            for maxfds in (0, \"foo\"):\n                with pytest.raises(\n                    ValueError, match=\"maxfds must be a positive integer\"\n                ):\n                    await stream.receive_fds(0, maxfds)  # type: ignore[arg-type]\n\n    async def test_send_fds(\n        self, server_sock: socket.socket, socket_path: Path, tmp_path: Path\n    ) -> None:\n        def serve() -> None:\n            fds = array.array(\"i\")\n            client, _ = server_sock.accept()\n            msg, ancdata, *_ = client.recvmsg(10, socket.CMSG_LEN(2 * fds.itemsize))\n            client.close()\n            assert msg == b\"test\"\n            for cmsg_level, cmsg_type, cmsg_data in ancdata:\n                assert cmsg_level == socket.SOL_SOCKET\n                assert cmsg_type == socket.SCM_RIGHTS\n                fds.frombytes(\n                    cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)]\n                )\n\n            text = \"\"\n            for fd in fds:\n                with os.fdopen(fd) as file:\n                    text += file.read()\n\n            assert text == \"Hello, World!\"\n\n        path1 = tmp_path / \"file1\"\n        path2 = tmp_path / \"file2\"\n        path1.write_text(\"Hello, \")\n        path2.write_text(\"World!\")\n        with path1.open() as file1, path2.open() as file2, fail_after(2):\n            assert isinstance(file1, io.TextIOWrapper)\n            assert isinstance(file2, io.TextIOWrapper)\n            async with await connect_unix(socket_path) as stream:\n                thread = Thread(target=serve, daemon=True)\n                thread.start()\n                await stream.send_fds(b\"test\", [file1, file2])\n                thread.join()\n\n    async def test_send_eof(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        def serve() -> None:\n            client, _ = server_sock.accept()\n            request = b\"\"\n            while True:\n                data = client.recv(100)\n                request += data\n                if not data:\n                    break\n\n            client.sendall(request[::-1])\n            client.close()\n\n        async with await connect_unix(socket_path) as stream:\n            thread = Thread(target=serve, daemon=True)\n            thread.start()\n            await stream.send(b\"hello, \")\n            await stream.send(b\"world\\n\")\n            await stream.send_eof()\n            response = await stream.receive()\n\n        thread.join()\n        assert response == b\"\\ndlrow ,olleh\"\n\n    async def test_iterate(self, server_sock: socket.socket, socket_path: Path) -> None:\n        def serve() -> None:\n            client, _ = server_sock.accept()\n            client.sendall(b\"bl\")\n            time.sleep(0.05)\n            client.sendall(b\"ah\")\n            client.close()\n\n        thread = Thread(target=serve, daemon=True)\n        thread.start()\n        chunks = []\n        async with await connect_unix(socket_path) as stream:\n            async for chunk in stream:\n                chunks.append(chunk)\n\n        thread.join()\n        assert chunks == [b\"bl\", b\"ah\"]\n\n    async def test_send_fds_bad_args(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async with await connect_unix(socket_path) as stream:\n            with pytest.raises(ValueError, match=\"message must not be empty\"):\n                await stream.send_fds(b\"\", [0])\n\n            with pytest.raises(ValueError, match=\"fds must not be empty\"):\n                await stream.send_fds(b\"test\", [])\n\n    async def test_concurrent_send(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async def send_data() -> NoReturn:\n            while True:\n                await client.send(b\"\\x00\" * 4096)\n\n        async with await connect_unix(socket_path) as client:\n            async with create_task_group() as tg:\n                tg.start_soon(send_data)\n                await wait_all_tasks_blocked()\n                with pytest.raises(BusyResourceError) as exc:\n                    await client.send(b\"foo\")\n\n                exc.match(\"already writing to\")\n                tg.cancel_scope.cancel()\n\n    async def test_concurrent_receive(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async with await connect_unix(socket_path) as client:\n            async with create_task_group() as tg:\n                tg.start_soon(client.receive)\n                await wait_all_tasks_blocked()\n                try:\n                    with pytest.raises(BusyResourceError) as exc:\n                        await client.receive()\n\n                    exc.match(\"already reading from\")\n                finally:\n                    tg.cancel_scope.cancel()\n\n    async def test_close_during_receive(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async def interrupt() -> None:\n            await wait_all_tasks_blocked()\n            await stream.aclose()\n\n        async with await connect_unix(socket_path) as stream:\n            async with create_task_group() as tg:\n                tg.start_soon(interrupt)\n                with pytest.raises(ClosedResourceError):\n                    await stream.receive()\n\n    async def test_receive_after_close(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        stream = await connect_unix(socket_path)\n        await stream.aclose()\n        with pytest.raises(ClosedResourceError):\n            await stream.receive()\n\n    async def test_send_after_close(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        stream = await connect_unix(socket_path)\n        await stream.aclose()\n        with pytest.raises(ClosedResourceError):\n            await stream.send(b\"foo\")\n\n    async def test_cannot_connect(self, socket_path: Path) -> None:\n        with pytest.raises(FileNotFoundError):\n            await connect_unix(socket_path)\n\n    async def test_connecting_using_bytes(\n        self, server_sock: socket.socket, socket_path: Path\n    ) -> None:\n        async with await connect_unix(str(socket_path).encode()):\n            pass\n\n    @pytest.mark.skipif(\n        platform.system() == \"Darwin\", reason=\"macOS requires valid UTF-8 paths\"\n    )\n    async def test_connecting_with_non_utf8(self, socket_path: Path) -> None:\n        actual_path = str(socket_path).encode() + b\"\\xf0\"\n        with socket.socket(socket.AF_UNIX) as server:\n            server.bind(actual_path)\n            server.listen(1)\n\n            async with await connect_unix(actual_path):\n                pass\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"UNIX sockets are not available on Windows\"\n)\nclass TestUNIXListener:\n    @pytest.fixture\n    def socket_path(self) -> Generator[Path, None, None]:\n        # Use stdlib tempdir generation\n        # Fixes `OSError: AF_UNIX path too long` from pytest generated temp_path\n        with tempfile.TemporaryDirectory() as path:\n            yield Path(path) / \"socket\"\n\n    @pytest.fixture(params=[False, True], ids=[\"str\", \"path\"])\n    def socket_path_or_str(self, request: SubRequest, socket_path: Path) -> Path | str:\n        return socket_path if request.param else str(socket_path)\n\n    async def test_extra_attributes(self, socket_path: Path) -> None:\n        async with await create_unix_listener(socket_path) as listener:\n            raw_socket = listener.extra(SocketAttribute.raw_socket)\n            assert listener.extra(SocketAttribute.family) == socket.AF_UNIX\n            assert (\n                listener.extra(SocketAttribute.local_address)\n                == raw_socket.getsockname()\n            )\n            pytest.raises(\n                TypedAttributeLookupError, listener.extra, SocketAttribute.local_port\n            )\n            pytest.raises(\n                TypedAttributeLookupError,\n                listener.extra,\n                SocketAttribute.remote_address,\n            )\n            pytest.raises(\n                TypedAttributeLookupError, listener.extra, SocketAttribute.remote_port\n            )\n\n    async def test_accept(self, socket_path_or_str: Path | str) -> None:\n        async with await create_unix_listener(socket_path_or_str) as listener:\n            client = socket.socket(socket.AF_UNIX)\n            client.settimeout(1)\n            client.connect(str(socket_path_or_str))\n            stream = await listener.accept()\n            client.sendall(b\"blah\")\n            request = await stream.receive()\n            await stream.send(request[::-1])\n            assert client.recv(100) == b\"halb\"\n            client.close()\n            await stream.aclose()\n\n    async def test_socket_options(self, socket_path: Path) -> None:\n        async with await create_unix_listener(socket_path) as listener:\n            listener_socket = listener.extra(SocketAttribute.raw_socket)\n            assert listener_socket.family == socket.AddressFamily.AF_UNIX\n            listener_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 80000)\n            assert listener_socket.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF) in (\n                80000,\n                160000,\n            )\n\n            client = socket.socket(listener_socket.family)\n            client.settimeout(1)\n            client.connect(listener_socket.getsockname())\n\n            async with await listener.accept() as stream:\n                assert stream.extra(SocketAttribute.raw_socket).gettimeout() == 0\n                assert stream.extra(SocketAttribute.family) == listener_socket.family\n\n            client.close()\n\n    async def test_send_after_eof(self, socket_path: Path) -> None:\n        async def handle(stream: SocketStream) -> None:\n            async with stream:\n                await stream.send(b\"Hello\\n\")\n\n        async with await create_unix_listener(\n            socket_path\n        ) as listener, create_task_group() as tg:\n            tg.start_soon(listener.serve, handle)\n            await wait_all_tasks_blocked()\n\n            with socket.socket(socket.AF_UNIX) as client:\n                client.connect(str(socket_path))\n                client.shutdown(socket.SHUT_WR)\n                client.setblocking(False)\n                with fail_after(1):\n                    while True:\n                        try:\n                            message = client.recv(10)\n                        except BlockingIOError:\n                            await sleep(0)\n                        else:\n                            assert message == b\"Hello\\n\"\n                            break\n\n            tg.cancel_scope.cancel()\n\n    async def test_bind_twice(self, socket_path: Path) -> None:\n        \"\"\"Test that the previous socket is removed before binding to the path.\"\"\"\n        for _ in range(2):\n            async with await create_unix_listener(socket_path):\n                pass\n\n    async def test_listening_bytes_path(self, socket_path: Path) -> None:\n        async with await create_unix_listener(str(socket_path).encode()):\n            pass\n\n    @pytest.mark.skipif(\n        platform.system() == \"Darwin\", reason=\"macOS requires valid UTF-8 paths\"\n    )\n    async def test_listening_invalid_ascii(self, socket_path: Path) -> None:\n        real_path = str(socket_path).encode() + b\"\\xf0\"\n        async with await create_unix_listener(real_path):\n            pass\n\n\nasync def test_multi_listener(tmp_path_factory: TempPathFactory) -> None:\n    async def handle(stream: SocketStream) -> None:\n        client_addresses.append(stream.extra(SocketAttribute.remote_address))\n        event.set()\n        await stream.aclose()\n\n    client_addresses: list[str | IPSockAddrType] = []\n    listeners: list[Listener] = [await create_tcp_listener(local_host=\"localhost\")]\n    with tempfile.TemporaryDirectory() as path:\n        if sys.platform != \"win32\":\n            listeners.append(await create_unix_listener(Path(path) / \"socket\"))\n\n        expected_addresses: list[str | IPSockAddrType] = []\n        async with MultiListener(listeners) as multi_listener:\n            async with create_task_group() as tg:\n                tg.start_soon(multi_listener.serve, handle)\n                for listener in multi_listener.listeners:\n                    event = Event()\n                    local_address = listener.extra(SocketAttribute.local_address)\n                    if (\n                        sys.platform != \"win32\"\n                        and listener.extra(SocketAttribute.family)\n                        == socket.AddressFamily.AF_UNIX\n                    ):\n                        assert isinstance(local_address, str)\n                        stream: SocketStream = await connect_unix(local_address)\n                    else:\n                        assert isinstance(local_address, tuple)\n                        stream = await connect_tcp(*local_address)\n\n                    expected_addresses.append(\n                        stream.extra(SocketAttribute.local_address)\n                    )\n                    await event.wait()\n                    await stream.aclose()\n\n                tg.cancel_scope.cancel()\n\n        assert client_addresses == expected_addresses\n\n\n@pytest.mark.network\n@pytest.mark.usefixtures(\"check_asyncio_bug\")\nclass TestUDPSocket:\n    async def test_extra_attributes(self, family: AnyIPAddressFamily) -> None:\n        async with await create_udp_socket(\n            family=family, local_host=\"localhost\"\n        ) as udp:\n            raw_socket = udp.extra(SocketAttribute.raw_socket)\n            assert raw_socket.gettimeout() == 0\n            assert udp.extra(SocketAttribute.family) == family\n            assert (\n                udp.extra(SocketAttribute.local_address) == raw_socket.getsockname()[:2]\n            )\n            assert udp.extra(SocketAttribute.local_port) == raw_socket.getsockname()[1]\n            pytest.raises(\n                TypedAttributeLookupError, udp.extra, SocketAttribute.remote_address\n            )\n            pytest.raises(\n                TypedAttributeLookupError, udp.extra, SocketAttribute.remote_port\n            )\n\n    async def test_send_receive(self, family: AnyIPAddressFamily) -> None:\n        async with await create_udp_socket(\n            local_host=\"localhost\", family=family\n        ) as sock:\n            host, port = sock.extra(SocketAttribute.local_address)  # type: ignore[misc]\n            await sock.sendto(b\"blah\", host, port)\n            request, addr = await sock.receive()\n            assert request == b\"blah\"\n            assert addr == sock.extra(SocketAttribute.local_address)\n\n            await sock.sendto(b\"halb\", host, port)\n            response, addr = await sock.receive()\n            assert response == b\"halb\"\n            assert addr == (host, port)\n\n    async def test_iterate(self, family: AnyIPAddressFamily) -> None:\n        async def serve() -> None:\n            async for packet, addr in server:\n                await server.send((packet[::-1], addr))\n\n        async with await create_udp_socket(\n            family=family, local_host=\"localhost\"\n        ) as server:\n            host, port = server.extra(  # type: ignore[misc]\n                SocketAttribute.local_address\n            )\n            async with await create_udp_socket(\n                family=family, local_host=\"localhost\"\n            ) as client:\n                async with create_task_group() as tg:\n                    tg.start_soon(serve)\n                    await client.sendto(b\"FOOBAR\", host, port)\n                    assert await client.receive() == (b\"RABOOF\", (host, port))\n                    await client.sendto(b\"123456\", host, port)\n                    assert await client.receive() == (b\"654321\", (host, port))\n                    tg.cancel_scope.cancel()\n\n    @pytest.mark.skipif(\n        not hasattr(socket, \"SO_REUSEPORT\"), reason=\"SO_REUSEPORT option not supported\"\n    )\n    async def test_reuse_port(self, family: AnyIPAddressFamily) -> None:\n        async with await create_udp_socket(\n            family=family, local_host=\"localhost\", reuse_port=True\n        ) as udp:\n            port = udp.extra(SocketAttribute.local_port)\n            assert port != 0\n            async with await create_udp_socket(\n                family=family, local_host=\"localhost\", local_port=port, reuse_port=True\n            ) as udp2:\n                assert port == udp2.extra(SocketAttribute.local_port)\n\n    async def test_concurrent_receive(self) -> None:\n        async with await create_udp_socket(\n            family=AddressFamily.AF_INET, local_host=\"localhost\"\n        ) as udp:\n            async with create_task_group() as tg:\n                tg.start_soon(udp.receive)\n                await wait_all_tasks_blocked()\n                try:\n                    with pytest.raises(BusyResourceError) as exc:\n                        await udp.receive()\n\n                    exc.match(\"already reading from\")\n                finally:\n                    tg.cancel_scope.cancel()\n\n    async def test_close_during_receive(self) -> None:\n        async def close_when_blocked() -> None:\n            await wait_all_tasks_blocked()\n            await udp.aclose()\n\n        async with await create_udp_socket(\n            family=AddressFamily.AF_INET, local_host=\"localhost\"\n        ) as udp:\n            async with create_task_group() as tg:\n                tg.start_soon(close_when_blocked)\n                with pytest.raises(ClosedResourceError):\n                    await udp.receive()\n\n    async def test_receive_after_close(self) -> None:\n        udp = await create_udp_socket(\n            family=AddressFamily.AF_INET, local_host=\"localhost\"\n        )\n        await udp.aclose()\n        with pytest.raises(ClosedResourceError):\n            await udp.receive()\n\n    async def test_send_after_close(self) -> None:\n        udp = await create_udp_socket(\n            family=AddressFamily.AF_INET, local_host=\"localhost\"\n        )\n        host, port = udp.extra(SocketAttribute.local_address)  # type: ignore[misc]\n        await udp.aclose()\n        with pytest.raises(ClosedResourceError):\n            await udp.sendto(b\"foo\", host, port)\n\n    async def test_create_unbound_socket(self, family: AnyIPAddressFamily) -> None:\n        \"\"\"Regression test for #360.\"\"\"\n        async with await create_udp_socket(family=family) as udp:\n            local_address = cast(\n                IPSockAddrType, udp.extra(SocketAttribute.local_address)\n            )\n            assert local_address[1] > 0\n\n\n@pytest.mark.network\n@pytest.mark.usefixtures(\"check_asyncio_bug\")\nclass TestConnectedUDPSocket:\n    async def test_extra_attributes(self, family: AnyIPAddressFamily) -> None:\n        async with await create_connected_udp_socket(\n            \"localhost\", 5000, family=family\n        ) as udp:\n            raw_socket = udp.extra(SocketAttribute.raw_socket)\n            assert udp.extra(SocketAttribute.family) == family\n            assert (\n                udp.extra(SocketAttribute.local_address) == raw_socket.getsockname()[:2]\n            )\n            assert udp.extra(SocketAttribute.local_port) == raw_socket.getsockname()[1]\n            assert (\n                udp.extra(SocketAttribute.remote_address)\n                == raw_socket.getpeername()[:2]\n            )\n            assert udp.extra(SocketAttribute.remote_port) == 5000\n\n    async def test_send_receive(self, family: AnyIPAddressFamily) -> None:\n        async with await create_udp_socket(\n            family=family, local_host=\"localhost\"\n        ) as udp1:\n            host, port = udp1.extra(SocketAttribute.local_address)  # type: ignore[misc]\n            async with await create_connected_udp_socket(\n                host, port, local_host=\"localhost\", family=family\n            ) as udp2:\n                host, port = udp2.extra(\n                    SocketAttribute.local_address  # type: ignore[misc]\n                )\n                await udp2.send(b\"blah\")\n                request = await udp1.receive()\n                assert request == (b\"blah\", (host, port))\n\n                await udp1.sendto(b\"halb\", host, port)\n                response = await udp2.receive()\n                assert response == b\"halb\"\n\n    async def test_iterate(self, family: AnyIPAddressFamily) -> None:\n        async def serve() -> None:\n            async for packet in udp2:\n                await udp2.send(packet[::-1])\n\n        async with await create_udp_socket(\n            family=family, local_host=\"localhost\"\n        ) as udp1:\n            host, port = udp1.extra(SocketAttribute.local_address)  # type: ignore[misc]\n            async with await create_connected_udp_socket(host, port) as udp2:\n                host, port = udp2.extra(  # type: ignore[misc]\n                    SocketAttribute.local_address\n                )\n                async with create_task_group() as tg:\n                    tg.start_soon(serve)\n                    await udp1.sendto(b\"FOOBAR\", host, port)\n                    assert await udp1.receive() == (b\"RABOOF\", (host, port))\n                    await udp1.sendto(b\"123456\", host, port)\n                    assert await udp1.receive() == (b\"654321\", (host, port))\n                    tg.cancel_scope.cancel()\n\n    @pytest.mark.skipif(\n        not hasattr(socket, \"SO_REUSEPORT\"), reason=\"SO_REUSEPORT option not supported\"\n    )\n    async def test_reuse_port(self, family: AnyIPAddressFamily) -> None:\n        async with await create_connected_udp_socket(\n            \"localhost\", 6000, family=family, local_host=\"localhost\", reuse_port=True\n        ) as udp:\n            port = udp.extra(SocketAttribute.local_port)\n            assert port != 0\n            async with await create_connected_udp_socket(\n                \"localhost\",\n                6001,\n                family=family,\n                local_host=\"localhost\",\n                local_port=port,\n                reuse_port=True,\n            ) as udp2:\n                assert port == udp2.extra(SocketAttribute.local_port)\n\n    async def test_concurrent_receive(self) -> None:\n        async with await create_connected_udp_socket(\n            \"localhost\", 5000, local_host=\"localhost\", family=AddressFamily.AF_INET\n        ) as udp:\n            async with create_task_group() as tg:\n                tg.start_soon(udp.receive)\n                await wait_all_tasks_blocked()\n                try:\n                    with pytest.raises(BusyResourceError) as exc:\n                        await udp.receive()\n\n                    exc.match(\"already reading from\")\n                finally:\n                    tg.cancel_scope.cancel()\n\n    async def test_close_during_receive(self) -> None:\n        async def close_when_blocked() -> None:\n            await wait_all_tasks_blocked()\n            await udp.aclose()\n\n        async with await create_connected_udp_socket(\n            \"localhost\", 5000, local_host=\"localhost\", family=AddressFamily.AF_INET\n        ) as udp:\n            async with create_task_group() as tg:\n                tg.start_soon(close_when_blocked)\n                with pytest.raises(ClosedResourceError):\n                    await udp.receive()\n\n    async def test_receive_after_close(self, family: AnyIPAddressFamily) -> None:\n        udp = await create_connected_udp_socket(\n            \"localhost\", 5000, local_host=\"localhost\", family=family\n        )\n        await udp.aclose()\n        with pytest.raises(ClosedResourceError):\n            await udp.receive()\n\n    async def test_send_after_close(self, family: AnyIPAddressFamily) -> None:\n        udp = await create_connected_udp_socket(\n            \"localhost\", 5000, local_host=\"localhost\", family=family\n        )\n        await udp.aclose()\n        with pytest.raises(ClosedResourceError):\n            await udp.send(b\"foo\")\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"UNIX sockets are not available on Windows\"\n)\nclass TestUNIXDatagramSocket:\n    @pytest.fixture\n    def socket_path(self) -> Generator[Path, None, None]:\n        # Use stdlib tempdir generation\n        # Fixes `OSError: AF_UNIX path too long` from pytest generated temp_path\n        with tempfile.TemporaryDirectory() as path:\n            yield Path(path) / \"socket\"\n\n    @pytest.fixture(params=[False, True], ids=[\"str\", \"path\"])\n    def socket_path_or_str(self, request: SubRequest, socket_path: Path) -> Path | str:\n        return socket_path if request.param else str(socket_path)\n\n    @pytest.fixture\n    def peer_socket_path(self) -> Generator[Path, None, None]:\n        # Use stdlib tempdir generation\n        # Fixes `OSError: AF_UNIX path too long` from pytest generated temp_path\n        with tempfile.TemporaryDirectory() as path:\n            yield Path(path) / \"peer_socket\"\n\n    async def test_extra_attributes(self, socket_path: Path) -> None:\n        async with await create_unix_datagram_socket(local_path=socket_path) as unix_dg:\n            raw_socket = unix_dg.extra(SocketAttribute.raw_socket)\n            assert raw_socket.gettimeout() == 0\n            assert unix_dg.extra(SocketAttribute.family) == socket.AF_UNIX\n            assert (\n                unix_dg.extra(SocketAttribute.local_address) == raw_socket.getsockname()\n            )\n            pytest.raises(\n                TypedAttributeLookupError, unix_dg.extra, SocketAttribute.local_port\n            )\n            pytest.raises(\n                TypedAttributeLookupError, unix_dg.extra, SocketAttribute.remote_address\n            )\n            pytest.raises(\n                TypedAttributeLookupError, unix_dg.extra, SocketAttribute.remote_port\n            )\n\n    async def test_send_receive(self, socket_path_or_str: Path | str) -> None:\n        async with await create_unix_datagram_socket(\n            local_path=socket_path_or_str,\n        ) as sock:\n            path = str(socket_path_or_str)\n\n            await sock.sendto(b\"blah\", path)\n            request, addr = await sock.receive()\n            assert request == b\"blah\"\n            assert addr == path\n\n            await sock.sendto(b\"halb\", path)\n            response, addr = await sock.receive()\n            assert response == b\"halb\"\n            assert addr == path\n\n    async def test_iterate(self, peer_socket_path: Path, socket_path: Path) -> None:\n        async def serve() -> None:\n            async for packet, addr in server:\n                await server.send((packet[::-1], addr))\n\n        async with await create_unix_datagram_socket(\n            local_path=peer_socket_path,\n        ) as server:\n            peer_path = str(peer_socket_path)\n            async with await create_unix_datagram_socket(\n                local_path=socket_path\n            ) as client:\n                async with create_task_group() as tg:\n                    tg.start_soon(serve)\n                    await client.sendto(b\"FOOBAR\", peer_path)\n                    assert await client.receive() == (b\"RABOOF\", peer_path)\n                    await client.sendto(b\"123456\", peer_path)\n                    assert await client.receive() == (b\"654321\", peer_path)\n                    tg.cancel_scope.cancel()\n\n    async def test_concurrent_receive(self) -> None:\n        async with await create_unix_datagram_socket() as unix_dg:\n            async with create_task_group() as tg:\n                tg.start_soon(unix_dg.receive)\n                await wait_all_tasks_blocked()\n                try:\n                    with pytest.raises(BusyResourceError) as exc:\n                        await unix_dg.receive()\n\n                    exc.match(\"already reading from\")\n                finally:\n                    tg.cancel_scope.cancel()\n\n    async def test_close_during_receive(self) -> None:\n        async def close_when_blocked() -> None:\n            await wait_all_tasks_blocked()\n            await unix_dg.aclose()\n\n        async with await create_unix_datagram_socket() as unix_dg:\n            async with create_task_group() as tg:\n                tg.start_soon(close_when_blocked)\n                with pytest.raises(ClosedResourceError):\n                    await unix_dg.receive()\n\n    async def test_receive_after_close(self) -> None:\n        unix_dg = await create_unix_datagram_socket()\n        await unix_dg.aclose()\n        with pytest.raises(ClosedResourceError):\n            await unix_dg.receive()\n\n    async def test_send_after_close(self, socket_path: Path) -> None:\n        unix_dg = await create_unix_datagram_socket(local_path=socket_path)\n        path = str(socket_path)\n        await unix_dg.aclose()\n        with pytest.raises(ClosedResourceError):\n            await unix_dg.sendto(b\"foo\", path)\n\n    async def test_local_path_bytes(self, socket_path: Path) -> None:\n        async with await create_unix_datagram_socket(\n            local_path=str(socket_path).encode()\n        ):\n            pass\n\n    @pytest.mark.skipif(\n        platform.system() == \"Darwin\", reason=\"macOS requires valid UTF-8 paths\"\n    )\n    async def test_local_path_invalid_ascii(self, socket_path: Path) -> None:\n        real_path = str(socket_path).encode() + b\"\\xf0\"\n        async with await create_unix_datagram_socket(local_path=real_path):\n            pass\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"UNIX sockets are not available on Windows\"\n)\nclass TestConnectedUNIXDatagramSocket:\n    @pytest.fixture\n    def socket_path(self) -> Generator[Path, None, None]:\n        # Use stdlib tempdir generation\n        # Fixes `OSError: AF_UNIX path too long` from pytest generated temp_path\n        with tempfile.TemporaryDirectory() as path:\n            yield Path(path) / \"socket\"\n\n    @pytest.fixture(params=[False, True], ids=[\"str\", \"path\"])\n    def socket_path_or_str(self, request: SubRequest, socket_path: Path) -> Path | str:\n        return socket_path if request.param else str(socket_path)\n\n    @pytest.fixture\n    def peer_socket_path(self) -> Generator[Path, None, None]:\n        # Use stdlib tempdir generation\n        # Fixes `OSError: AF_UNIX path too long` from pytest generated temp_path\n        with tempfile.TemporaryDirectory() as path:\n            yield Path(path) / \"peer_socket\"\n\n    @pytest.fixture(params=[False, True], ids=[\"peer_str\", \"peer_path\"])\n    def peer_socket_path_or_str(\n        self, request: SubRequest, peer_socket_path: Path\n    ) -> Path | str:\n        return peer_socket_path if request.param else str(peer_socket_path)\n\n    @pytest.fixture\n    def peer_sock(self, peer_socket_path: Path) -> Iterable[socket.socket]:\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n        sock.settimeout(1)\n        sock.bind(str(peer_socket_path))\n        yield sock\n        sock.close()\n\n    async def test_extra_attributes(\n        self,\n        socket_path: Path,\n        peer_socket_path: Path,\n        peer_sock: socket.socket,\n    ) -> None:\n        async with await create_connected_unix_datagram_socket(\n            remote_path=peer_socket_path,\n            local_path=socket_path,\n        ) as unix_dg:\n            raw_socket = unix_dg.extra(SocketAttribute.raw_socket)\n            assert raw_socket is not None\n            assert unix_dg.extra(SocketAttribute.family) == AddressFamily.AF_UNIX\n            assert unix_dg.extra(SocketAttribute.local_address) == str(socket_path)\n            assert unix_dg.extra(SocketAttribute.remote_address) == str(\n                peer_socket_path\n            )\n            pytest.raises(\n                TypedAttributeLookupError, unix_dg.extra, SocketAttribute.local_port\n            )\n            pytest.raises(\n                TypedAttributeLookupError, unix_dg.extra, SocketAttribute.remote_port\n            )\n\n    async def test_send_receive(\n        self,\n        socket_path_or_str: Path | str,\n        peer_socket_path_or_str: Path | str,\n    ) -> None:\n        async with await create_unix_datagram_socket(\n            local_path=peer_socket_path_or_str,\n        ) as unix_dg1:\n            async with await create_connected_unix_datagram_socket(\n                peer_socket_path_or_str,\n                local_path=socket_path_or_str,\n            ) as unix_dg2:\n                socket_path = str(socket_path_or_str)\n\n                await unix_dg2.send(b\"blah\")\n                request = await unix_dg1.receive()\n                assert request == (b\"blah\", socket_path)\n\n                await unix_dg1.sendto(b\"halb\", socket_path)\n                response = await unix_dg2.receive()\n                assert response == b\"halb\"\n\n    async def test_iterate(\n        self,\n        socket_path: Path,\n        peer_socket_path: Path,\n    ) -> None:\n        async def serve() -> None:\n            async for packet in unix_dg2:\n                await unix_dg2.send(packet[::-1])\n\n        async with await create_unix_datagram_socket(\n            local_path=peer_socket_path,\n        ) as unix_dg1:\n            async with await create_connected_unix_datagram_socket(\n                peer_socket_path, local_path=socket_path\n            ) as unix_dg2:\n                path = str(socket_path)\n                async with create_task_group() as tg:\n                    tg.start_soon(serve)\n                    await unix_dg1.sendto(b\"FOOBAR\", path)\n                    assert await unix_dg1.receive() == (b\"RABOOF\", path)\n                    await unix_dg1.sendto(b\"123456\", path)\n                    assert await unix_dg1.receive() == (b\"654321\", path)\n                    tg.cancel_scope.cancel()\n\n    async def test_concurrent_receive(\n        self, peer_socket_path: Path, peer_sock: socket.socket\n    ) -> None:\n        async with await create_connected_unix_datagram_socket(\n            peer_socket_path\n        ) as unix_dg:\n            async with create_task_group() as tg:\n                tg.start_soon(unix_dg.receive)\n                await wait_all_tasks_blocked()\n                try:\n                    with pytest.raises(BusyResourceError) as exc:\n                        await unix_dg.receive()\n\n                    exc.match(\"already reading from\")\n                finally:\n                    tg.cancel_scope.cancel()\n\n    async def test_close_during_receive(\n        self, peer_socket_path_or_str: Path | str, peer_sock: socket.socket\n    ) -> None:\n        async def close_when_blocked() -> None:\n            await wait_all_tasks_blocked()\n            await udp.aclose()\n\n        async with await create_connected_unix_datagram_socket(\n            peer_socket_path_or_str\n        ) as udp:\n            async with create_task_group() as tg:\n                tg.start_soon(close_when_blocked)\n                with pytest.raises(ClosedResourceError):\n                    await udp.receive()\n\n    async def test_receive_after_close(\n        self, peer_socket_path_or_str: Path | str, peer_sock: socket.socket\n    ) -> None:\n        udp = await create_connected_unix_datagram_socket(peer_socket_path_or_str)\n        await udp.aclose()\n        with pytest.raises(ClosedResourceError):\n            await udp.receive()\n\n    async def test_send_after_close(\n        self, peer_socket_path_or_str: Path | str, peer_sock: socket.socket\n    ) -> None:\n        udp = await create_connected_unix_datagram_socket(peer_socket_path_or_str)\n        await udp.aclose()\n        with pytest.raises(ClosedResourceError):\n            await udp.send(b\"foo\")\n\n\n@pytest.mark.network\nasync def test_getaddrinfo() -> None:\n    # IDNA 2003 gets this wrong\n    correct = await getaddrinfo(\"fa\u00df.de\", 0)\n    wrong = await getaddrinfo(\"fass.de\", 0)\n    assert correct != wrong\n\n\n@pytest.mark.parametrize(\n    \"sock_type\", [socket.SOCK_STREAM, socket.SocketKind.SOCK_STREAM]\n)\nasync def test_getaddrinfo_ipv6addr(\n    sock_type: Literal[socket.SocketKind.SOCK_STREAM],\n) -> None:\n    # IDNA trips up over raw IPv6 addresses\n    proto = 0 if platform.system() == \"Windows\" else 6\n    assert await getaddrinfo(\"::1\", 0, type=sock_type) == [\n        (\n            socket.AddressFamily.AF_INET6,\n            socket.SocketKind.SOCK_STREAM,\n            proto,\n            \"\",\n            (\"::1\", 0),\n        )\n    ]\n\n\nasync def test_getnameinfo() -> None:\n    expected_result = socket.getnameinfo((\"127.0.0.1\", 6666), 0)\n    result = await getnameinfo((\"127.0.0.1\", 6666))\n    assert result == expected_result\n", "tests/test_typedattr.py": "from __future__ import annotations\n\nfrom typing import Any, Callable, Mapping\n\nimport pytest\n\nfrom anyio import TypedAttributeProvider\n\n\nclass DummyAttributeProvider(TypedAttributeProvider):\n    def get_dummyattr(self) -> str:\n        raise KeyError(\"foo\")\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {str: self.get_dummyattr}\n\n\ndef test_typedattr_keyerror() -> None:\n    \"\"\"\n    Test that if the extra attribute getter raises KeyError, it won't be confused for a\n    missing attribute.\n\n    \"\"\"\n    with pytest.raises(KeyError, match=\"^'foo'$\"):\n        DummyAttributeProvider().extra(str)\n", "tests/test_pytest_plugin.py": "from __future__ import annotations\n\nimport pytest\nfrom _pytest.logging import LogCaptureFixture\nfrom _pytest.pytester import Pytester\n\nfrom anyio import get_all_backends\n\npytestmark = pytest.mark.filterwarnings(\n    \"ignore:The TerminalReporter.writer attribute is deprecated\"\n    \":pytest.PytestDeprecationWarning:\"\n)\n\npytest_args = \"-v\", \"-p\", \"anyio\", \"-p\", \"no:asyncio\", \"-p\", \"no:trio\"\n\n\ndef test_plugin(testdir: Pytester) -> None:\n    testdir.makeconftest(\n        \"\"\"\n        from contextvars import ContextVar\n        import sniffio\n        import pytest\n\n        from anyio import sleep\n\n        var = ContextVar(\"var\")\n\n\n        @pytest.fixture\n        async def async_fixture():\n            await sleep(0)\n            return sniffio.current_async_library()\n\n\n        @pytest.fixture\n        async def context_variable():\n            token = var.set(\"testvalue\")\n            yield var\n            var.reset(token)\n\n\n        @pytest.fixture\n        async def some_feature():\n            yield None\n            await sleep(0)\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import sniffio\n        from hypothesis import strategies, given\n\n        from anyio import get_all_backends, sleep\n\n\n        @pytest.mark.anyio\n        async def test_marked_test() -> None:\n            # Test that tests marked with @pytest.mark.anyio are run\n            pass\n\n        @pytest.mark.anyio\n        async def test_async_fixture_from_marked_test(async_fixture):\n            # Test that async functions can use async fixtures\n            assert async_fixture in get_all_backends()\n\n        def test_async_fixture_from_sync_test(anyio_backend_name, async_fixture):\n            # Test that regular functions can use async fixtures too\n            assert async_fixture == anyio_backend_name\n\n        @pytest.mark.anyio\n        async def test_skip_inline(some_feature):\n            # Test for github #214\n            pytest.skip(\"Test that skipping works\")\n\n        @pytest.mark.anyio\n        async def test_contextvar(context_variable):\n            # Test that a contextvar set in an async fixture is visible to the test\n            assert context_variable.get() == \"testvalue\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest(*pytest_args)\n    result.assert_outcomes(\n        passed=4 * len(get_all_backends()), skipped=len(get_all_backends())\n    )\n\n\ndef test_asyncio(testdir: Pytester, caplog: LogCaptureFixture) -> None:\n    testdir.makeconftest(\n        \"\"\"\n        import asyncio\n        import pytest\n        import threading\n\n\n        @pytest.fixture(scope='class')\n        def anyio_backend():\n            return 'asyncio'\n\n        @pytest.fixture\n        async def setup_fail_fixture():\n            def callback():\n                raise RuntimeError('failing fixture setup')\n\n            asyncio.get_running_loop().call_soon(callback)\n            await asyncio.sleep(0)\n            yield None\n\n        @pytest.fixture\n        async def teardown_fail_fixture():\n            def callback():\n                raise RuntimeError('failing fixture teardown')\n\n            yield None\n            asyncio.get_running_loop().call_soon(callback)\n            await asyncio.sleep(0)\n\n        @pytest.fixture\n        def no_thread_leaks_fixture():\n            # this has to be non-async fixture so that it wraps up\n            # after the event loop gets closed\n            threads_before = threading.enumerate()\n            yield\n            threads_after = threading.enumerate()\n            leaked_threads = set(threads_after) - set(threads_before)\n            assert not leaked_threads\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import asyncio\n\n        import pytest\n\n        pytestmark = pytest.mark.anyio\n\n\n        class TestClassFixtures:\n            @pytest.fixture(scope='class')\n            async def async_class_fixture(self, anyio_backend):\n                await asyncio.sleep(0)\n                return anyio_backend\n\n            def test_class_fixture_in_test_method(\n                self,\n                async_class_fixture,\n                anyio_backend_name\n            ):\n                assert anyio_backend_name == 'asyncio'\n                assert async_class_fixture == 'asyncio'\n\n        async def test_callback_exception_during_test() -> None:\n            def callback():\n                nonlocal started\n                started = True\n                raise Exception('foo')\n\n            started = False\n            asyncio.get_running_loop().call_soon(callback)\n            await asyncio.sleep(0)\n            assert started\n\n        async def test_callback_exception_during_setup(setup_fail_fixture):\n            pass\n\n        async def test_callback_exception_during_teardown(teardown_fail_fixture):\n            pass\n\n        async def test_exception_handler_no_exception():\n            asyncio.get_event_loop().call_exception_handler(\n                {\"message\": \"bogus error\"}\n            )\n            await asyncio.sleep(0.1)\n\n        async def test_shutdown_default_executor(no_thread_leaks_fixture):\n            # Test for github #503\n            asyncio.get_event_loop().run_in_executor(None, lambda: 1)\n        \"\"\"\n    )\n\n    result = testdir.runpytest(*pytest_args)\n    result.assert_outcomes(passed=4, failed=1, errors=2)\n    assert len(caplog.messages) == 1\n    assert caplog.messages[0] == \"bogus error\"\n\n\ndef test_autouse_async_fixture(testdir: Pytester) -> None:\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        autouse_backend = None\n\n\n        @pytest.fixture(autouse=True)\n        async def autouse_async_fixture(anyio_backend_name):\n            global autouse_backend\n            autouse_backend = anyio_backend_name\n\n        @pytest.fixture\n        def autouse_backend_name():\n            return autouse_backend\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        import sniffio\n        from anyio import get_all_backends, sleep\n\n\n        def test_autouse_backend(autouse_backend_name):\n            # Test that async autouse fixtures are triggered\n            assert autouse_backend_name in get_all_backends()\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(*pytest_args)\n    result.assert_outcomes(passed=len(get_all_backends()))\n\n\ndef test_cancel_scope_in_asyncgen_fixture(testdir: Pytester) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        from anyio import create_task_group, sleep\n\n\n        @pytest.fixture\n        async def asyncgen_fixture():\n            async with create_task_group() as tg:\n                tg.cancel_scope.cancel()\n                await sleep(1)\n\n            yield 1\n\n\n        @pytest.mark.anyio\n        async def test_cancel_in_asyncgen_fixture(asyncgen_fixture):\n            assert asyncgen_fixture == 1\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(*pytest_args)\n    result.assert_outcomes(passed=len(get_all_backends()))\n\n\ndef test_module_scoped_task_group_fixture(testdir: Pytester) -> None:\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        from anyio import (\n            CancelScope,\n            create_memory_object_stream,\n            create_task_group,\n            get_all_backends,\n        )\n\n\n        @pytest.fixture(scope=\"module\", params=get_all_backends())\n        def anyio_backend():\n            return 'asyncio'\n\n\n        @pytest.fixture(scope=\"module\")\n        async def task_group():\n            async with create_task_group() as tg:\n                yield tg\n\n\n        @pytest.fixture\n        async def streams(task_group):\n            async def echo_messages(*, task_status):\n                with CancelScope() as cancel_scope:\n                    task_status.started(cancel_scope)\n                    async for obj in receive1:\n                        await send2.send(obj)\n\n            send1, receive1 = create_memory_object_stream()\n            send2, receive2 = create_memory_object_stream()\n            cancel_scope = await task_group.start(echo_messages)\n            yield send1, receive2\n            cancel_scope.cancel()\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n\n        @pytest.mark.anyio\n        async def test_task_group(streams):\n            send1, receive2 = streams\n            await send1.send(\"hello\")\n            assert await receive2.receive() == \"hello\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(*pytest_args)\n    result.assert_outcomes(passed=len(get_all_backends()))\n\n\ndef test_async_fixture_teardown_after_sync_test(testdir: Pytester) -> None:\n    # Regression test for #619\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        from anyio import create_task_group, sleep\n\n        @pytest.fixture(scope=\"session\")\n        def anyio_backend():\n            return \"asyncio\"\n\n\n        @pytest.fixture(scope=\"module\")\n        async def bbbbbb():\n            yield \"\"\n\n\n        @pytest.fixture(scope=\"module\")\n        async def aaaaaa():\n            yield \"\"\n\n\n        @pytest.mark.anyio\n        async def test_1(bbbbbb):\n            pass\n\n\n        @pytest.mark.anyio\n        async def test_2(aaaaaa, bbbbbb):\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(*pytest_args)\n    result.assert_outcomes(passed=2)\n\n\ndef test_hypothesis_module_mark(testdir: Pytester) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from hypothesis import given\n        from hypothesis.strategies import just\n\n        pytestmark = pytest.mark.anyio\n\n\n        @given(x=just(1))\n        async def test_hypothesis_wrapper(x):\n            assert isinstance(x, int)\n\n\n        @given(x=just(1))\n        def test_hypothesis_wrapper_regular(x):\n            assert isinstance(x, int)\n\n\n        @pytest.mark.xfail(strict=True)\n        @given(x=just(1))\n        async def test_hypothesis_wrapper_failing(x):\n            pytest.fail('This test failed successfully')\n        \"\"\"\n    )\n\n    result = testdir.runpytest(*pytest_args)\n    result.assert_outcomes(\n        passed=len(get_all_backends()) + 1, xfailed=len(get_all_backends())\n    )\n\n\ndef test_hypothesis_function_mark(testdir: Pytester) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from hypothesis import given\n        from hypothesis.strategies import just\n\n\n        @pytest.mark.anyio\n        @given(x=just(1))\n        async def test_anyio_mark_first(x):\n            assert isinstance(x, int)\n\n\n        @given(x=just(1))\n        @pytest.mark.anyio\n        async def test_anyio_mark_last(x):\n            assert isinstance(x, int)\n\n\n        @pytest.mark.xfail(strict=True)\n        @pytest.mark.anyio\n        @given(x=just(1))\n        async def test_anyio_mark_first_fail(x):\n            pytest.fail('This test failed successfully')\n\n\n        @given(x=just(1))\n        @pytest.mark.xfail(strict=True)\n        @pytest.mark.anyio\n        async def test_anyio_mark_last_fail(x):\n            pytest.fail('This test failed successfully')\n        \"\"\"\n    )\n\n    result = testdir.runpytest(*pytest_args)\n    result.assert_outcomes(\n        passed=2 * len(get_all_backends()), xfailed=2 * len(get_all_backends())\n    )\n", "tests/test_synchronization.py": "from __future__ import annotations\n\nimport asyncio\nfrom typing import Any\n\nimport pytest\n\nfrom anyio import (\n    CancelScope,\n    Condition,\n    Event,\n    Lock,\n    Semaphore,\n    WouldBlock,\n    create_task_group,\n    fail_after,\n    run,\n    to_thread,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import CapacityLimiter, TaskStatus\n\npytestmark = pytest.mark.anyio\n\n\nclass TestLock:\n    async def test_contextmanager(self) -> None:\n        async def task() -> None:\n            assert lock.locked()\n            async with lock:\n                results.append(\"2\")\n\n        results = []\n        lock = Lock()\n        async with create_task_group() as tg:\n            async with lock:\n                tg.start_soon(task)\n                await wait_all_tasks_blocked()\n                results.append(\"1\")\n\n        assert not lock.locked()\n        assert results == [\"1\", \"2\"]\n\n    async def test_manual_acquire(self) -> None:\n        async def task() -> None:\n            assert lock.locked()\n            await lock.acquire()\n            try:\n                results.append(\"2\")\n            finally:\n                lock.release()\n\n        results = []\n        lock = Lock()\n        async with create_task_group() as tg:\n            await lock.acquire()\n            try:\n                tg.start_soon(task)\n                await wait_all_tasks_blocked()\n                results.append(\"1\")\n            finally:\n                lock.release()\n\n        assert not lock.locked()\n        assert results == [\"1\", \"2\"]\n\n    async def test_acquire_nowait(self) -> None:\n        lock = Lock()\n        lock.acquire_nowait()\n        assert lock.locked()\n\n    async def test_acquire_nowait_wouldblock(self) -> None:\n        async def try_lock() -> None:\n            pytest.raises(WouldBlock, lock.acquire_nowait)\n\n        lock = Lock()\n        async with lock, create_task_group() as tg:\n            assert lock.locked()\n            tg.start_soon(try_lock)\n\n    @pytest.mark.parametrize(\n        \"release_first\",\n        [pytest.param(False, id=\"releaselast\"), pytest.param(True, id=\"releasefirst\")],\n    )\n    async def test_cancel_during_acquire(self, release_first: bool) -> None:\n        acquired = False\n\n        async def task(*, task_status: TaskStatus) -> None:\n            nonlocal acquired\n            task_status.started()\n            async with lock:\n                acquired = True\n\n        lock = Lock()\n        async with create_task_group() as tg:\n            await lock.acquire()\n            await tg.start(task)\n            tg.cancel_scope.cancel()\n            with CancelScope(shield=True):\n                if release_first:\n                    lock.release()\n                    await wait_all_tasks_blocked()\n                else:\n                    await wait_all_tasks_blocked()\n                    lock.release()\n\n        assert not acquired\n        assert not lock.locked()\n\n    async def test_statistics(self) -> None:\n        async def waiter() -> None:\n            async with lock:\n                pass\n\n        lock = Lock()\n        async with create_task_group() as tg:\n            assert not lock.statistics().locked\n            assert lock.statistics().tasks_waiting == 0\n            async with lock:\n                assert lock.statistics().locked\n                assert lock.statistics().tasks_waiting == 0\n                for i in range(1, 3):\n                    tg.start_soon(waiter)\n                    await wait_all_tasks_blocked()\n                    assert lock.statistics().tasks_waiting == i\n\n        assert not lock.statistics().locked\n        assert lock.statistics().tasks_waiting == 0\n\n    @pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\n    async def test_asyncio_deadlock(self) -> None:\n        \"\"\"Regression test for #398.\"\"\"\n        lock = Lock()\n\n        async def acquire() -> None:\n            async with lock:\n                await asyncio.sleep(0)\n\n        loop = asyncio.get_running_loop()\n        task1 = loop.create_task(acquire())\n        task2 = loop.create_task(acquire())\n        await asyncio.sleep(0)\n        task1.cancel()\n        await asyncio.wait_for(task2, 1)\n\n    def test_instantiate_outside_event_loop(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def use_lock() -> None:\n            async with lock:\n                pass\n\n        lock = Lock()\n        statistics = lock.statistics()\n        assert not statistics.locked\n        assert statistics.owner is None\n        assert statistics.tasks_waiting == 0\n\n        run(use_lock, backend=anyio_backend_name, backend_options=anyio_backend_options)\n\n\nclass TestEvent:\n    async def test_event(self) -> None:\n        async def setter() -> None:\n            assert not event.is_set()\n            event.set()\n\n        event = Event()\n        async with create_task_group() as tg:\n            tg.start_soon(setter)\n            await event.wait()\n\n        assert event.is_set()\n\n    async def test_event_cancel(self) -> None:\n        task_started = event_set = False\n\n        async def task() -> None:\n            nonlocal task_started, event_set\n            task_started = True\n            await event.wait()\n            event_set = True\n\n        event = Event()\n        async with create_task_group() as tg:\n            tg.start_soon(task)\n            tg.cancel_scope.cancel()\n            event.set()\n\n        assert task_started\n        assert not event_set\n\n    async def test_event_wait_before_set_before_cancel(self) -> None:\n        setter_started = waiter_woke = False\n\n        async def setter() -> None:\n            nonlocal setter_started\n            setter_started = True\n            assert not event.is_set()\n            event.set()\n            tg.cancel_scope.cancel()\n\n        event = Event()\n        async with create_task_group() as tg:\n            tg.start_soon(setter)\n            await event.wait()\n            waiter_woke = True\n\n        assert setter_started\n        assert waiter_woke\n\n    async def test_statistics(self) -> None:\n        async def waiter() -> None:\n            await event.wait()\n\n        event = Event()\n        async with create_task_group() as tg:\n            assert event.statistics().tasks_waiting == 0\n            for i in range(1, 3):\n                tg.start_soon(waiter)\n                await wait_all_tasks_blocked()\n                assert event.statistics().tasks_waiting == i\n\n            event.set()\n\n        assert event.statistics().tasks_waiting == 0\n\n    def test_instantiate_outside_event_loop(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def use_event() -> None:\n            event.set()\n            await event.wait()\n\n        event = Event()\n        assert not event.is_set()\n        assert event.statistics().tasks_waiting == 0\n\n        run(\n            use_event, backend=anyio_backend_name, backend_options=anyio_backend_options\n        )\n\n\nclass TestCondition:\n    async def test_contextmanager(self) -> None:\n        async def notifier() -> None:\n            async with condition:\n                condition.notify_all()\n\n        condition = Condition()\n        async with create_task_group() as tg:\n            async with condition:\n                assert condition.locked()\n                tg.start_soon(notifier)\n                await condition.wait()\n\n    async def test_manual_acquire(self) -> None:\n        async def notifier() -> None:\n            await condition.acquire()\n            try:\n                condition.notify_all()\n            finally:\n                condition.release()\n\n        condition = Condition()\n        async with create_task_group() as tg:\n            await condition.acquire()\n            try:\n                assert condition.locked()\n                tg.start_soon(notifier)\n                await condition.wait()\n            finally:\n                condition.release()\n\n    async def test_acquire_nowait(self) -> None:\n        condition = Condition()\n        condition.acquire_nowait()\n        assert condition.locked()\n\n    async def test_acquire_nowait_wouldblock(self) -> None:\n        async def try_lock() -> None:\n            pytest.raises(WouldBlock, condition.acquire_nowait)\n\n        condition = Condition()\n        async with condition, create_task_group() as tg:\n            assert condition.locked()\n            tg.start_soon(try_lock)\n\n    async def test_wait_cancel(self) -> None:\n        task_started = notified = False\n\n        async def task() -> None:\n            nonlocal task_started, notified\n            task_started = True\n            async with condition:\n                event.set()\n                await condition.wait()\n                notified = True\n\n        event = Event()\n        condition = Condition()\n        async with create_task_group() as tg:\n            tg.start_soon(task)\n            await event.wait()\n            await wait_all_tasks_blocked()\n            tg.cancel_scope.cancel()\n\n        assert task_started\n        assert not notified\n\n    async def test_statistics(self) -> None:\n        async def waiter() -> None:\n            async with condition:\n                await condition.wait()\n\n        condition = Condition()\n        async with create_task_group() as tg:\n            assert not condition.statistics().lock_statistics.locked\n            assert condition.statistics().tasks_waiting == 0\n            async with condition:\n                assert condition.statistics().lock_statistics.locked\n                assert condition.statistics().tasks_waiting == 0\n\n            for i in range(1, 3):\n                tg.start_soon(waiter)\n                await wait_all_tasks_blocked()\n                assert condition.statistics().tasks_waiting == i\n\n            for i in range(1, -1, -1):\n                async with condition:\n                    condition.notify(1)\n\n                await wait_all_tasks_blocked()\n                assert condition.statistics().tasks_waiting == i\n\n        assert not condition.statistics().lock_statistics.locked\n        assert condition.statistics().tasks_waiting == 0\n\n    def test_instantiate_outside_event_loop(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def use_condition() -> None:\n            async with condition:\n                pass\n\n        condition = Condition()\n        assert condition.statistics().tasks_waiting == 0\n\n        run(\n            use_condition,\n            backend=anyio_backend_name,\n            backend_options=anyio_backend_options,\n        )\n\n\nclass TestSemaphore:\n    async def test_contextmanager(self) -> None:\n        async def acquire() -> None:\n            async with semaphore:\n                assert semaphore.value in (0, 1)\n\n        semaphore = Semaphore(2)\n        async with create_task_group() as tg:\n            tg.start_soon(acquire, name=\"task 1\")\n            tg.start_soon(acquire, name=\"task 2\")\n\n        assert semaphore.value == 2\n\n    async def test_manual_acquire(self) -> None:\n        async def acquire() -> None:\n            await semaphore.acquire()\n            try:\n                assert semaphore.value in (0, 1)\n            finally:\n                semaphore.release()\n\n        semaphore = Semaphore(2)\n        async with create_task_group() as tg:\n            tg.start_soon(acquire, name=\"task 1\")\n            tg.start_soon(acquire, name=\"task 2\")\n\n        assert semaphore.value == 2\n\n    async def test_acquire_nowait(self) -> None:\n        semaphore = Semaphore(1)\n        semaphore.acquire_nowait()\n        assert semaphore.value == 0\n        pytest.raises(WouldBlock, semaphore.acquire_nowait)\n\n    @pytest.mark.parametrize(\n        \"release_first\",\n        [pytest.param(False, id=\"releaselast\"), pytest.param(True, id=\"releasefirst\")],\n    )\n    async def test_cancel_during_acquire(self, release_first: bool) -> None:\n        acquired = False\n\n        async def task(*, task_status: TaskStatus) -> None:\n            nonlocal acquired\n            task_status.started()\n            async with semaphore:\n                acquired = True\n\n        semaphore = Semaphore(1)\n        async with create_task_group() as tg:\n            await semaphore.acquire()\n            await tg.start(task)\n            tg.cancel_scope.cancel()\n            with CancelScope(shield=True):\n                if release_first:\n                    semaphore.release()\n                    await wait_all_tasks_blocked()\n                else:\n                    await wait_all_tasks_blocked()\n                    semaphore.release()\n\n        assert not acquired\n        assert semaphore.value == 1\n\n    @pytest.mark.parametrize(\"max_value\", [2, None])\n    async def test_max_value(self, max_value: int | None) -> None:\n        semaphore = Semaphore(0, max_value=max_value)\n        assert semaphore.max_value == max_value\n\n    async def test_max_value_exceeded(self) -> None:\n        semaphore = Semaphore(1, max_value=2)\n        semaphore.release()\n        pytest.raises(ValueError, semaphore.release)\n\n    async def test_statistics(self) -> None:\n        async def waiter() -> None:\n            async with semaphore:\n                pass\n\n        semaphore = Semaphore(1)\n        async with create_task_group() as tg:\n            assert semaphore.statistics().tasks_waiting == 0\n            async with semaphore:\n                assert semaphore.statistics().tasks_waiting == 0\n                for i in range(1, 3):\n                    tg.start_soon(waiter)\n                    await wait_all_tasks_blocked()\n                    assert semaphore.statistics().tasks_waiting == i\n\n        assert semaphore.statistics().tasks_waiting == 0\n\n    async def test_acquire_race(self) -> None:\n        \"\"\"\n        Test against a race condition: when a task waiting on acquire() is rescheduled\n        but another task snatches the last available slot, the task should not raise\n        WouldBlock.\n\n        \"\"\"\n        semaphore = Semaphore(1)\n        async with create_task_group() as tg:\n            semaphore.acquire_nowait()\n            tg.start_soon(semaphore.acquire)\n            await wait_all_tasks_blocked()\n            semaphore.release()\n            pytest.raises(WouldBlock, semaphore.acquire_nowait)\n\n    @pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\n    async def test_asyncio_deadlock(self) -> None:\n        \"\"\"Regression test for #398.\"\"\"\n        semaphore = Semaphore(1)\n\n        async def acquire() -> None:\n            async with semaphore:\n                await asyncio.sleep(0)\n\n        loop = asyncio.get_running_loop()\n        task1 = loop.create_task(acquire())\n        task2 = loop.create_task(acquire())\n        await asyncio.sleep(0)\n        task1.cancel()\n        await asyncio.wait_for(task2, 1)\n\n    def test_instantiate_outside_event_loop(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def use_semaphore() -> None:\n            async with semaphore:\n                pass\n\n        semaphore = Semaphore(1)\n        assert semaphore.statistics().tasks_waiting == 0\n\n        run(\n            use_semaphore,\n            backend=anyio_backend_name,\n            backend_options=anyio_backend_options,\n        )\n\n\nclass TestCapacityLimiter:\n    async def test_bad_init_type(self) -> None:\n        pytest.raises(TypeError, CapacityLimiter, 1.0).match(\n            \"total_tokens must be an int or math.inf\"\n        )\n\n    async def test_bad_init_value(self) -> None:\n        pytest.raises(ValueError, CapacityLimiter, 0).match(\"total_tokens must be >= 1\")\n\n    async def test_borrow(self) -> None:\n        limiter = CapacityLimiter(2)\n        assert limiter.total_tokens == 2\n        assert limiter.available_tokens == 2\n        assert limiter.borrowed_tokens == 0\n        async with limiter:\n            assert limiter.total_tokens == 2\n            assert limiter.available_tokens == 1\n            assert limiter.borrowed_tokens == 1\n\n    async def test_limit(self) -> None:\n        value = 0\n\n        async def taskfunc() -> None:\n            nonlocal value\n            for _ in range(5):\n                async with limiter:\n                    assert value == 0\n                    value = 1\n                    await wait_all_tasks_blocked()\n                    value = 0\n\n        limiter = CapacityLimiter(1)\n        async with create_task_group() as tg:\n            for _ in range(3):\n                tg.start_soon(taskfunc)\n\n    async def test_borrow_twice(self) -> None:\n        limiter = CapacityLimiter(1)\n        await limiter.acquire()\n        with pytest.raises(RuntimeError) as exc:\n            await limiter.acquire()\n\n        exc.match(\n            \"this borrower is already holding one of this CapacityLimiter's tokens\"\n        )\n\n    async def test_bad_release(self) -> None:\n        limiter = CapacityLimiter(1)\n        with pytest.raises(RuntimeError) as exc:\n            limiter.release()\n\n        exc.match(\"this borrower isn't holding any of this CapacityLimiter's tokens\")\n\n    async def test_increase_tokens(self) -> None:\n        async def setter() -> None:\n            # Wait until waiter() is inside the limiter block\n            await event1.wait()\n            async with limiter:\n                # This can only happen when total_tokens has been increased\n                event2.set()\n\n        async def waiter() -> None:\n            async with limiter:\n                event1.set()\n                await event2.wait()\n\n        limiter = CapacityLimiter(1)\n        event1, event2 = Event(), Event()\n        async with create_task_group() as tg:\n            tg.start_soon(setter)\n            tg.start_soon(waiter)\n            await wait_all_tasks_blocked()\n            assert event1.is_set()\n            assert not event2.is_set()\n            limiter.total_tokens = 2\n\n        assert event2.is_set()\n\n    async def test_current_default_thread_limiter(self) -> None:\n        limiter = to_thread.current_default_thread_limiter()\n        assert isinstance(limiter, CapacityLimiter)\n        assert limiter.total_tokens == 40\n\n    async def test_statistics(self) -> None:\n        async def waiter() -> None:\n            async with limiter:\n                pass\n\n        limiter = CapacityLimiter(1)\n        assert limiter.statistics().total_tokens == 1\n        assert limiter.statistics().borrowed_tokens == 0\n        assert limiter.statistics().tasks_waiting == 0\n        async with create_task_group() as tg:\n            async with limiter:\n                assert limiter.statistics().borrowed_tokens == 1\n                assert limiter.statistics().tasks_waiting == 0\n                for i in range(1, 3):\n                    tg.start_soon(waiter)\n                    await wait_all_tasks_blocked()\n                    assert limiter.statistics().tasks_waiting == i\n\n        assert limiter.statistics().tasks_waiting == 0\n        assert limiter.statistics().borrowed_tokens == 0\n\n    @pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\n    async def test_asyncio_deadlock(self) -> None:\n        \"\"\"Regression test for #398.\"\"\"\n        limiter = CapacityLimiter(1)\n\n        async def acquire() -> None:\n            async with limiter:\n                await asyncio.sleep(0)\n\n        loop = asyncio.get_running_loop()\n        task1 = loop.create_task(acquire())\n        task2 = loop.create_task(acquire())\n        await asyncio.sleep(0)\n        task1.cancel()\n        await asyncio.wait_for(task2, 1)\n\n    async def test_ordered_queue(self) -> None:\n        limiter = CapacityLimiter(1)\n        results = []\n        event = Event()\n\n        async def append(x: int, task_status: TaskStatus) -> None:\n            task_status.started()\n            async with limiter:\n                await event.wait()\n                results.append(x)\n\n        async with create_task_group() as tg:\n            for i in [0, 1, 2]:\n                await tg.start(append, i)\n\n            event.set()\n\n        assert results == [0, 1, 2]\n\n    async def test_increase_tokens_lets_others_acquire(self) -> None:\n        limiter = CapacityLimiter(1)\n        entered_events = [Event() for _ in range(3)]\n        continue_event = Event()\n\n        async def worker(entered_event: Event) -> None:\n            async with limiter:\n                entered_event.set()\n                await continue_event.wait()\n\n        async with create_task_group() as tg:\n            for event in entered_events[:2]:\n                tg.start_soon(worker, event)\n\n            # One task should be able to acquire the limiter while the other is left\n            # waiting\n            await wait_all_tasks_blocked()\n            assert sum(ev.is_set() for ev in entered_events) == 1\n\n            # Increase the total tokens and start another worker.\n            # All tasks should be able to acquire the limiter now.\n            limiter.total_tokens = 3\n            tg.start_soon(worker, entered_events[2])\n            with fail_after(1):\n                for ev in entered_events[1:]:\n                    await ev.wait()\n\n            # Allow all tasks to exit\n            continue_event.set()\n\n    def test_instantiate_outside_event_loop(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def use_limiter() -> None:\n            async with limiter:\n                pass\n\n        limiter = CapacityLimiter(1)\n        limiter.total_tokens = 2\n\n        with pytest.raises(TypeError):\n            limiter.total_tokens = \"2\"  # type: ignore[assignment]\n\n        with pytest.raises(TypeError):\n            limiter.total_tokens = 3.0\n\n        assert limiter.total_tokens == 2\n        assert limiter.borrowed_tokens == 0\n        statistics = limiter.statistics()\n        assert statistics.total_tokens == 2\n        assert statistics.borrowed_tokens == 0\n        assert statistics.borrowers == ()\n        assert statistics.tasks_waiting == 0\n\n        run(\n            use_limiter,\n            backend=anyio_backend_name,\n            backend_options=anyio_backend_options,\n        )\n\n    async def test_total_tokens_as_kwarg(self) -> None:\n        # Regression test for #515\n        limiter = CapacityLimiter(total_tokens=1)\n        assert limiter.total_tokens == 1\n", "tests/conftest.py": "from __future__ import annotations\n\nimport asyncio\nimport ssl\nfrom collections.abc import Generator\nfrom ssl import SSLContext\nfrom typing import Any\nfrom unittest.mock import Mock\n\nimport pytest\nimport trustme\nfrom _pytest.fixtures import SubRequest\nfrom trustme import CA\n\nuvloop_marks = []\ntry:\n    import uvloop\nexcept ImportError:\n    uvloop_marks.append(pytest.mark.skip(reason=\"uvloop not available\"))\n    uvloop = Mock()\nelse:\n    if hasattr(asyncio.AbstractEventLoop, \"shutdown_default_executor\") and not hasattr(\n        uvloop.loop.Loop, \"shutdown_default_executor\"\n    ):\n        uvloop_marks.append(\n            pytest.mark.skip(reason=\"uvloop is missing shutdown_default_executor()\")\n        )\n\npytest_plugins = [\"pytester\", \"pytest_mock\"]\n\n\n@pytest.fixture(\n    params=[\n        pytest.param(\n            (\"asyncio\", {\"debug\": True, \"loop_factory\": None}),\n            id=\"asyncio\",\n        ),\n        pytest.param(\n            (\"asyncio\", {\"debug\": True, \"loop_factory\": uvloop.new_event_loop}),\n            marks=uvloop_marks,\n            id=\"asyncio+uvloop\",\n        ),\n        pytest.param(\"trio\"),\n    ]\n)\ndef anyio_backend(request: SubRequest) -> tuple[str, dict[str, Any]]:\n    return request.param\n\n\n@pytest.fixture(scope=\"session\")\ndef ca() -> CA:\n    return trustme.CA()\n\n\n@pytest.fixture(scope=\"session\")\ndef server_context(ca: CA) -> SSLContext:\n    server_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    if hasattr(ssl, \"OP_IGNORE_UNEXPECTED_EOF\"):\n        server_context.options &= ~ssl.OP_IGNORE_UNEXPECTED_EOF\n\n    ca.issue_cert(\"localhost\").configure_cert(server_context)\n    return server_context\n\n\n@pytest.fixture(scope=\"session\")\ndef client_context(ca: CA) -> SSLContext:\n    client_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n    if hasattr(ssl, \"OP_IGNORE_UNEXPECTED_EOF\"):\n        client_context.options &= ~ssl.OP_IGNORE_UNEXPECTED_EOF\n\n    ca.configure_trust(client_context)\n    return client_context\n\n\n@pytest.fixture\ndef asyncio_event_loop() -> Generator[asyncio.AbstractEventLoop, None, None]:\n    loop = asyncio.DefaultEventLoopPolicy().new_event_loop()\n    asyncio.set_event_loop(loop)\n    yield loop\n    asyncio.set_event_loop(None)\n    loop.close()\n", "tests/test_to_process.py": "from __future__ import annotations\n\nimport os\nimport sys\nimport time\nfrom functools import partial\nfrom pathlib import Path\nfrom unittest.mock import Mock\n\nimport pytest\nfrom pytest import MonkeyPatch\n\nfrom anyio import (\n    CancelScope,\n    create_task_group,\n    fail_after,\n    to_process,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import Process\n\npytestmark = pytest.mark.anyio\n\n\nasync def test_run_sync_in_process_pool() -> None:\n    \"\"\"\n    Test that the function runs in a different process, and the same process in both\n    calls.\n\n    \"\"\"\n    worker_pid = await to_process.run_sync(os.getpid)\n    assert worker_pid != os.getpid()\n    assert await to_process.run_sync(os.getpid) == worker_pid\n\n\nasync def test_identical_sys_path() -> None:\n    \"\"\"Test that partial() can be used to pass keyword arguments.\"\"\"\n    assert await to_process.run_sync(eval, \"sys.path\") == sys.path\n\n\nasync def test_partial() -> None:\n    \"\"\"Test that partial() can be used to pass keyword arguments.\"\"\"\n    assert await to_process.run_sync(partial(sorted, reverse=True), [\"a\", \"b\"]) == [\n        \"b\",\n        \"a\",\n    ]\n\n\nasync def test_exception() -> None:\n    \"\"\"Test that exceptions are delivered properly.\"\"\"\n    with pytest.raises(ValueError, match=\"invalid literal for int\"):\n        assert await to_process.run_sync(int, \"a\")\n\n\nasync def test_print() -> None:\n    \"\"\"Test that print() won't interfere with parent-worker communication.\"\"\"\n    worker_pid = await to_process.run_sync(os.getpid)\n    await to_process.run_sync(print, \"hello\")\n    await to_process.run_sync(print, \"world\")\n    assert await to_process.run_sync(os.getpid) == worker_pid\n\n\nasync def test_cancel_before() -> None:\n    \"\"\"\n    Test that starting to_process.run_sync() in a cancelled scope does not cause a\n    worker process to be reserved.\n\n    \"\"\"\n    with CancelScope() as scope:\n        scope.cancel()\n        await to_process.run_sync(os.getpid)\n\n    pytest.raises(LookupError, to_process._process_pool_workers.get)\n\n\nasync def test_cancel_during() -> None:\n    \"\"\"\n    Test that cancelling an operation on the worker process causes the process to be\n    killed.\n\n    \"\"\"\n    worker_pid = await to_process.run_sync(os.getpid)\n    with fail_after(4):\n        async with create_task_group() as tg:\n            tg.start_soon(partial(to_process.run_sync, cancellable=True), time.sleep, 5)\n            await wait_all_tasks_blocked()\n            tg.cancel_scope.cancel()\n\n    # The previous worker was killed so we should get a new one now\n    assert await to_process.run_sync(os.getpid) != worker_pid\n\n\nasync def test_exec_while_pruning() -> None:\n    \"\"\"\n    Test that in the case when one or more idle workers are pruned, the originally\n    selected idle worker is re-added to the queue of idle workers.\n    \"\"\"\n\n    worker_pid1 = await to_process.run_sync(os.getpid)\n    workers = to_process._process_pool_workers.get()\n    idle_workers = to_process._process_pool_idle_workers.get()\n    real_worker = next(iter(workers))\n\n    fake_idle_process = Mock(Process)\n    workers.add(fake_idle_process)\n    try:\n        # Add a mock worker process that's guaranteed to be eligible for pruning\n        idle_workers.appendleft(\n            (fake_idle_process, -to_process.WORKER_MAX_IDLE_TIME - 1)\n        )\n\n        worker_pid2 = await to_process.run_sync(os.getpid)\n        assert worker_pid1 == worker_pid2\n        fake_idle_process.kill.assert_called_once_with()\n        assert idle_workers[0][0] is real_worker\n    finally:\n        workers.discard(fake_idle_process)\n\n\nasync def test_nonexistent_main_module(\n    monkeypatch: MonkeyPatch, tmp_path: Path\n) -> None:\n    \"\"\"\n    Test that worker process creation won't fail if the detected path to the `__main__`\n    module doesn't exist. Regression test for #696.\n    \"\"\"\n\n    script_path = tmp_path / \"badscript\"\n    script_path.touch()\n    monkeypatch.setattr(\"__main__.__file__\", str(script_path / \"__main__.py\"))\n    await to_process.run_sync(os.getpid)\n", "tests/test_taskgroups.py": "from __future__ import annotations\n\nimport asyncio\nimport math\nimport sys\nimport time\nfrom collections.abc import AsyncGenerator, Coroutine, Generator\nfrom typing import Any, NoReturn, cast\n\nimport pytest\nfrom exceptiongroup import catch\n\nimport anyio\nfrom anyio import (\n    TASK_STATUS_IGNORED,\n    CancelScope,\n    create_task_group,\n    current_effective_deadline,\n    current_time,\n    fail_after,\n    get_cancelled_exc_class,\n    get_current_task,\n    move_on_after,\n    sleep,\n    sleep_forever,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import TaskGroup, TaskStatus\nfrom anyio.lowlevel import checkpoint\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import BaseExceptionGroup, ExceptionGroup\n\npytestmark = pytest.mark.anyio\n\n\nasync def async_error(text: str, delay: float = 0.1) -> NoReturn:\n    try:\n        if delay:\n            await sleep(delay)\n    finally:\n        raise Exception(text)\n\n\nasync def test_already_closed() -> None:\n    async with create_task_group() as tg:\n        pass\n\n    with pytest.raises(RuntimeError) as exc:\n        tg.start_soon(async_error, \"fail\")\n\n    exc.match(\"This task group is not active; no new tasks can be started\")\n\n\nasync def test_success() -> None:\n    async def async_add(value: str) -> None:\n        results.add(value)\n\n    results: set[str] = set()\n    async with create_task_group() as tg:\n        tg.start_soon(async_add, \"a\")\n        tg.start_soon(async_add, \"b\")\n\n    assert results == {\"a\", \"b\"}\n\n\n@pytest.mark.parametrize(\n    \"module\",\n    [\n        pytest.param(asyncio, id=\"asyncio\"),\n        pytest.param(pytest.importorskip(\"trio\"), id=\"trio\"),\n    ],\n)\ndef test_run_natively(module: Any) -> None:\n    async def testfunc() -> None:\n        async with create_task_group() as tg:\n            tg.start_soon(sleep, 0)\n\n    if module is asyncio:\n        asyncio.run(testfunc())\n    else:\n        module.run(testfunc)\n\n\nasync def test_start_soon_while_running() -> None:\n    async def task_func() -> None:\n        tg.start_soon(sleep, 0)\n\n    async with create_task_group() as tg:\n        tg.start_soon(task_func)\n\n\nasync def test_start_soon_after_error() -> None:\n    with pytest.raises(ExceptionGroup):\n        async with create_task_group() as tg:\n            a = 1 / 0  # noqa: F841\n\n    with pytest.raises(RuntimeError) as exc:\n        tg.start_soon(sleep, 0)\n\n    exc.match(\"This task group is not active; no new tasks can be started\")\n\n\nasync def test_start_no_value() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        task_status.started()\n\n    async with create_task_group() as tg:\n        value = await tg.start(taskfunc)\n        assert value is None\n\n\nasync def test_start_called_twice() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        task_status.started()\n\n        with pytest.raises(\n            RuntimeError, match=\"called 'started' twice on the same task status\"\n        ):\n            task_status.started()\n\n    async with create_task_group() as tg:\n        value = await tg.start(taskfunc)\n        assert value is None\n\n\nasync def test_no_called_started_twice() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        task_status.started()\n\n    async with create_task_group() as tg:\n        coro = tg.start(taskfunc)\n        tg.cancel_scope.cancel()\n        await coro\n\n\nasync def test_start_with_value() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        task_status.started(\"foo\")\n\n    async with create_task_group() as tg:\n        value = await tg.start(taskfunc)\n        assert value == \"foo\"\n\n\nasync def test_start_crash_before_started_call() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> NoReturn:\n        raise Exception(\"foo\")\n\n    async with create_task_group() as tg:\n        with pytest.raises(Exception) as exc:\n            await tg.start(taskfunc)\n\n    exc.match(\"foo\")\n\n\nasync def test_start_crash_after_started_call() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> NoReturn:\n        task_status.started(2)\n        raise Exception(\"foo\")\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            value = await tg.start(taskfunc)\n\n    assert len(exc.value.exceptions) == 1\n    assert str(exc.value.exceptions[0]) == \"foo\"\n    assert value == 2\n\n\nasync def test_start_no_started_call() -> None:\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        pass\n\n    async with create_task_group() as tg:\n        with pytest.raises(RuntimeError) as exc:\n            await tg.start(taskfunc)\n\n    exc.match(\"hild exited\")\n\n\nasync def test_start_cancelled() -> None:\n    started = finished = False\n\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        nonlocal started, finished\n        started = True\n        await sleep(2)\n        finished = True\n\n    async with create_task_group() as tg:\n        tg.cancel_scope.cancel()\n        await tg.start(taskfunc)\n\n    assert started\n    assert not finished\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_start_native_host_cancelled() -> None:\n    started = finished = False\n\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        nonlocal started, finished\n        started = True\n        await sleep(2)\n        finished = True\n\n    async def start_another() -> None:\n        async with create_task_group() as tg:\n            await tg.start(taskfunc)\n\n    task = asyncio.get_running_loop().create_task(start_another())\n    await wait_all_tasks_blocked()\n    task.cancel()\n    with pytest.raises(asyncio.CancelledError):\n        await task\n\n    assert started\n    assert not finished\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_start_native_child_cancelled() -> None:\n    task = None\n    finished = False\n\n    async def taskfunc(*, task_status: TaskStatus) -> None:\n        nonlocal task, finished\n        task = asyncio.current_task()\n        await sleep(2)\n        finished = True\n\n    async def start_another() -> None:\n        async with create_task_group() as tg2:\n            await tg2.start(taskfunc)\n\n    async with create_task_group() as tg:\n        tg.start_soon(start_another)\n        await wait_all_tasks_blocked()\n        assert task is not None\n        task.cancel()\n\n    assert not finished\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_propagate_native_cancellation_from_taskgroup() -> None:\n    async def taskfunc() -> None:\n        async with create_task_group() as tg:\n            tg.start_soon(asyncio.sleep, 2)\n\n    task = asyncio.create_task(taskfunc())\n    await wait_all_tasks_blocked()\n    task.cancel()\n    with pytest.raises(asyncio.CancelledError):\n        await task\n\n\nasync def test_start_exception_delivery(anyio_backend_name: str) -> None:\n    def task_fn(*, task_status: TaskStatus = TASK_STATUS_IGNORED) -> None:\n        task_status.started(\"hello\")\n\n    if anyio_backend_name == \"trio\":\n        pattern = \"appears to be synchronous\"\n    else:\n        pattern = \"is not a coroutine object\"\n\n    async with anyio.create_task_group() as tg:\n        with pytest.raises(TypeError, match=pattern):\n            await tg.start(task_fn)  # type: ignore[arg-type]\n\n\nasync def test_start_cancel_after_error() -> None:\n    \"\"\"Regression test for #517.\"\"\"\n    sleep_completed = False\n\n    async def sleep_and_raise() -> None:\n        await wait_all_tasks_blocked()\n        raise RuntimeError(\"This should cancel the second start() call\")\n\n    async def sleep_only(task_status: TaskStatus[None]) -> None:\n        nonlocal sleep_completed\n        await sleep(1)\n        sleep_completed = True\n        task_status.started()\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with anyio.create_task_group() as outer_tg:\n            async with anyio.create_task_group() as inner_tg:\n                inner_tg.start_soon(sleep_and_raise)\n                await outer_tg.start(sleep_only)\n\n    assert isinstance(exc.value.exceptions[0], ExceptionGroup)\n    assert isinstance(exc.value.exceptions[0].exceptions[0], RuntimeError)\n    assert not sleep_completed\n\n\nasync def test_host_exception() -> None:\n    result = None\n\n    async def set_result(value: str) -> None:\n        nonlocal result\n        await sleep(3)\n        result = value\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(set_result, \"a\")\n            raise Exception(\"dummy error\")\n\n    assert len(exc.value.exceptions) == 1\n    assert str(exc.value.exceptions[0]) == \"dummy error\"\n    assert result is None\n\n\nasync def test_level_cancellation() -> None:\n    marker = None\n\n    async def dummy() -> None:\n        nonlocal marker\n        marker = 1\n        # At this point the task has been cancelled so sleep() will raise an exception\n        await sleep(0)\n        # Execution should never get this far\n        marker = 2\n\n    async with create_task_group() as tg:\n        tg.start_soon(dummy)\n        assert marker is None\n        tg.cancel_scope.cancel()\n\n    assert marker == 1\n\n\nasync def test_failing_child_task_cancels_host() -> None:\n    async def child() -> NoReturn:\n        await wait_all_tasks_blocked()\n        raise Exception(\"foo\")\n\n    sleep_completed = False\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(child)\n            await sleep(0.5)\n            sleep_completed = True\n\n    assert len(exc.value.exceptions) == 1\n    assert str(exc.value.exceptions[0]) == \"foo\"\n    assert not sleep_completed\n\n\nasync def test_failing_host_task_cancels_children() -> None:\n    sleep_completed = False\n\n    async def child() -> None:\n        nonlocal sleep_completed\n        await sleep(1)\n        sleep_completed = True\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(child)\n            await wait_all_tasks_blocked()\n            raise Exception(\"foo\")\n\n    assert len(exc.value.exceptions) == 1\n    assert str(exc.value.exceptions[0]) == \"foo\"\n    assert not sleep_completed\n\n\nasync def test_cancel_scope_in_another_task() -> None:\n    local_scope = None\n    result = False\n\n    async def child() -> None:\n        nonlocal result, local_scope\n        with CancelScope() as local_scope:\n            await sleep(2)\n            result = True\n\n    async with create_task_group() as tg:\n        tg.start_soon(child)\n        while local_scope is None:\n            await sleep(0)\n\n        local_scope.cancel()\n\n    assert not result\n\n\nasync def test_cancel_propagation() -> None:\n    async def g() -> NoReturn:\n        async with create_task_group():\n            await sleep(1)\n\n        assert False\n\n    async with create_task_group() as tg:\n        tg.start_soon(g)\n        await sleep(0)\n        tg.cancel_scope.cancel()\n\n\nasync def test_cancel_twice() -> None:\n    \"\"\"Test that the same task can receive two cancellations.\"\"\"\n\n    async def cancel_group() -> None:\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    for _ in range(2):\n        async with create_task_group() as tg:\n            tg.start_soon(cancel_group)\n            await sleep(1)\n            pytest.fail(\"Execution should not reach this point\")\n\n\nasync def test_cancel_exiting_task_group() -> None:\n    \"\"\"\n    Test that if a task group is waiting for subtasks to finish and it receives a\n    cancellation, the subtasks are also cancelled and the waiting continues.\n\n    \"\"\"\n    cancel_received = False\n\n    async def waiter() -> None:\n        nonlocal cancel_received\n        try:\n            await sleep(5)\n        finally:\n            cancel_received = True\n\n    async def subgroup() -> None:\n        async with create_task_group() as tg2:\n            tg2.start_soon(waiter)\n\n    async with create_task_group() as tg:\n        tg.start_soon(subgroup)\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    assert cancel_received\n\n\nasync def test_cancel_before_entering_scope() -> None:\n    \"\"\"\n    Test that CancelScope.cancel() is honored even if called before entering the scope.\n\n    \"\"\"\n    cancel_scope = anyio.CancelScope()\n    cancel_scope.cancel()\n    with cancel_scope:\n        await anyio.sleep(1)  # Checkpoint to allow anyio to check for cancellation\n        pytest.fail(\"execution should not reach this point\")\n\n\n@pytest.mark.xfail(\n    sys.version_info < (3, 11), reason=\"Requires asyncio.Task.cancelling()\"\n)\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_cancel_counter_nested_scopes() -> None:\n    with CancelScope() as root_scope:\n        with CancelScope():\n            root_scope.cancel()\n            await sleep(0.5)\n\n    assert not cast(asyncio.Task, asyncio.current_task()).cancelling()\n\n\nasync def test_exception_group_children() -> None:\n    with pytest.raises(BaseExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(async_error, \"task1\")\n            tg.start_soon(async_error, \"task2\", 0.15)\n\n    assert len(exc.value.exceptions) == 2\n    assert sorted(str(e) for e in exc.value.exceptions) == [\"task1\", \"task2\"]\n\n\nasync def test_exception_group_host() -> None:\n    with pytest.raises(BaseExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(async_error, \"child\", 2)\n            await wait_all_tasks_blocked()\n            raise Exception(\"host\")\n\n    assert len(exc.value.exceptions) == 2\n    assert sorted(str(e) for e in exc.value.exceptions) == [\"child\", \"host\"]\n\n\nasync def test_escaping_cancelled_exception() -> None:\n    async with create_task_group() as tg:\n        tg.cancel_scope.cancel()\n        await sleep(0)\n\n\nasync def test_cancel_scope_cleared() -> None:\n    with move_on_after(0.1):\n        await sleep(1)\n\n    await sleep(0)\n\n\n@pytest.mark.parametrize(\"delay\", [0, 0.1], ids=[\"instant\", \"delayed\"])\nasync def test_fail_after(delay: float) -> None:\n    with pytest.raises(TimeoutError):\n        with fail_after(delay) as scope:\n            await sleep(1)\n\n    assert scope.cancel_called\n    assert scope.cancelled_caught\n\n\nasync def test_fail_after_no_timeout() -> None:\n    with fail_after(None) as scope:\n        assert scope.deadline == float(\"inf\")\n        await sleep(0.1)\n\n    assert not scope.cancel_called\n    assert not scope.cancelled_caught\n\n\nasync def test_fail_after_after_cancellation() -> None:\n    event = anyio.Event()\n    async with anyio.create_task_group() as tg:\n        tg.cancel_scope.cancel()\n        await event.wait()\n\n    block_complete = False\n    with pytest.raises(TimeoutError):\n        with fail_after(0.1):\n            await anyio.sleep(0.5)\n            block_complete = True\n\n    assert not block_complete\n\n\nasync def test_fail_after_cancelled_before_deadline() -> None:\n    \"\"\"\n    Test that fail_after() won't raise TimeoutError if its scope is cancelled before the\n    deadline.\n\n    \"\"\"\n    with fail_after(1) as scope:\n        scope.cancel()\n        await checkpoint()\n\n\n@pytest.mark.xfail(\n    reason=\"There is currently no way to tell if cancellation happened due to timeout \"\n    \"explicitly if the deadline has been exceeded\"\n)\nasync def test_fail_after_scope_cancelled_before_timeout() -> None:\n    with fail_after(0.1) as scope:\n        scope.cancel()\n        time.sleep(0.11)  # noqa: ASYNC101\n        await sleep(0)\n\n\n@pytest.mark.parametrize(\"delay\", [0, 0.1], ids=[\"instant\", \"delayed\"])\nasync def test_move_on_after(delay: float) -> None:\n    result = False\n    with move_on_after(delay) as scope:\n        await sleep(1)\n        result = True\n\n    assert not result\n    assert scope.cancel_called\n    assert scope.cancelled_caught\n\n\nasync def test_move_on_after_no_timeout() -> None:\n    result = False\n    with move_on_after(None) as scope:\n        assert scope.deadline == float(\"inf\")\n        await sleep(0.1)\n        result = True\n\n    assert result\n    assert not scope.cancel_called\n\n\nasync def test_nested_move_on_after() -> None:\n    sleep_completed = inner_scope_completed = False\n    with move_on_after(0.1) as outer_scope:\n        assert current_effective_deadline() == outer_scope.deadline\n        with move_on_after(1) as inner_scope:\n            assert current_effective_deadline() == outer_scope.deadline\n            await sleep(2)\n            sleep_completed = True\n\n        inner_scope_completed = True\n\n    assert not sleep_completed\n    assert not inner_scope_completed\n    assert outer_scope.cancel_called\n    assert outer_scope.cancelled_caught\n    assert not inner_scope.cancel_called\n    assert not inner_scope.cancelled_caught\n\n\nasync def test_shielding() -> None:\n    async def cancel_when_ready() -> None:\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    inner_sleep_completed = outer_sleep_completed = False\n    async with create_task_group() as tg:\n        tg.start_soon(cancel_when_ready)\n        with move_on_after(10, shield=True) as inner_scope:\n            assert inner_scope.shield\n            await sleep(0.1)\n            inner_sleep_completed = True\n\n        await sleep(1)\n        outer_sleep_completed = True\n\n    assert inner_sleep_completed\n    assert not outer_sleep_completed\n    assert tg.cancel_scope.cancel_called\n    assert not inner_scope.cancel_called\n\n\nasync def test_cancel_from_shielded_scope() -> None:\n    async with create_task_group() as tg:\n        with CancelScope(shield=True) as inner_scope:\n            assert inner_scope.shield\n            tg.cancel_scope.cancel()\n            assert current_effective_deadline() == math.inf\n\n        assert current_effective_deadline() == -math.inf\n\n        with pytest.raises(get_cancelled_exc_class()):\n            await sleep(0.01)\n\n        with pytest.raises(get_cancelled_exc_class()):\n            await sleep(0.01)\n\n\nasync def test_cancel_shielded_scope() -> None:\n    with CancelScope(shield=True) as cancel_scope:\n        assert cancel_scope.shield\n        cancel_scope.cancel()\n        assert current_effective_deadline() == -math.inf\n\n        with pytest.raises(get_cancelled_exc_class()):\n            await sleep(0)\n\n\nasync def test_cancelled_not_caught() -> None:\n    with CancelScope() as scope:\n        scope.cancel()\n\n    assert scope.cancel_called\n    assert not scope.cancelled_caught\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_cancel_host_asyncgen() -> None:\n    done = False\n\n    async def host_task() -> None:\n        nonlocal done\n        async with create_task_group() as tg:\n            with CancelScope(shield=True) as inner_scope:\n                assert inner_scope.shield\n                tg.cancel_scope.cancel()\n\n            with pytest.raises(get_cancelled_exc_class()):\n                await sleep(0)\n\n            with pytest.raises(get_cancelled_exc_class()):\n                await sleep(0)\n\n            done = True\n\n    async def host_agen_fn() -> AsyncGenerator[None, None]:\n        await host_task()\n        yield\n        pytest.fail(\"host_agen_fn should only be __anext__ed once\")\n\n    host_agen = host_agen_fn()\n    try:\n        loop = asyncio.get_running_loop()\n        await loop.create_task(host_agen.__anext__())  # type: ignore[arg-type]\n    finally:\n        await host_agen.aclose()\n\n    assert done\n\n\nasync def test_shielding_immediate_scope_cancelled() -> None:\n    async def cancel_when_ready() -> None:\n        await wait_all_tasks_blocked()\n        scope.cancel()\n\n    sleep_completed = False\n    async with create_task_group() as tg:\n        with CancelScope(shield=True) as scope:\n            tg.start_soon(cancel_when_ready)\n            await sleep(0.5)\n            sleep_completed = True\n\n    assert not sleep_completed\n\n\nasync def test_shielding_mutate() -> None:\n    completed = False\n\n    async def task(task_status: TaskStatus) -> NoReturn:\n        nonlocal completed\n        with CancelScope() as scope:\n            # Enable the shield a little after the scope starts to make this test\n            # general, even though it has no bearing on the current implementation.\n            await sleep(0.1)\n            scope.shield = True\n            task_status.started()\n            await sleep(0.1)\n            completed = True\n            scope.shield = False\n            await sleep(1)\n            pytest.fail(\"Execution should not reach this point\")\n\n    async with create_task_group() as tg:\n        await tg.start(task)\n        tg.cancel_scope.cancel()\n\n    assert completed\n\n\nasync def test_cancel_scope_in_child_task() -> None:\n    child_scope = None\n\n    async def child() -> None:\n        nonlocal child_scope\n        with CancelScope() as child_scope:\n            await sleep(2)\n\n    host_done = False\n    async with create_task_group() as tg:\n        tg.start_soon(child)\n        await wait_all_tasks_blocked()\n        assert child_scope is not None\n        child_scope.cancel()\n        await sleep(0.1)\n        host_done = True\n\n    assert host_done\n    assert not tg.cancel_scope.cancel_called\n\n\nasync def test_exception_cancels_siblings() -> None:\n    sleep_completed = False\n\n    async def child(fail: bool) -> None:\n        if fail:\n            raise Exception(\"foo\")\n        else:\n            nonlocal sleep_completed\n            await sleep(1)\n            sleep_completed = True\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(child, False)\n            await wait_all_tasks_blocked()\n            tg.start_soon(child, True)\n\n    assert len(exc.value.exceptions) == 1\n    assert str(exc.value.exceptions[0]) == \"foo\"\n    assert not sleep_completed\n\n\nasync def test_cancel_cascade() -> None:\n    async def do_something() -> NoReturn:\n        async with create_task_group() as tg2:\n            tg2.start_soon(sleep, 1)\n\n        raise Exception(\"foo\")\n\n    async with create_task_group() as tg:\n        tg.start_soon(do_something)\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n\nasync def test_cancelled_parent() -> None:\n    async def child() -> NoReturn:\n        with CancelScope():\n            await sleep(1)\n\n        raise Exception(\"foo\")\n\n    async def parent(tg: TaskGroup) -> None:\n        await wait_all_tasks_blocked()\n        tg.start_soon(child)\n\n    async with create_task_group() as tg:\n        tg.start_soon(parent, tg)\n        tg.cancel_scope.cancel()\n\n\nasync def test_shielded_deadline() -> None:\n    with move_on_after(10):\n        with CancelScope(shield=True):\n            with move_on_after(1000):\n                assert current_effective_deadline() - current_time() > 900\n\n\nasync def test_deadline_reached_on_start() -> None:\n    with move_on_after(0):\n        await sleep(0)\n        pytest.fail(\"Execution should not reach this point\")\n\n\nasync def test_deadline_moved() -> None:\n    with fail_after(0.1) as scope:\n        scope.deadline += 0.3\n        await sleep(0.2)\n\n\nasync def test_timeout_error_with_multiple_cancellations() -> None:\n    with pytest.raises(TimeoutError):\n        with fail_after(0.1):\n            async with create_task_group() as tg:\n                tg.start_soon(sleep, 2)\n                await sleep(2)\n\n\nasync def test_nested_fail_after() -> None:\n    async def killer(scope: CancelScope) -> None:\n        await wait_all_tasks_blocked()\n        scope.cancel()\n\n    async with create_task_group() as tg:\n        with CancelScope() as scope:\n            with CancelScope():\n                tg.start_soon(killer, scope)\n                with fail_after(1):\n                    await sleep(2)\n                    pytest.fail(\"Execution should not reach this point\")\n\n                pytest.fail(\"Execution should not reach this point either\")\n\n            pytest.fail(\"Execution should also not reach this point\")\n\n    assert scope.cancel_called\n\n\nasync def test_nested_shield() -> None:\n    async def killer(scope: CancelScope) -> None:\n        await wait_all_tasks_blocked()\n        scope.cancel()\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            with CancelScope() as scope:\n                with CancelScope(shield=True):\n                    tg.start_soon(killer, scope)\n                    with fail_after(0.2):\n                        await sleep(2)\n\n    assert len(exc.value.exceptions) == 1\n    assert isinstance(exc.value.exceptions[0], TimeoutError)\n\n\nasync def test_triple_nested_shield_checkpoint_in_outer() -> None:\n    \"\"\"Regression test for #370.\"\"\"\n\n    got_past_checkpoint = False\n\n    async def taskfunc() -> None:\n        nonlocal got_past_checkpoint\n\n        with CancelScope() as scope1:\n            with CancelScope() as scope2:\n                with CancelScope(shield=True):\n                    scope1.cancel()\n                    scope2.cancel()\n\n            await checkpoint()\n            got_past_checkpoint = True\n\n    async with create_task_group() as tg:\n        tg.start_soon(taskfunc)\n\n    assert not got_past_checkpoint\n\n\nasync def test_triple_nested_shield_checkpoint_in_middle() -> None:\n    got_past_checkpoint = False\n\n    async def taskfunc() -> None:\n        nonlocal got_past_checkpoint\n\n        with CancelScope() as scope1:\n            with CancelScope():\n                with CancelScope(shield=True):\n                    scope1.cancel()\n\n                await checkpoint()\n                got_past_checkpoint = True\n\n    async with create_task_group() as tg:\n        tg.start_soon(taskfunc)\n\n    assert not got_past_checkpoint\n\n\ndef test_task_group_in_generator(\n    anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n) -> None:\n    async def task_group_generator() -> AsyncGenerator[None, None]:\n        async with create_task_group():\n            yield\n\n    gen = task_group_generator()\n    anyio.run(\n        gen.__anext__,\n        backend=anyio_backend_name,\n        backend_options=anyio_backend_options,\n    )\n    pytest.raises(\n        StopAsyncIteration,\n        anyio.run,\n        gen.__anext__,\n        backend=anyio_backend_name,\n        backend_options=anyio_backend_options,\n    )\n\n\nasync def test_exception_group_filtering() -> None:\n    \"\"\"Test that CancelledErrors are filtered out of nested exception groups.\"\"\"\n\n    async def fail(name: str) -> NoReturn:\n        try:\n            await anyio.sleep(0.1)\n        finally:\n            raise Exception(f\"{name} task failed\")\n\n    async def fn() -> None:\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(fail, \"parent\")\n            async with anyio.create_task_group() as tg2:\n                tg2.start_soon(fail, \"child\")\n                await anyio.sleep(1)\n\n    with pytest.raises(BaseExceptionGroup) as exc:\n        await fn()\n\n    assert len(exc.value.exceptions) == 2\n    assert str(exc.value.exceptions[0]) == \"parent task failed\"\n    assert isinstance(exc.value.exceptions[1], ExceptionGroup)\n    assert len(exc.value.exceptions[1].exceptions) == 1\n    assert str(exc.value.exceptions[1].exceptions[0]) == \"child task failed\"\n\n\nasync def test_cancel_propagation_with_inner_spawn() -> None:\n    async def g() -> NoReturn:\n        async with anyio.create_task_group() as tg2:\n            tg2.start_soon(anyio.sleep, 10)\n            await anyio.sleep(1)\n\n        assert False\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(g)\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n\nasync def test_escaping_cancelled_error_from_cancelled_task() -> None:\n    \"\"\"\n    Regression test for issue #88. No CancelledError should escape the outer scope.\n\n    \"\"\"\n    with CancelScope() as scope:\n        with move_on_after(0.1):\n            await sleep(1)\n\n        scope.cancel()\n\n\n@pytest.mark.skipif(\n    sys.version_info >= (3, 11),\n    reason=\"Generator based coroutines have been removed in Python 3.11\",\n)\n@pytest.mark.filterwarnings(\n    'ignore:\"@coroutine\" decorator is deprecated:DeprecationWarning'\n)\ndef test_cancel_generator_based_task() -> None:\n    async def native_coro_part() -> None:\n        with CancelScope() as scope:\n            asyncio.get_running_loop().call_soon(scope.cancel)\n            await asyncio.sleep(1)\n            pytest.fail(\"Execution should not have reached this line\")\n\n    @asyncio.coroutine  # type: ignore[attr-defined]\n    def generator_part() -> Generator[object, BaseException, None]:\n        yield from native_coro_part()  # type: ignore[misc]\n\n    anyio.run(generator_part, backend=\"asyncio\")\n\n\n@pytest.mark.skipif(\n    sys.version_info >= (3, 11),\n    reason=\"Generator based coroutines have been removed in Python 3.11\",\n)\n@pytest.mark.filterwarnings(\n    'ignore:\"@coroutine\" decorator is deprecated:DeprecationWarning'\n)\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_schedule_old_style_coroutine_func() -> None:\n    \"\"\"\n    Test that we give a sensible error when a user tries to spawn a task from a\n    generator-style coroutine function.\n    \"\"\"\n\n    @asyncio.coroutine  # type: ignore[attr-defined]\n    def corofunc() -> Generator[Any, Any, None]:\n        yield from asyncio.sleep(1)  # type: ignore[misc]\n\n    async with create_task_group() as tg:\n        funcname = (\n            f\"{__name__}.test_schedule_old_style_coroutine_func.<locals>.corofunc\"\n        )\n        with pytest.raises(\n            TypeError,\n            match=f\"Expected {funcname}\\\\(\\\\) to return a coroutine, but the return \"\n            f\"value \\\\(<generator .+>\\\\) is not a coroutine object\",\n        ):\n            tg.start_soon(corofunc)\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_cancel_native_future_tasks() -> None:\n    async def wait_native_future() -> None:\n        loop = asyncio.get_running_loop()\n        await loop.create_future()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(wait_native_future)\n        tg.cancel_scope.cancel()\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_cancel_native_future_tasks_cancel_scope() -> None:\n    async def wait_native_future() -> None:\n        with anyio.CancelScope():\n            loop = asyncio.get_running_loop()\n            await loop.create_future()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(wait_native_future)\n        tg.cancel_scope.cancel()\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_cancel_completed_task() -> None:\n    loop = asyncio.get_running_loop()\n    old_exception_handler = loop.get_exception_handler()\n    exceptions = []\n\n    def exception_handler(*args: object, **kwargs: object) -> None:\n        exceptions.append((args, kwargs))\n\n    loop.set_exception_handler(exception_handler)\n    try:\n\n        async def noop() -> None:\n            pass\n\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(noop)\n            tg.cancel_scope.cancel()\n\n        assert exceptions == []\n    finally:\n        loop.set_exception_handler(old_exception_handler)\n\n\nasync def test_task_in_sync_spawn_callback() -> None:\n    outer_task_id = anyio.get_current_task().id\n    inner_task_id = None\n\n    def task_wrap() -> Coroutine[object, object, None]:\n        assert anyio.get_current_task().id == outer_task_id\n\n        async def corofn() -> None:\n            nonlocal inner_task_id\n            inner_task_id = anyio.get_current_task().id\n\n        return corofn()\n\n    async with create_task_group() as tg:\n        tg.start_soon(task_wrap)\n\n    assert inner_task_id is not None\n    assert inner_task_id != outer_task_id\n\n\nasync def test_shielded_cancel_sleep_time() -> None:\n    \"\"\"\n    Test that cancelling a shielded tasks spends more time sleeping than cancelling.\n\n    \"\"\"\n    event = anyio.Event()\n    hang_time = 0.2\n\n    async def set_event() -> None:\n        await sleep(hang_time)\n        event.set()\n\n    async def never_cancel_task() -> None:\n        with CancelScope(shield=True):\n            await sleep(0.2)\n            await event.wait()\n\n    async with create_task_group() as tg:\n        tg.start_soon(set_event)\n\n        async with create_task_group() as tg:\n            tg.start_soon(never_cancel_task)\n            tg.cancel_scope.cancel()\n            process_time = time.process_time()\n\n        assert (time.process_time() - process_time) < hang_time\n\n\nasync def test_cancelscope_wrong_exit_order() -> None:\n    \"\"\"\n    Test that a RuntimeError is raised if the task tries to exit cancel scopes in the\n    wrong order.\n\n    \"\"\"\n    scope1 = CancelScope()\n    scope2 = CancelScope()\n    scope1.__enter__()\n    scope2.__enter__()\n    pytest.raises(RuntimeError, scope1.__exit__, None, None, None)\n\n\nasync def test_cancelscope_exit_before_enter() -> None:\n    \"\"\"\n    Test that a RuntimeError is raised if one tries to exit a cancel scope before\n    entering.\n\n    \"\"\"\n    scope = CancelScope()\n    pytest.raises(RuntimeError, scope.__exit__, None, None, None)\n\n\n@pytest.mark.parametrize(\n    \"anyio_backend\", [\"asyncio\"]\n)  # trio does not check for this yet\nasync def test_cancelscope_exit_in_wrong_task() -> None:\n    async def enter_scope(scope: CancelScope) -> None:\n        scope.__enter__()\n\n    async def exit_scope(scope: CancelScope) -> None:\n        scope.__exit__(None, None, None)\n\n    scope = CancelScope()\n    async with create_task_group() as tg:\n        tg.start_soon(enter_scope, scope)\n\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(exit_scope, scope)\n\n    assert len(exc.value.exceptions) == 1\n    assert str(exc.value.exceptions[0]) == (\n        \"Attempted to exit cancel scope in a different task than it was entered in\"\n    )\n\n\ndef test_unhandled_exception_group(caplog: pytest.LogCaptureFixture) -> None:\n    def crash() -> NoReturn:\n        raise KeyboardInterrupt\n\n    async def nested() -> None:\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(anyio.sleep, 5)\n            await anyio.sleep(5)\n\n    async def main() -> NoReturn:\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(nested)\n            await wait_all_tasks_blocked()\n            asyncio.get_running_loop().call_soon(crash)\n            await anyio.sleep(5)\n\n        pytest.fail(\"Execution should never reach this point\")\n\n    with pytest.raises(KeyboardInterrupt):\n        anyio.run(main, backend=\"asyncio\")\n\n    assert not caplog.messages\n\n\nasync def test_single_cancellation_exc() -> None:\n    \"\"\"\n    Test that only a single cancellation exception bubbles out of the task group when\n    case it was cancelled via an outer scope and no actual errors were raised.\n\n    \"\"\"\n    with CancelScope() as outer:\n        try:\n            async with create_task_group() as tg:\n                tg.start_soon(sleep, 5)\n                await wait_all_tasks_blocked()\n                outer.cancel()\n                await sleep(5)\n        except BaseException as exc:\n            if isinstance(exc, get_cancelled_exc_class()):\n                raise\n\n            pytest.fail(f\"Raised the wrong type of exception: {exc}\")\n        else:\n            pytest.fail(\"Did not raise a cancellation exception\")\n\n\nasync def test_start_soon_parent_id() -> None:\n    root_task_id = get_current_task().id\n    parent_id: int | None = None\n\n    async def subtask() -> None:\n        nonlocal parent_id\n        parent_id = get_current_task().parent_id\n\n    async def starter_task() -> None:\n        tg.start_soon(subtask)\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(starter_task)\n\n    assert parent_id == root_task_id\n\n\nasync def test_start_parent_id() -> None:\n    root_task_id = get_current_task().id\n    starter_task_id: int | None = None\n    initial_parent_id: int | None = None\n    permanent_parent_id: int | None = None\n\n    async def subtask(*, task_status: TaskStatus) -> None:\n        nonlocal initial_parent_id, permanent_parent_id\n        initial_parent_id = get_current_task().parent_id\n        task_status.started()\n        permanent_parent_id = get_current_task().parent_id\n\n    async def starter_task() -> None:\n        nonlocal starter_task_id\n        starter_task_id = get_current_task().id\n        await tg.start(subtask)\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(starter_task)\n\n    assert initial_parent_id != permanent_parent_id\n    assert initial_parent_id == starter_task_id\n    assert permanent_parent_id == root_task_id\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"Task uncancelling is only supported on Python 3.11\",\n)\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nclass TestUncancel:\n    async def test_uncancel_after_native_cancel(self) -> None:\n        task = cast(asyncio.Task, asyncio.current_task())\n        with pytest.raises(asyncio.CancelledError), CancelScope():\n            task.cancel()\n            await anyio.sleep(0)\n\n        assert task.cancelling() == 1\n        task.uncancel()\n\n    async def test_uncancel_after_scope_cancel(self) -> None:\n        task = cast(asyncio.Task, asyncio.current_task())\n        with CancelScope() as scope:\n            scope.cancel()\n            await anyio.sleep(0)\n\n        assert task.cancelling() == 0\n\n    async def test_uncancel_after_scope_and_native_cancel(self) -> None:\n        task = cast(asyncio.Task, asyncio.current_task())\n        with pytest.raises(asyncio.CancelledError), CancelScope() as scope:\n            scope.cancel()\n            task.cancel()\n            await anyio.sleep(0)\n\n        assert task.cancelling() == 1\n        task.uncancel()\n\n    async def test_cancel_message_replaced(self) -> None:\n        task = asyncio.current_task()\n        assert task\n        try:\n            task.cancel()\n            await anyio.sleep(0)\n        except asyncio.CancelledError:\n            try:\n                with CancelScope() as scope:\n                    scope.cancel()\n                    try:\n                        await anyio.sleep(0)\n                    except asyncio.CancelledError:\n                        raise asyncio.CancelledError\n            except asyncio.CancelledError:\n                pytest.fail(\"Should have swallowed the CancelledError\")\n\n\nasync def test_cancel_before_entering_task_group() -> None:\n    with CancelScope() as scope:\n        scope.cancel()\n        try:\n            async with create_task_group():\n                pass\n        except get_cancelled_exc_class():\n            pytest.fail(\"This should not raise a cancellation exception\")\n\n\nasync def test_reraise_cancelled_in_excgroup() -> None:\n    def handler(excgrp: BaseExceptionGroup) -> None:\n        raise\n\n    with CancelScope() as scope:\n        scope.cancel()\n        with catch({get_cancelled_exc_class(): handler}):\n            await anyio.sleep_forever()\n\n\nasync def test_cancel_child_task_when_host_is_shielded() -> None:\n    # Regression test for #642\n    # Tests that cancellation propagates to a child task even if the host task is within\n    # a shielded cancel scope.\n    cancelled = anyio.Event()\n\n    async def wait_cancel() -> None:\n        try:\n            await anyio.sleep_forever()\n        except anyio.get_cancelled_exc_class():\n            cancelled.set()\n            raise\n\n    with CancelScope() as parent_scope:\n        async with anyio.create_task_group() as task_group:\n            task_group.start_soon(wait_cancel)\n            await wait_all_tasks_blocked()\n\n            with CancelScope(shield=True), fail_after(1):\n                parent_scope.cancel()\n                await cancelled.wait()\n\n\nasync def test_start_cancels_parent_scope() -> None:\n    \"\"\"Regression test for #685 / #710.\"\"\"\n    started: bool = False\n\n    async def in_task_group(task_status: TaskStatus[None]) -> None:\n        nonlocal started\n        started = True\n        await sleep_forever()\n\n    async with create_task_group() as tg:\n        with CancelScope() as inner_scope:\n            inner_scope.cancel()\n            await tg.start(in_task_group)\n\n    assert started\n    assert not tg.cancel_scope.cancel_called\n\n\nclass TestTaskStatusTyping:\n    \"\"\"\n    These tests do not do anything at run time, but since the test suite is also checked\n    with a static type checker, it ensures that the `TaskStatus` typing works as\n    intended.\n    \"\"\"\n\n    async def typetest_None(*, task_status: TaskStatus[None]) -> None:\n        task_status.started()\n        task_status.started(None)\n\n    async def typetest_None_Union(*, task_status: TaskStatus[int | None]) -> None:\n        task_status.started()\n        task_status.started(None)\n\n    async def typetest_non_None(*, task_status: TaskStatus[int]) -> None:\n        # We use `type: ignore` and `--warn-unused-ignores` to get type checking errors\n        # if these ever stop failing.\n        task_status.started()  # type: ignore[call-arg]\n        task_status.started(None)  # type: ignore[arg-type]\n\n    async def typetest_variance_good(*, task_status: TaskStatus[float]) -> None:\n        task_status2: TaskStatus[int] = task_status\n        task_status2.started(0)\n\n    async def typetest_variance_bad(*, task_status: TaskStatus[int]) -> None:\n        # We use `type: ignore` and `--warn-unused-ignores` to get type checking errors\n        # if these ever stop failing.\n        task_status2: TaskStatus[float] = task_status  # type: ignore[assignment]\n        task_status2.started(0.0)\n\n    async def typetest_optional_status(\n        *,\n        task_status: TaskStatus[int] = TASK_STATUS_IGNORED,\n    ) -> None:\n        task_status.started(1)\n", "tests/test_eventloop.py": "from __future__ import annotations\n\nimport asyncio\nimport math\nfrom asyncio import get_running_loop\nfrom unittest.mock import AsyncMock\n\nimport pytest\nfrom pytest import MonkeyPatch\nfrom pytest_mock.plugin import MockerFixture\n\nfrom anyio import run, sleep_forever, sleep_until\n\npytestmark = pytest.mark.anyio\nfake_current_time = 1620581544.0\n\n\n@pytest.fixture\ndef fake_sleep(mocker: MockerFixture) -> AsyncMock:\n    mocker.patch(\"anyio._core._eventloop.current_time\", return_value=fake_current_time)\n    return mocker.patch(\"anyio._core._eventloop.sleep\", AsyncMock())\n\n\nasync def test_sleep_until(fake_sleep: AsyncMock) -> None:\n    deadline = fake_current_time + 500.102352\n    await sleep_until(deadline)\n    fake_sleep.assert_called_once_with(deadline - fake_current_time)\n\n\nasync def test_sleep_until_in_past(fake_sleep: AsyncMock) -> None:\n    deadline = fake_current_time - 500.102352\n    await sleep_until(deadline)\n    fake_sleep.assert_called_once_with(0)\n\n\nasync def test_sleep_forever(fake_sleep: AsyncMock) -> None:\n    await sleep_forever()\n    fake_sleep.assert_called_once_with(math.inf)\n\n\ndef test_run_task() -> None:\n    \"\"\"Test that anyio.run() on asyncio will work with a callable returning a Future.\"\"\"\n\n    async def async_add(x: int, y: int) -> int:\n        return x + y\n\n    result = run(asyncio.create_task, async_add(1, 2), backend=\"asyncio\")\n    assert result == 3\n\n\nclass TestAsyncioOptions:\n    def test_debug(self) -> None:\n        async def main() -> bool:\n            return get_running_loop().get_debug()\n\n        debug = run(main, backend=\"asyncio\", backend_options={\"debug\": True})\n        assert debug is True\n\n    def test_debug_via_env(self, monkeypatch: MonkeyPatch) -> None:\n        async def main() -> bool:\n            return get_running_loop().get_debug()\n\n        monkeypatch.setenv(\"PYTHONASYNCIODEBUG\", \"1\")\n        debug = run(main, backend=\"asyncio\")\n        assert debug is True\n\n    def test_loop_factory(self) -> None:\n        async def main() -> type:\n            return type(get_running_loop())\n\n        uvloop = pytest.importorskip(\"uvloop\", reason=\"uvloop not installed\")\n        loop_class = run(\n            main,\n            backend=\"asyncio\",\n            backend_options={\"loop_factory\": uvloop.new_event_loop},\n        )\n        assert issubclass(loop_class, uvloop.Loop)\n\n    def test_use_uvloop(self) -> None:\n        async def main() -> type:\n            return type(get_running_loop())\n\n        uvloop = pytest.importorskip(\"uvloop\", reason=\"uvloop not installed\")\n        loop_class = run(main, backend=\"asyncio\", backend_options={\"use_uvloop\": True})\n        assert issubclass(loop_class, uvloop.Loop)\n", "tests/test_subprocesses.py": "from __future__ import annotations\n\nimport os\nimport platform\nimport sys\nfrom pathlib import Path\nfrom subprocess import CalledProcessError\nfrom textwrap import dedent\n\nimport pytest\nfrom _pytest.fixtures import FixtureRequest\n\nfrom anyio import CancelScope, ClosedResourceError, open_process, run_process\nfrom anyio.streams.buffered import BufferedByteReceiveStream\n\npytestmark = pytest.mark.anyio\n\n\n@pytest.mark.parametrize(\n    \"shell, command\",\n    [\n        pytest.param(\n            True,\n            f'{sys.executable} -c \"import sys; print(sys.stdin.read()[::-1])\"',\n            id=\"shell\",\n        ),\n        pytest.param(\n            False,\n            [sys.executable, \"-c\", \"import sys; print(sys.stdin.read()[::-1])\"],\n            id=\"exec\",\n        ),\n    ],\n)\nasync def test_run_process(\n    shell: bool, command: str | list[str], anyio_backend_name: str\n) -> None:\n    process = await run_process(command, input=b\"abc\")\n    assert process.returncode == 0\n    assert process.stdout.rstrip() == b\"cba\"\n\n\nasync def test_run_process_checked() -> None:\n    with pytest.raises(CalledProcessError) as exc:\n        await run_process(\n            [\n                sys.executable,\n                \"-c\",\n                'import sys; print(\"stderr-text\", file=sys.stderr); '\n                'print(\"stdout-text\"); sys.exit(1)',\n            ],\n            check=True,\n        )\n\n    assert exc.value.returncode == 1\n    assert exc.value.stdout.rstrip() == b\"stdout-text\"\n    assert exc.value.stderr.rstrip() == b\"stderr-text\"\n\n\n@pytest.mark.skipif(\n    platform.system() == \"Windows\",\n    reason=\"process.terminate() kills the process instantly on Windows\",\n)\nasync def test_terminate(tmp_path: Path) -> None:\n    script_path = tmp_path / \"script.py\"\n    script_path.write_text(\n        dedent(\n            \"\"\"\\\n        import signal, sys, time\n\n        def terminate(signum, frame):\n            sys.exit(2)\n\n        signal.signal(signal.SIGTERM, terminate)\n        print('ready', flush=True)\n        time.sleep(5)\n    \"\"\"\n        )\n    )\n    async with await open_process([sys.executable, str(script_path)]) as process:\n        stdout = process.stdout\n        assert stdout is not None\n        buffered_stdout = BufferedByteReceiveStream(stdout)\n        line = await buffered_stdout.receive_until(b\"\\n\", 100)\n        assert line.rstrip() == b\"ready\"\n\n        process.terminate()\n        assert await process.wait() == 2\n\n\nasync def test_process_cwd(tmp_path: Path) -> None:\n    \"\"\"Test that `cwd` is successfully passed to the subprocess implementation\"\"\"\n    cmd = [sys.executable, \"-c\", \"import os; print(os.getcwd())\"]\n    result = await run_process(cmd, cwd=tmp_path)\n    assert result.stdout.decode().strip() == str(tmp_path)\n\n\nasync def test_process_env() -> None:\n    \"\"\"Test that `env` is successfully passed to the subprocess implementation\"\"\"\n    env = os.environ.copy()\n    env.update({\"foo\": \"bar\"})\n    cmd = [sys.executable, \"-c\", \"import os; print(os.environ['foo'])\"]\n    result = await run_process(cmd, env=env)\n    assert result.stdout.decode().strip() == env[\"foo\"]\n\n\n@pytest.mark.skipif(\n    platform.system() == \"Windows\", reason=\"Windows does not have os.getsid()\"\n)\nasync def test_process_new_session_sid() -> None:\n    \"\"\"\n    Test that start_new_session is successfully passed to the subprocess implementation.\n\n    \"\"\"\n    sid = os.getsid(os.getpid())\n    cmd = [sys.executable, \"-c\", \"import os; print(os.getsid(os.getpid()))\"]\n\n    result = await run_process(cmd)\n    assert result.stdout.decode().strip() == str(sid)\n\n    result = await run_process(cmd, start_new_session=True)\n    assert result.stdout.decode().strip() != str(sid)\n\n\nasync def test_run_process_connect_to_file(tmp_path: Path) -> None:\n    stdinfile = tmp_path / \"stdin\"\n    stdinfile.write_text(\"Hello, process!\\n\")\n    stdoutfile = tmp_path / \"stdout\"\n    stderrfile = tmp_path / \"stderr\"\n    with stdinfile.open(\"rb\") as fin, stdoutfile.open(\"wb\") as fout, stderrfile.open(\n        \"wb\"\n    ) as ferr:\n        async with await open_process(\n            [\n                sys.executable,\n                \"-c\",\n                \"import sys; txt = sys.stdin.read().strip(); \"\n                'print(\"stdin says\", repr(txt), \"but stderr says NO!\", '\n                \"file=sys.stderr); \"\n                'print(\"stdin says\", repr(txt), \"and stdout says YES!\")',\n            ],\n            stdin=fin,\n            stdout=fout,\n            stderr=ferr,\n        ) as p:\n            assert await p.wait() == 0\n\n    assert (\n        stdoutfile.read_text() == \"stdin says 'Hello, process!' and stdout says YES!\\n\"\n    )\n    assert (\n        stderrfile.read_text() == \"stdin says 'Hello, process!' but stderr says NO!\\n\"\n    )\n\n\nasync def test_run_process_inherit_stdout(capfd: pytest.CaptureFixture[str]) -> None:\n    await run_process(\n        [\n            sys.executable,\n            \"-c\",\n            'import sys; print(\"stderr-text\", file=sys.stderr); '\n            'print(\"stdout-text\")',\n        ],\n        check=True,\n        stdout=None,\n        stderr=None,\n    )\n    out, err = capfd.readouterr()\n    assert out == \"stdout-text\" + os.linesep\n    assert err == \"stderr-text\" + os.linesep\n\n\nasync def test_process_aexit_cancellation_doesnt_orphan_process() -> None:\n    \"\"\"\n    Regression test for #669.\n\n    Ensures that open_process.__aexit__() doesn't leave behind an orphan process when\n    cancelled.\n\n    \"\"\"\n    with CancelScope() as scope:\n        async with await open_process(\n            [sys.executable, \"-c\", \"import time; time.sleep(1)\"]\n        ) as process:\n            scope.cancel()\n\n    assert process.returncode is not None\n    assert process.returncode != 0\n\n\nasync def test_process_aexit_cancellation_closes_standard_streams(\n    request: FixtureRequest,\n    anyio_backend_name: str,\n) -> None:\n    \"\"\"\n    Regression test for #669.\n\n    Ensures that open_process.__aexit__() closes standard streams when cancelled. Also\n    ensures that process.std{in.send,{out,err}.receive}() raise ClosedResourceError on a\n    closed stream.\n\n    \"\"\"\n    if anyio_backend_name == \"asyncio\":\n        # Avoid pytest.xfail here due to https://github.com/pytest-dev/pytest/issues/9027\n        request.node.add_marker(\n            pytest.mark.xfail(reason=\"#671 needs to be resolved first\")\n        )\n\n    with CancelScope() as scope:\n        async with await open_process(\n            [sys.executable, \"-c\", \"import time; time.sleep(1)\"]\n        ) as process:\n            scope.cancel()\n\n    assert process.stdin is not None\n\n    with pytest.raises(ClosedResourceError):\n        await process.stdin.send(b\"foo\")\n\n    assert process.stdout is not None\n\n    with pytest.raises(ClosedResourceError):\n        await process.stdout.receive(1)\n\n    assert process.stderr is not None\n\n    with pytest.raises(ClosedResourceError):\n        await process.stderr.receive(1)\n", "tests/test_fileio.py": "from __future__ import annotations\n\nimport os\nimport pathlib\nimport platform\nimport socket\nimport stat\nimport sys\n\nimport pytest\nfrom _pytest.tmpdir import TempPathFactory\n\nfrom anyio import AsyncFile, Path, open_file, wrap_file\n\npytestmark = pytest.mark.anyio\n\n\nclass TestAsyncFile:\n    @pytest.fixture(scope=\"class\")\n    def testdata(cls) -> bytes:\n        return b\"\".join(bytes([i] * 1000) for i in range(10))\n\n    @pytest.fixture\n    def testdatafile(\n        self, tmp_path_factory: TempPathFactory, testdata: bytes\n    ) -> pathlib.Path:\n        file = tmp_path_factory.mktemp(\"file\").joinpath(\"testdata\")\n        file.write_bytes(testdata)\n        return file\n\n    async def test_open_close(self, testdatafile: pathlib.Path) -> None:\n        f = await open_file(testdatafile)\n        await f.aclose()\n\n    async def test_read(self, testdatafile: pathlib.Path, testdata: bytes) -> None:\n        async with await open_file(testdatafile, \"rb\") as f:\n            data = await f.read()\n\n        assert f.closed\n        assert data == testdata\n\n    async def test_write(self, testdatafile: pathlib.Path, testdata: bytes) -> None:\n        async with await open_file(testdatafile, \"ab\") as f:\n            await f.write(b\"f\" * 1000)\n\n        assert testdatafile.stat().st_size == len(testdata) + 1000\n\n    async def test_async_iteration(self, tmp_path: pathlib.Path) -> None:\n        lines = [\"blah blah\\n\", \"foo foo\\n\", \"bar bar\"]\n        testpath = tmp_path.joinpath(\"testfile\")\n        testpath.write_text(\"\".join(lines), \"ascii\")\n        async with await open_file(str(testpath)) as f:\n            lines_i = iter(lines)\n            async for line in f:\n                assert line == next(lines_i)\n\n    async def test_wrap_file(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testdata\"\n        with path.open(\"w\") as fp:\n            wrapped = wrap_file(fp)\n            await wrapped.write(\"dummydata\")\n\n        assert path.read_text() == \"dummydata\"\n\n\nclass TestPath:\n    @pytest.fixture\n    def populated_tmpdir(self, tmp_path: pathlib.Path) -> pathlib.Path:\n        tmp_path.joinpath(\"testfile\").touch()\n        tmp_path.joinpath(\"testfile2\").touch()\n\n        subdir = tmp_path / \"subdir\"\n        sibdir = tmp_path / \"sibdir\"\n\n        for subpath in (subdir, sibdir):\n            subpath.mkdir()\n            subpath.joinpath(\"dummyfile1.txt\").touch()\n            subpath.joinpath(\"dummyfile2.txt\").touch()\n\n        return tmp_path\n\n    async def test_properties(self) -> None:\n        \"\"\"\n        Ensure that all public properties and methods are available on the async Path\n        class.\n\n        \"\"\"\n        path = pathlib.Path(\"/test/path/another/part\")\n        stdlib_properties = {\n            p for p in dir(path) if p.startswith(\"__\") or not p.startswith(\"_\")\n        }\n        stdlib_properties.discard(\"link_to\")\n        stdlib_properties.discard(\"__class_getitem__\")\n        stdlib_properties.discard(\"__enter__\")\n        stdlib_properties.discard(\"__exit__\")\n\n        async_path = Path(path)\n        anyio_properties = {\n            p for p in dir(async_path) if p.startswith(\"__\") or not p.startswith(\"_\")\n        }\n\n        missing = stdlib_properties - anyio_properties\n        assert not missing\n\n    def test_repr(self) -> None:\n        assert repr(Path(\"/foo\")) == \"Path('/foo')\"\n\n    def test_bytes(self) -> None:\n        assert bytes(Path(\"/foo-\u00e5\u00e4\u00f6\")) == os.fsencode(f\"{os.path.sep}foo-\u00e5\u00e4\u00f6\")\n\n    def test_hash(self) -> None:\n        assert hash(Path(\"/foo\")) == hash(pathlib.Path(\"/foo\"))\n\n    def test_comparison(self) -> None:\n        path1 = Path(\"/foo1\")\n        path2 = Path(\"/foo2\")\n        assert path1 < path2\n        assert path1 <= path2\n        assert path2 > path1\n        assert path2 >= path1\n\n    def test_truediv(self) -> None:\n        result = Path(\"/foo\") / \"bar\"\n        assert isinstance(result, Path)\n        assert result == pathlib.Path(\"/foo/bar\")\n\n    def test_rtruediv(self) -> None:\n        result = \"/foo\" / Path(\"bar\")\n        assert isinstance(result, Path)\n        assert result == pathlib.Path(\"/foo/bar\")\n\n    def test_parts_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.txt\").parts == (os.path.sep, \"abc\", \"xyz\", \"foo.txt\")\n\n    @pytest.mark.skipif(\n        platform.system() != \"Windows\", reason=\"Drive only makes sense on Windows\"\n    )\n    def test_drive_property(self) -> None:\n        assert Path(\"c:\\\\abc\\\\xyz\").drive == \"c:\"\n\n    def test_root_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.txt\").root == os.path.sep\n\n    def test_anchor_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.txt.zip\").anchor == os.path.sep\n\n    def test_parents_property(self) -> None:\n        parents = Path(\"/abc/xyz/foo.txt\").parents\n        assert len(parents) == 3\n        assert all(isinstance(parent, Path) for parent in parents)\n        assert str(parents[0]) == f\"{os.path.sep}abc{os.path.sep}xyz\"\n        assert str(parents[1]) == f\"{os.path.sep}abc\"\n        assert str(parents[2]) == os.path.sep\n\n    def test_parent_property(self) -> None:\n        parent = Path(\"/abc/xyz/foo.txt\").parent\n        assert isinstance(parent, Path)\n        assert str(parent) == f\"{os.path.sep}abc{os.path.sep}xyz\"\n\n    def test_name_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.txt.zip\").name == \"foo.txt.zip\"\n\n    def test_suffix_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.txt.zip\").suffix == \".zip\"\n\n    def test_suffixes_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.tar.gz\").suffixes == [\".tar\", \".gz\"]\n\n    def test_stem_property(self) -> None:\n        assert Path(\"/abc/xyz/foo.txt.zip\").stem == \"foo.txt\"\n\n    async def test_absolute(self) -> None:\n        result = await Path(\"../foo/bar\").absolute()\n        assert isinstance(result, Path)\n        assert result == pathlib.Path.cwd() / \"../foo/bar\"\n\n    @pytest.mark.skipif(\n        platform.system() != \"Windows\", reason=\"Only makes sense on Windows\"\n    )\n    def test_as_posix(self) -> None:\n        assert Path(\"c:\\\\foo\\\\bar\").as_posix() == \"c:/foo/bar\"\n\n    def test_as_uri(self) -> None:\n        if platform.system() == \"Windows\":\n            assert Path(\"c:\\\\foo\\\\bar\").as_uri() == \"file:///c:/foo/bar\"\n        else:\n            assert Path(\"/foo/bar\").as_uri() == \"file:///foo/bar\"\n\n    @pytest.mark.skipif(\n        sys.version_info < (3, 13),\n        reason=\"Path.from_uri() is only available on Python 3.13+\",\n    )\n    def test_from_uri(self) -> None:\n        path = Path.from_uri(\"file:///foo/bar\")\n        assert isinstance(path, Path)\n        assert path.as_uri() == \"file:///foo/bar\"\n\n    async def test_cwd(self) -> None:\n        result = await Path.cwd()\n        assert isinstance(result, Path)\n        assert result == pathlib.Path.cwd()\n\n    async def test_exists(self, tmp_path: pathlib.Path) -> None:\n        assert not await Path(\"~/btelkbee\").exists()\n        assert await Path(tmp_path).exists()\n\n    async def test_expanduser(self) -> None:\n        result = await Path(\"~/btelkbee\").expanduser()\n        assert isinstance(result, Path)\n        assert str(result) == os.path.expanduser(f\"~{os.path.sep}btelkbee\")\n\n    async def test_home(self) -> None:\n        result = await Path.home()\n        assert isinstance(result, Path)\n        assert result == pathlib.Path.home()\n\n    @pytest.mark.parametrize(\n        \"arg, result\",\n        [\n            (\"c:/xyz\" if platform.system() == \"Windows\" else \"/xyz\", True),\n            (\"../xyz\", False),\n        ],\n    )\n    def test_is_absolute(self, arg: str, result: bool) -> None:\n        assert Path(arg).is_absolute() == result\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"Block devices are not available on Windows\",\n    )\n    async def test_is_block_device(self) -> None:\n        assert not await Path(\"/btelkbee\").is_block_device()\n        with os.scandir(\"/dev\") as iterator:\n            for entry in iterator:\n                if stat.S_ISBLK(entry.stat(follow_symlinks=False).st_mode):\n                    assert await Path(entry.path).is_block_device()\n                    break\n            else:\n                pytest.skip(\"Could not find a suitable block device\")\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"Character devices are not available on Windows\",\n    )\n    async def test_is_char_device(self) -> None:\n        assert not await Path(\"/btelkbee\").is_char_device()\n        assert await Path(\"/dev/random\").is_char_device()\n\n    async def test_is_dir(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somedir\"\n        assert not await Path(path).is_dir()\n        path.mkdir()\n        assert await Path(path).is_dir()\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\", reason=\"mkfifo() is not available on Windows\"\n    )\n    async def test_is_fifo(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somefifo\"\n        assert not await Path(path).is_fifo()\n        os.mkfifo(path)\n        assert await Path(path).is_fifo()\n\n    async def test_is_file(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somefile\"\n        assert not await Path(path).is_file()\n        path.touch()\n        assert await Path(path).is_file()\n\n    @pytest.mark.skipif(\n        sys.version_info < (3, 12),\n        reason=\"Path.is_junction() is only available on Python 3.12+\",\n    )\n    async def test_is_junction(self, tmp_path: pathlib.Path) -> None:\n        assert not await Path(tmp_path).is_junction()\n\n    async def test_is_mount(self) -> None:\n        assert not await Path(\"/gfobj4ewiotj\").is_mount()\n        assert await Path(\"/\").is_mount()\n\n    @pytest.mark.filterwarnings(\"ignore::DeprecationWarning\")\n    def test_is_reserved(self) -> None:\n        expected_result = platform.system() == \"Windows\"\n        assert Path(\"nul\").is_reserved() == expected_result\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"UNIX sockets are not available on Windows\",\n    )\n    async def test_is_socket(self, tmp_path_factory: TempPathFactory) -> None:\n        path = tmp_path_factory.mktemp(\"unix\").joinpath(\"socket\")\n        assert not await Path(path).is_socket()\n        with socket.socket(socket.AF_UNIX) as sock:\n            sock.bind(str(path))\n            assert await Path(path).is_socket()\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"symbolic links are not supported on Windows\",\n    )\n    async def test_is_symlink(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        assert not await Path(path).is_symlink()\n        path.symlink_to(\"/foo\")\n        assert await Path(path).is_symlink()\n\n    @pytest.mark.parametrize(\"arg, result\", [(\"/xyz/abc\", True), (\"/xyz/baz\", False)])\n    def test_is_relative_to(self, arg: str, result: bool) -> None:\n        assert Path(\"/xyz/abc/foo\").is_relative_to(arg) == result\n\n    async def test_glob(self, populated_tmpdir: pathlib.Path) -> None:\n        all_paths = []\n        async for path in Path(populated_tmpdir).glob(\"**/*.txt\"):\n            assert isinstance(path, Path)\n            all_paths.append(path.relative_to(populated_tmpdir))\n\n        all_paths.sort()\n        assert all_paths == [\n            Path(\"sibdir\") / \"dummyfile1.txt\",\n            Path(\"sibdir\") / \"dummyfile2.txt\",\n            Path(\"subdir\") / \"dummyfile1.txt\",\n            Path(\"subdir\") / \"dummyfile2.txt\",\n        ]\n\n    async def test_rglob(self, populated_tmpdir: pathlib.Path) -> None:\n        all_paths = []\n        async for path in Path(populated_tmpdir).rglob(\"*.txt\"):\n            assert isinstance(path, Path)\n            all_paths.append(path.relative_to(populated_tmpdir))\n\n        all_paths.sort()\n        assert all_paths == [\n            Path(\"sibdir\") / \"dummyfile1.txt\",\n            Path(\"sibdir\") / \"dummyfile2.txt\",\n            Path(\"subdir\") / \"dummyfile1.txt\",\n            Path(\"subdir\") / \"dummyfile2.txt\",\n        ]\n\n    async def test_iterdir(self, populated_tmpdir: pathlib.Path) -> None:\n        all_paths = []\n        async for path in Path(populated_tmpdir).iterdir():\n            assert isinstance(path, Path)\n            all_paths.append(path.name)\n\n        all_paths.sort()\n        assert all_paths == [\"sibdir\", \"subdir\", \"testfile\", \"testfile2\"]\n\n    def test_joinpath(self) -> None:\n        path = Path(\"/foo\").joinpath(\"bar\")\n        assert path == Path(\"/foo/bar\")\n\n    @pytest.mark.skipif(\n        sys.version_info < (3, 13),\n        reason=\"Path.full_match() is only available on Python 3.13+\",\n    )\n    def test_fullmatch(self) -> None:\n        assert Path(\"/foo/bar\").full_match(\"/foo/*\")\n        assert not Path(\"/foo/bar\").full_match(\"/baz/*\")\n\n    def test_match(self) -> None:\n        assert Path(\"/foo/bar\").match(\"/foo/*\")\n        assert not Path(\"/foo/bar\").match(\"/baz/*\")\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\", reason=\"chmod() is not available on Windows\"\n    )\n    async def test_chmod(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        path.touch(0o666)\n        await Path(path).chmod(0o444)\n        assert path.stat().st_mode & 0o777 == 0o444\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\", reason=\"hard links are not supported on Windows\"\n    )\n    async def test_hardlink_to(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        target = tmp_path / \"link\"\n        target.touch()\n        await Path(path).hardlink_to(Path(target))\n        assert path.stat().st_nlink == 2\n        assert target.stat().st_nlink == 2\n\n    @pytest.mark.skipif(\n        not hasattr(os, \"lchmod\"), reason=\"os.lchmod() is not available\"\n    )\n    async def test_lchmod(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        path.symlink_to(\"/foo/bar/baz\")\n        await Path(path).lchmod(0o600)\n        assert path.lstat().st_mode & 0o777 == 0o600\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"symbolic links are not supported on Windows\",\n    )\n    async def test_lstat(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path.joinpath(\"testfile\")\n        path.symlink_to(\"/foo/bar/baz\")\n        result = await Path(path).lstat()\n        assert isinstance(result, os.stat_result)\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"owner and group are not supported on Windows\",\n    )\n    async def test_group(self, tmp_path: pathlib.Path) -> None:\n        import grp\n\n        group_name = grp.getgrgid(os.getegid()).gr_name\n        assert await Path(tmp_path).group() == group_name\n\n    async def test_mkdir(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testdir\"\n        await Path(path).mkdir()\n        assert path.is_dir()\n\n    async def test_open(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        path.write_bytes(b\"bibbitibobbitiboo\")\n        fp = await Path(path).open(\"rb\")\n        assert isinstance(fp, AsyncFile)\n        assert fp.name == str(path)\n        await fp.aclose()\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"owner and group are not supported on Windows\",\n    )\n    async def test_owner(self, tmp_path: pathlib.Path) -> None:\n        import pwd\n\n        user_name = pwd.getpwuid(os.geteuid()).pw_name\n        assert await Path(tmp_path).owner() == user_name\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"symbolic links are not supported on Windows\",\n    )\n    async def test_readlink(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path.joinpath(\"testfile\")\n        path.symlink_to(\"/foo/bar/baz\")\n        link_target = await Path(path).readlink()\n        assert isinstance(link_target, Path)\n        assert str(link_target) == \"/foo/bar/baz\"\n\n    async def test_read_bytes(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        path.write_bytes(b\"bibbitibobbitiboo\")\n        assert await Path(path).read_bytes() == b\"bibbitibobbitiboo\"\n\n    async def test_read_text(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        path.write_text(\"some text \u00e5\u00e4\u00f6\", encoding=\"utf-8\")\n        assert await Path(path).read_text(encoding=\"utf-8\") == \"some text \u00e5\u00e4\u00f6\"\n\n    async def test_relative_to_subpath(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"subdir\"\n        assert path.relative_to(tmp_path) == Path(\"subdir\")\n\n    @pytest.mark.skipif(\n        sys.version_info < (3, 12),\n        reason=\"Path.relative_to(walk_up=<bool>) is only available on Python 3.12+\",\n    )\n    async def test_relative_to_sibling(\n        self,\n        populated_tmpdir: pathlib.Path,\n        monkeypatch: pytest.MonkeyPatch,\n    ) -> None:\n        subdir = Path(populated_tmpdir / \"subdir\")\n        sibdir = Path(populated_tmpdir / \"sibdir\")\n\n        with pytest.raises(ValueError):\n            subdir.relative_to(sibdir, walk_up=False)\n\n        monkeypatch.chdir(sibdir)\n        relpath = subdir.relative_to(sibdir, walk_up=True) / \"dummyfile1.txt\"\n        assert os.access(relpath, os.R_OK)\n\n    async def test_rename(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somefile\"\n        path.touch()\n        target = tmp_path / \"anotherfile\"\n        result = await Path(path).rename(Path(target))\n        assert isinstance(result, Path)\n        assert result == target\n\n    async def test_replace(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somefile\"\n        path.write_text(\"hello\")\n        target = tmp_path / \"anotherfile\"\n        target.write_text(\"world\")\n        result = await Path(path).replace(Path(target))\n        assert isinstance(result, Path)\n        assert result == target\n        assert target.read_text() == \"hello\"\n\n    async def test_resolve(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somedir\" / \"..\" / \"somefile\"\n        result = await Path(path).resolve()\n        assert result == tmp_path / \"somefile\"\n\n    async def test_rmdir(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somedir\"\n        path.mkdir()\n        await Path(path).rmdir()\n        assert not path.exists()\n\n    async def test_samefile(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"somefile\"\n        path.touch()\n        assert await Path(tmp_path / \"somefile\").samefile(Path(path))\n\n    async def test_stat(self, tmp_path: pathlib.Path) -> None:\n        result = await Path(tmp_path).stat()\n        assert isinstance(result, os.stat_result)\n\n    async def test_touch(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        await Path(path).touch()\n        assert path.is_file()\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"symbolic links are not supported on Windows\",\n    )\n    async def test_symlink_to(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        target = tmp_path / \"link\"\n        await Path(path).symlink_to(Path(target))\n        assert path.is_symlink()\n\n    async def test_unlink(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        path.touch()\n        await Path(path).unlink()\n        assert not path.exists()\n\n    async def test_unlink_missing_file(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        await Path(path).unlink(missing_ok=True)\n        with pytest.raises(FileNotFoundError):\n            await Path(path).unlink(missing_ok=False)\n\n    @pytest.mark.skipif(\n        sys.version_info < (3, 12),\n        reason=\"Path.walk() is only available on Python 3.12+\",\n    )\n    async def test_walk(self, tmp_path: pathlib.Path) -> None:\n        subdir = tmp_path / \"subdir\"\n        subdir.mkdir()\n        subdir.joinpath(\"file1\").touch()\n        subdir.joinpath(\"file2\").touch()\n\n        path = Path(tmp_path)\n        iterator = Path(tmp_path).walk().__aiter__()\n\n        root, dirs, files = await iterator.__anext__()\n        assert root == path\n        assert dirs == [\"subdir\"]\n        assert files == []\n\n        root, dirs, files = await iterator.__anext__()\n        assert root == path / \"subdir\"\n        assert dirs == []\n        assert sorted(files) == [\"file1\", \"file2\"]\n\n        with pytest.raises(StopAsyncIteration):\n            await iterator.__anext__()\n\n    def test_with_name(self) -> None:\n        assert Path(\"/xyz/foo.txt\").with_name(\"bar\").name == \"bar\"\n\n    def test_with_stem(self) -> None:\n        assert Path(\"/xyz/foo.txt\").with_stem(\"bar\").name == \"bar.txt\"\n\n    def test_with_suffix(self) -> None:\n        assert Path(\"/xyz/foo.txt.gz\").with_suffix(\".zip\").name == \"foo.txt.zip\"\n\n    async def test_write_bytes(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        await Path(path).write_bytes(b\"bibbitibobbitiboo\")\n        assert path.read_bytes() == b\"bibbitibobbitiboo\"\n\n    async def test_write_text(self, tmp_path: pathlib.Path) -> None:\n        path = tmp_path / \"testfile\"\n        await Path(path).write_text(\"some text \u00e5\u00e4\u00f6\", encoding=\"utf-8\")\n        assert path.read_text(encoding=\"utf-8\") == \"some text \u00e5\u00e4\u00f6\"\n", "tests/__init__.py": "", "tests/test_lowlevel.py": "from __future__ import annotations\n\nfrom typing import Any\n\nimport pytest\n\nfrom anyio import create_task_group, run\nfrom anyio.lowlevel import (\n    RunVar,\n    cancel_shielded_checkpoint,\n    checkpoint,\n    checkpoint_if_cancelled,\n)\n\npytestmark = pytest.mark.anyio\n\n\n@pytest.mark.parametrize(\"cancel\", [False, True])\nasync def test_checkpoint_if_cancelled(cancel: bool) -> None:\n    finished = second_finished = False\n\n    async def func() -> None:\n        nonlocal finished\n        tg.start_soon(second_func)\n        if cancel:\n            tg.cancel_scope.cancel()\n\n        await checkpoint_if_cancelled()\n        finished = True\n\n    async def second_func() -> None:\n        nonlocal second_finished\n        assert finished != cancel\n        second_finished = True\n\n    async with create_task_group() as tg:\n        tg.start_soon(func)\n\n    assert finished != cancel\n    assert second_finished\n\n\n@pytest.mark.parametrize(\"cancel\", [False, True])\nasync def test_cancel_shielded_checkpoint(cancel: bool) -> None:\n    finished = second_finished = False\n\n    async def func() -> None:\n        nonlocal finished\n        await cancel_shielded_checkpoint()\n        finished = True\n\n    async def second_func() -> None:\n        nonlocal second_finished\n        assert not finished\n        second_finished = True\n\n    async with create_task_group() as tg:\n        tg.start_soon(func)\n        tg.start_soon(second_func)\n        if cancel:\n            tg.cancel_scope.cancel()\n\n    assert finished\n    assert second_finished\n\n\n@pytest.mark.parametrize(\"cancel\", [False, True])\nasync def test_checkpoint(cancel: bool) -> None:\n    finished = second_finished = False\n\n    async def func() -> None:\n        nonlocal finished\n        await checkpoint()\n        finished = True\n\n    async def second_func() -> None:\n        nonlocal second_finished\n        assert not finished\n        second_finished = True\n\n    async with create_task_group() as tg:\n        tg.start_soon(func)\n        tg.start_soon(second_func)\n        if cancel:\n            tg.cancel_scope.cancel()\n\n    assert finished != cancel\n    assert second_finished\n\n\nclass TestRunVar:\n    def test_get_set(\n        self,\n        anyio_backend_name: str,\n        anyio_backend_options: dict[str, Any],\n    ) -> None:\n        async def taskfunc(index: int) -> None:\n            assert var.get() == index\n            var.set(index + 1)\n\n        async def main() -> None:\n            pytest.raises(LookupError, var.get)\n            for i in range(2):\n                var.set(i)\n                async with create_task_group() as tg:\n                    tg.start_soon(taskfunc, i)\n\n                assert var.get() == i + 1\n\n        var = RunVar[int](\"var\")\n        for _ in range(2):\n            run(main, backend=anyio_backend_name, backend_options=anyio_backend_options)\n\n    async def test_reset_token_used_on_wrong_runvar(self) -> None:\n        var1 = RunVar[str](\"var1\")\n        var2 = RunVar[str](\"var2\")\n        token = var1.set(\"blah\")\n        with pytest.raises(\n            ValueError, match=\"This token does not belong to this RunVar\"\n        ):\n            var2.reset(token)\n\n    async def test_reset_token_used_twice(self) -> None:\n        var = RunVar[str](\"var\")\n        token = var.set(\"blah\")\n        var.reset(token)\n        with pytest.raises(ValueError, match=\"This token has already been used\"):\n            var.reset(token)\n", "tests/test_from_thread.py": "from __future__ import annotations\n\nimport math\nimport sys\nimport threading\nimport time\nfrom collections.abc import Awaitable, Callable\nfrom concurrent import futures\nfrom concurrent.futures import CancelledError, Future\nfrom contextlib import asynccontextmanager, suppress\nfrom contextvars import ContextVar\nfrom typing import Any, AsyncGenerator, Literal, NoReturn, TypeVar\n\nimport pytest\nimport sniffio\nfrom _pytest.logging import LogCaptureFixture\n\nfrom anyio import (\n    CancelScope,\n    Event,\n    create_task_group,\n    fail_after,\n    from_thread,\n    get_all_backends,\n    get_cancelled_exc_class,\n    get_current_task,\n    run,\n    sleep,\n    to_thread,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import TaskStatus\nfrom anyio.from_thread import BlockingPortal, start_blocking_portal\nfrom anyio.lowlevel import checkpoint\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import BaseExceptionGroup, ExceptionGroup\n\npytestmark = pytest.mark.anyio\n\nT_Retval = TypeVar(\"T_Retval\")\n\n\nasync def async_add(a: int, b: int) -> int:\n    assert threading.current_thread() is threading.main_thread()\n    return a + b\n\n\nasync def asyncgen_add(a: int, b: int) -> AsyncGenerator[int, Any]:\n    yield a + b\n\n\ndef sync_add(a: int, b: int) -> int:\n    assert threading.current_thread() is threading.main_thread()\n    return a + b\n\n\ndef thread_worker_async(\n    func: Callable[..., Awaitable[T_Retval]], *args: Any\n) -> T_Retval:\n    assert threading.current_thread() is not threading.main_thread()\n    return from_thread.run(func, *args)\n\n\ndef thread_worker_sync(func: Callable[..., T_Retval], *args: Any) -> T_Retval:\n    assert threading.current_thread() is not threading.main_thread()\n    return from_thread.run_sync(func, *args)\n\n\n@pytest.mark.parametrize(\"cancel\", [True, False])\nasync def test_thread_cancelled(cancel: bool) -> None:\n    event = threading.Event()\n    thread_finished_future: Future[None] = Future()\n\n    def sync_function() -> None:\n        event.wait(3)\n        try:\n            from_thread.check_cancelled()\n        except BaseException as exc:\n            thread_finished_future.set_exception(exc)\n        else:\n            thread_finished_future.set_result(None)\n\n    async with create_task_group() as tg:\n        tg.start_soon(to_thread.run_sync, sync_function)\n        await wait_all_tasks_blocked()\n        if cancel:\n            tg.cancel_scope.cancel()\n\n        event.set()\n\n    if cancel:\n        with pytest.raises(get_cancelled_exc_class()):\n            thread_finished_future.result(3)\n    else:\n        thread_finished_future.result(3)\n\n\nasync def test_thread_cancelled_and_abandoned() -> None:\n    event = threading.Event()\n    thread_finished_future: Future[None] = Future()\n\n    def sync_function() -> None:\n        event.wait(3)\n        try:\n            from_thread.check_cancelled()\n        except BaseException as exc:\n            thread_finished_future.set_exception(exc)\n        else:\n            thread_finished_future.set_result(None)\n\n    async with create_task_group() as tg:\n        tg.start_soon(lambda: to_thread.run_sync(sync_function, abandon_on_cancel=True))\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    event.set()\n    with pytest.raises(get_cancelled_exc_class()):\n        thread_finished_future.result(3)\n\n\nasync def test_cancelscope_propagation() -> None:\n    async def async_time_bomb() -> None:\n        cancel_scope.cancel()\n        with fail_after(1):\n            await sleep(3)\n\n    with CancelScope() as cancel_scope:\n        await to_thread.run_sync(from_thread.run, async_time_bomb)\n\n    assert cancel_scope.cancelled_caught\n\n\nasync def test_cancelscope_propagation_when_abandoned() -> None:\n    host_cancelled_event = Event()\n    completed_event = Event()\n\n    async def async_time_bomb() -> None:\n        cancel_scope.cancel()\n        with fail_after(3):\n            await host_cancelled_event.wait()\n\n        completed_event.set()\n\n    with CancelScope() as cancel_scope:\n        await to_thread.run_sync(\n            from_thread.run, async_time_bomb, abandon_on_cancel=True\n        )\n\n    assert cancel_scope.cancelled_caught\n    host_cancelled_event.set()\n    with fail_after(3):\n        await completed_event.wait()\n\n\nclass TestRunAsyncFromThread:\n    async def test_run_corofunc_from_thread(self) -> None:\n        result = await to_thread.run_sync(thread_worker_async, async_add, 1, 2)\n        assert result == 3\n\n    async def test_run_asyncgen_from_thread(self) -> None:\n        gen = asyncgen_add(1, 2)\n        try:\n            result = await to_thread.run_sync(thread_worker_async, gen.__anext__)\n            assert result == 3\n        finally:\n            await gen.aclose()\n\n    async def test_run_sync_from_thread(self) -> None:\n        result = await to_thread.run_sync(thread_worker_sync, sync_add, 1, 2)\n        assert result == 3\n\n    def test_run_sync_from_thread_pooling(self) -> None:\n        async def main() -> None:\n            thread_ids = set()\n            for _ in range(5):\n                thread_ids.add(await to_thread.run_sync(threading.get_ident))\n\n            # Expects that all the work has been done in the same worker thread\n            assert len(thread_ids) == 1\n            assert thread_ids.pop() != threading.get_ident()\n            assert threading.active_count() == initial_count + 1\n\n        # The thread should not exist after the event loop has been closed\n        initial_count = threading.active_count()\n        run(main, backend=\"asyncio\")\n\n        for _ in range(10):\n            if threading.active_count() == initial_count:\n                return\n\n            time.sleep(0.1)\n\n        pytest.fail(\"Worker thread did not exit within 1 second\")\n\n    async def test_run_async_from_thread_exception(self) -> None:\n        with pytest.raises(TypeError) as exc:\n            await to_thread.run_sync(thread_worker_async, async_add, 1, \"foo\")\n\n        exc.match(\"unsupported operand type\")\n\n    async def test_run_sync_from_thread_exception(self) -> None:\n        with pytest.raises(TypeError) as exc:\n            await to_thread.run_sync(thread_worker_sync, sync_add, 1, \"foo\")\n\n        exc.match(\"unsupported operand type\")\n\n    async def test_run_anyio_async_func_from_thread(self) -> None:\n        def worker(delay: float) -> Literal[True]:\n            from_thread.run(sleep, delay)\n            return True\n\n        assert await to_thread.run_sync(worker, 0)\n\n    def test_run_async_from_unclaimed_thread(self) -> None:\n        async def foo() -> None:\n            pass\n\n        exc = pytest.raises(RuntimeError, from_thread.run, foo)\n        exc.match(\"This function can only be run from an AnyIO worker thread\")\n\n    async def test_contextvar_propagation(self, anyio_backend_name: str) -> None:\n        var = ContextVar(\"var\", default=1)\n\n        async def async_func() -> int:\n            await checkpoint()\n            return var.get()\n\n        def worker() -> int:\n            var.set(6)\n            return from_thread.run(async_func)\n\n        assert await to_thread.run_sync(worker) == 6\n\n    async def test_sniffio(self, anyio_backend_name: str) -> None:\n        async def async_func() -> str:\n            return sniffio.current_async_library()\n\n        def worker() -> str:\n            sniffio.current_async_library_cvar.set(\"something invalid for async_func\")\n            return from_thread.run(async_func)\n\n        assert await to_thread.run_sync(worker) == anyio_backend_name\n\n\nclass TestRunSyncFromThread:\n    def test_run_sync_from_unclaimed_thread(self) -> None:\n        def foo() -> None:\n            pass\n\n        exc = pytest.raises(RuntimeError, from_thread.run_sync, foo)\n        exc.match(\"This function can only be run from an AnyIO worker thread\")\n\n    async def test_contextvar_propagation(self) -> None:\n        var = ContextVar(\"var\", default=1)\n\n        def worker() -> int:\n            var.set(6)\n            return from_thread.run_sync(var.get)\n\n        assert await to_thread.run_sync(worker) == 6\n\n    async def test_sniffio(self, anyio_backend_name: str) -> None:\n        def worker() -> str:\n            sniffio.current_async_library_cvar.set(\"something invalid for async_func\")\n            return from_thread.run_sync(sniffio.current_async_library)\n\n        assert await to_thread.run_sync(worker) == anyio_backend_name\n\n\nclass TestBlockingPortal:\n    class AsyncCM:\n        def __init__(self, ignore_error: bool):\n            self.ignore_error = ignore_error\n\n        async def __aenter__(self) -> Literal[\"test\"]:\n            return \"test\"\n\n        async def __aexit__(\n            self, exc_type: object, exc_val: object, exc_tb: object\n        ) -> bool:\n            return self.ignore_error\n\n    async def test_call_corofunc(self) -> None:\n        async with BlockingPortal() as portal:\n            result = await to_thread.run_sync(portal.call, async_add, 1, 2)\n            assert result == 3\n\n    async def test_call_anext(self) -> None:\n        gen = asyncgen_add(1, 2)\n        try:\n            async with BlockingPortal() as portal:\n                result = await to_thread.run_sync(portal.call, gen.__anext__)\n                assert result == 3\n        finally:\n            await gen.aclose()\n\n    async def test_aexit_with_exception(self) -> None:\n        \"\"\"\n        Test that when the portal exits with an exception, all tasks are cancelled.\n\n        \"\"\"\n\n        def external_thread() -> None:\n            try:\n                portal.call(sleep, 3)\n            except BaseException as exc:\n                results.append(exc)\n            else:\n                results.append(None)\n\n        results: list[BaseException | None] = []\n        with suppress(Exception):\n            async with BlockingPortal() as portal:\n                thread1 = threading.Thread(target=external_thread)\n                thread1.start()\n                thread2 = threading.Thread(target=external_thread)\n                thread2.start()\n                await sleep(0.1)\n                assert not results\n                raise Exception\n\n        await to_thread.run_sync(thread1.join)\n        await to_thread.run_sync(thread2.join)\n\n        assert len(results) == 2\n        assert isinstance(results[0], CancelledError)\n        assert isinstance(results[1], CancelledError)\n\n    async def test_aexit_without_exception(self) -> None:\n        \"\"\"Test that when the portal exits, it waits for all tasks to finish.\"\"\"\n\n        def external_thread() -> None:\n            try:\n                portal.call(sleep, 0.2)\n            except BaseException as exc:\n                results.append(exc)\n            else:\n                results.append(None)\n\n        results: list[BaseException | None] = []\n        async with BlockingPortal() as portal:\n            thread1 = threading.Thread(target=external_thread)\n            thread1.start()\n            thread2 = threading.Thread(target=external_thread)\n            thread2.start()\n            await sleep(0.1)\n            assert not results\n\n        await to_thread.run_sync(thread1.join)\n        await to_thread.run_sync(thread2.join)\n\n        assert results == [None, None]\n\n    async def test_call_portal_from_event_loop_thread(self) -> None:\n        async with BlockingPortal() as portal:\n            exc = pytest.raises(RuntimeError, portal.call, threading.get_ident)\n            exc.match(\"This method cannot be called from the event loop thread\")\n\n    def test_start_with_new_event_loop(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def async_get_thread_id() -> int:\n            return threading.get_ident()\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            thread_id = portal.call(async_get_thread_id)\n\n        assert isinstance(thread_id, int)\n        assert thread_id != threading.get_ident()\n\n    def test_start_with_nonexistent_backend(self) -> None:\n        with pytest.raises(LookupError) as exc:\n            with start_blocking_portal(\"foo\"):\n                pass\n\n        exc.match(\"No such backend: foo\")\n\n    def test_call_stopped_portal(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            pass\n\n        pytest.raises(RuntimeError, portal.call, threading.get_ident).match(\n            \"This portal is not running\"\n        )\n\n    def test_start_task_soon(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def event_waiter() -> Literal[\"test\"]:\n            await event1.wait()\n            event2.set()\n            return \"test\"\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            event1 = portal.call(Event)\n            event2 = portal.call(Event)\n            future = portal.start_task_soon(event_waiter)\n            portal.call(event1.set)\n            portal.call(event2.wait)\n            assert future.result() == \"test\"\n\n    def test_start_task_soon_cancel_later(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def noop() -> None:\n            await sleep(2)\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            future = portal.start_task_soon(noop)\n            portal.call(wait_all_tasks_blocked)\n            future.cancel()\n\n        assert future.cancelled()\n\n    def test_start_task_soon_cancel_immediately(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        cancelled = False\n\n        async def event_waiter() -> None:\n            nonlocal cancelled\n            try:\n                await sleep(3)\n            except get_cancelled_exc_class():\n                cancelled = True\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            future = portal.start_task_soon(event_waiter)\n            future.cancel()\n\n        assert cancelled\n\n    def test_start_task_soon_with_name(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        task_name = None\n\n        async def taskfunc() -> None:\n            nonlocal task_name\n            task_name = get_current_task().name\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            portal.start_task_soon(taskfunc, name=\"testname\")\n\n        assert task_name == \"testname\"\n\n    def test_async_context_manager_success(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            with portal.wrap_async_context_manager(\n                TestBlockingPortal.AsyncCM(False)\n            ) as cm:\n                assert cm == \"test\"\n\n    def test_async_context_manager_error(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            with pytest.raises(Exception) as exc:\n                with portal.wrap_async_context_manager(\n                    TestBlockingPortal.AsyncCM(False)\n                ) as cm:\n                    assert cm == \"test\"\n                    raise Exception(\"should NOT be ignored\")\n\n                exc.match(\"should NOT be ignored\")\n\n    def test_async_context_manager_error_ignore(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            with portal.wrap_async_context_manager(\n                TestBlockingPortal.AsyncCM(True)\n            ) as cm:\n                assert cm == \"test\"\n                raise Exception(\"should be ignored\")\n\n    def test_async_context_manager_exception_in_task_group(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        \"\"\"Regression test for #381.\"\"\"\n\n        async def failing_func() -> None:\n            0 / 0\n\n        @asynccontextmanager\n        async def run_in_context() -> AsyncGenerator[None, None]:\n            async with create_task_group() as tg:\n                tg.start_soon(failing_func)\n                yield\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            with pytest.raises(ExceptionGroup) as exc:\n                with portal.wrap_async_context_manager(run_in_context()):\n                    pass\n\n            assert len(exc.value.exceptions) == 1\n            assert isinstance(exc.value.exceptions[0], ZeroDivisionError)\n\n    def test_start_no_value(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def taskfunc(*, task_status: TaskStatus) -> None:\n            task_status.started()\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            future, value = portal.start_task(taskfunc)\n            assert value is None\n            assert future.result() is None\n\n    def test_start_with_value(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def taskfunc(*, task_status: TaskStatus) -> None:\n            task_status.started(\"foo\")\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            future, value = portal.start_task(taskfunc)\n            assert value == \"foo\"\n            assert future.result() is None\n\n    def test_start_crash_before_started_call(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def taskfunc(*, task_status: object) -> NoReturn:\n            raise Exception(\"foo\")\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            with pytest.raises(Exception, match=\"foo\"):\n                portal.start_task(taskfunc)\n\n    def test_start_crash_after_started_call(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def taskfunc(*, task_status: TaskStatus) -> NoReturn:\n            task_status.started(2)\n            raise Exception(\"foo\")\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            future, value = portal.start_task(taskfunc)\n            assert value == 2\n            with pytest.raises(Exception, match=\"foo\"):\n                future.result()\n\n    def test_start_no_started_call(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def taskfunc(*, task_status: TaskStatus) -> None:\n            pass\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            with pytest.raises(RuntimeError, match=\"Task exited\"):\n                portal.start_task(taskfunc)\n\n    def test_start_with_name(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        async def taskfunc(*, task_status: TaskStatus) -> None:\n            task_status.started(get_current_task().name)\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            future, start_value = portal.start_task(taskfunc, name=\"testname\")\n            assert start_value == \"testname\"\n\n    def test_contextvar_propagation_sync(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        var = ContextVar(\"var\", default=1)\n        var.set(6)\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            propagated_value = portal.call(var.get)\n\n        assert propagated_value == 6\n\n    def test_contextvar_propagation_async(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        var = ContextVar(\"var\", default=1)\n        var.set(6)\n\n        async def get_var() -> int:\n            await checkpoint()\n            return var.get()\n\n        with start_blocking_portal(anyio_backend_name, anyio_backend_options) as portal:\n            propagated_value = portal.call(get_var)\n\n        assert propagated_value == 6\n\n    @pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\n    async def test_asyncio_run_sync_called(self, caplog: LogCaptureFixture) -> None:\n        \"\"\"Regression test for #357.\"\"\"\n\n        async def in_loop() -> None:\n            raise CancelledError\n\n        async with BlockingPortal() as portal:\n            await to_thread.run_sync(portal.start_task_soon, in_loop)\n\n        assert not caplog.text\n\n    def test_raise_baseexception_from_task(\n        self, anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n    ) -> None:\n        \"\"\"\n        Test that when a task raises a BaseException, it does not trigger additional\n        exceptions when trying to close the portal.\n\n        \"\"\"\n\n        async def raise_baseexception() -> None:\n            raise BaseException(\"fatal error\")\n\n        with pytest.raises(BaseExceptionGroup) as outer_exc:\n            with start_blocking_portal(\n                anyio_backend_name, anyio_backend_options\n            ) as portal:\n                with pytest.raises(BaseException, match=\"fatal error\") as exc:\n                    portal.call(raise_baseexception)\n\n                assert exc.value.__context__ is None\n\n        assert len(outer_exc.value.exceptions) == 1\n        assert str(outer_exc.value.exceptions[0]) == \"fatal error\"\n\n    @pytest.mark.parametrize(\"portal_backend_name\", get_all_backends())\n    async def test_from_async(\n        self, anyio_backend_name: str, portal_backend_name: str\n    ) -> None:\n        \"\"\"\n        Test that portals don't deadlock when started/used from async code.\n\n        Note: This test will deadlock if there is a regression. A deadlock should be\n        treated as a failure. See also\n        https://github.com/agronholm/anyio/pull/524#discussion_r1183080886.\n\n        \"\"\"\n        if anyio_backend_name == \"trio\" and portal_backend_name == \"trio\":\n            pytest.xfail(\"known bug (#525)\")\n\n        with start_blocking_portal(portal_backend_name) as portal:\n            portal.call(checkpoint)\n\n    async def test_cancel_portal_future(self) -> None:\n        \"\"\"Regression test for #575.\"\"\"\n        event = Event()\n\n        def sync_thread() -> None:\n            fs = [portal.start_task_soon(sleep, math.inf)]\n            from_thread.run_sync(event.set)\n            done, not_done = futures.wait(\n                fs, timeout=1, return_when=futures.FIRST_COMPLETED\n            )\n            assert not not_done\n\n        async with from_thread.BlockingPortal() as portal:\n            async with create_task_group() as tg:\n                tg.start_soon(to_thread.run_sync, sync_thread)\n                # Ensure thread has time to start the task\n                await event.wait()\n                await portal.stop(cancel_remaining=True)\n", "tests/test_debugging.py": "from __future__ import annotations\n\nimport asyncio\nimport sys\nfrom collections.abc import AsyncGenerator, Coroutine, Generator\nfrom typing import Any, cast\n\nimport pytest\n\nimport anyio\nfrom anyio import (\n    Event,\n    TaskInfo,\n    create_task_group,\n    get_current_task,\n    get_running_tasks,\n    move_on_after,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import TaskStatus\n\npytestmark = pytest.mark.anyio\n\n\nget_coro = asyncio.Task.get_coro\n\n\ndef test_main_task_name(\n    anyio_backend_name: str, anyio_backend_options: dict[str, Any]\n) -> None:\n    task_name = None\n\n    async def main() -> None:\n        nonlocal task_name\n        task_name = get_current_task().name\n\n    anyio.run(main, backend=anyio_backend_name, backend_options=anyio_backend_options)\n    assert task_name == \"tests.test_debugging.test_main_task_name.<locals>.main\"\n\n    # Work around sniffio/asyncio bug that leaves behind an unclosed event loop\n    if anyio_backend_name == \"asyncio\":\n        import asyncio\n        import gc\n\n        for loop in [\n            obj\n            for obj in gc.get_objects()\n            if isinstance(obj, asyncio.AbstractEventLoop)\n        ]:\n            loop.close()\n\n\n@pytest.mark.parametrize(\n    \"name_input,expected\",\n    [\n        (None, \"tests.test_debugging.test_non_main_task_name.<locals>.non_main\"),\n        (b\"name\", \"b'name'\"),\n        (\"name\", \"name\"),\n        (\"\", \"\"),\n    ],\n)\nasync def test_non_main_task_name(\n    name_input: bytes | str | None, expected: str\n) -> None:\n    async def non_main(*, task_status: TaskStatus) -> None:\n        task_status.started(anyio.get_current_task().name)\n\n    async with anyio.create_task_group() as tg:\n        name = await tg.start(non_main, name=name_input)\n\n    assert name == expected\n\n\nasync def test_get_running_tasks() -> None:\n    async def inspect() -> None:\n        await wait_all_tasks_blocked()\n        new_tasks = set(get_running_tasks()) - existing_tasks\n        task_infos[:] = sorted(new_tasks, key=lambda info: info.name or \"\")\n        event.set()\n\n    event = Event()\n    task_infos: list[TaskInfo] = []\n    host_task = get_current_task()\n    async with create_task_group() as tg:\n        existing_tasks = set(get_running_tasks())\n        tg.start_soon(event.wait, name=\"task1\")\n        tg.start_soon(event.wait, name=\"task2\")\n        tg.start_soon(inspect)\n\n    assert len(task_infos) == 3\n    expected_names = [\n        \"task1\",\n        \"task2\",\n        \"tests.test_debugging.test_get_running_tasks.<locals>.inspect\",\n    ]\n    for task, expected_name in zip(task_infos, expected_names):\n        assert task.parent_id == host_task.id\n        assert task.name == expected_name\n        assert repr(task).endswith(f\"TaskInfo(id={task.id}, name={expected_name!r})\")\n\n\n@pytest.mark.skipif(\n    sys.version_info >= (3, 11),\n    reason=\"Generator based coroutines have been removed in Python 3.11\",\n)\n@pytest.mark.filterwarnings(\n    'ignore:\"@coroutine\" decorator is deprecated:DeprecationWarning'\n)\ndef test_wait_generator_based_task_blocked(\n    asyncio_event_loop: asyncio.AbstractEventLoop,\n) -> None:\n    async def native_coro_part() -> None:\n        await wait_all_tasks_blocked()\n        gen = cast(Generator, get_coro(gen_task))\n        assert not gen.gi_running\n        coro = cast(Coroutine, gen.gi_yieldfrom)\n        assert coro.cr_code.co_name == \"wait\"\n\n        event.set()\n\n    @asyncio.coroutine  # type: ignore[attr-defined]\n    def generator_part() -> Generator[object, BaseException, None]:\n        yield from event.wait()  # type: ignore[misc]\n\n    event = asyncio.Event()\n    gen_task: asyncio.Task[None] = asyncio_event_loop.create_task(generator_part())\n    asyncio_event_loop.run_until_complete(native_coro_part())\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"])\nasync def test_wait_all_tasks_blocked_asend(anyio_backend: str) -> None:\n    \"\"\"Test that wait_all_tasks_blocked() does not crash on an `asend()` object.\"\"\"\n\n    async def agen_func() -> AsyncGenerator[None, None]:\n        yield\n\n    agen = agen_func()\n    coro = agen.asend(None)\n    loop = asyncio.get_running_loop()\n    task = loop.create_task(cast(\"Coroutine[Any, Any, Any]\", coro))\n    await wait_all_tasks_blocked()\n    await task\n    await agen.aclose()\n\n\nasync def test_wait_all_tasks_blocked_cancelled_task() -> None:\n    done = False\n\n    async def self_cancel(*, task_status: TaskStatus) -> None:\n        nonlocal done\n        task_status.started()\n        with move_on_after(-1):\n            await Event().wait()\n\n        done = True\n\n    async with create_task_group() as tg:\n        await tg.start(self_cancel)\n        await wait_all_tasks_blocked()\n        assert done\n", "tests/streams/test_tls.py": "from __future__ import annotations\n\nimport socket\nimport ssl\nfrom contextlib import ExitStack\nfrom threading import Thread\nfrom typing import ContextManager, NoReturn\n\nimport pytest\nfrom pytest_mock import MockerFixture\nfrom trustme import CA\n\nfrom anyio import (\n    BrokenResourceError,\n    EndOfStream,\n    Event,\n    connect_tcp,\n    create_memory_object_stream,\n    create_task_group,\n    create_tcp_listener,\n    to_thread,\n)\nfrom anyio.abc import AnyByteStream, SocketAttribute, SocketStream\nfrom anyio.streams.stapled import StapledObjectStream\nfrom anyio.streams.tls import TLSAttribute, TLSListener, TLSStream\n\npytestmark = pytest.mark.anyio\n\n\nclass TestTLSStream:\n    async def test_send_receive(\n        self, server_context: ssl.SSLContext, client_context: ssl.SSLContext\n    ) -> None:\n        def serve_sync() -> None:\n            conn, addr = server_sock.accept()\n            conn.settimeout(1)\n            data = conn.recv(10)\n            conn.send(data[::-1])\n            conn.close()\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=False\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync)\n        server_thread.start()\n\n        async with await connect_tcp(*server_sock.getsockname()) as stream:\n            wrapper = await TLSStream.wrap(\n                stream, hostname=\"localhost\", ssl_context=client_context\n            )\n            await wrapper.send(b\"hello\")\n            response = await wrapper.receive()\n\n        server_thread.join()\n        server_sock.close()\n        assert response == b\"olleh\"\n\n    async def test_extra_attributes(\n        self, server_context: ssl.SSLContext, client_context: ssl.SSLContext\n    ) -> None:\n        def serve_sync() -> None:\n            conn, addr = server_sock.accept()\n            with conn:\n                conn.settimeout(1)\n                conn.recv(1)\n\n        server_context.set_alpn_protocols([\"h2\"])\n        client_context.set_alpn_protocols([\"h2\"])\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=True\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync)\n        server_thread.start()\n\n        async with await connect_tcp(*server_sock.getsockname()) as stream:\n            wrapper = await TLSStream.wrap(\n                stream,\n                hostname=\"localhost\",\n                ssl_context=client_context,\n                standard_compatible=False,\n            )\n            async with wrapper:\n                for name, attribute in SocketAttribute.__dict__.items():\n                    if not name.startswith(\"_\"):\n                        assert wrapper.extra(attribute) == stream.extra(attribute)\n\n                assert wrapper.extra(TLSAttribute.alpn_protocol) == \"h2\"\n                assert isinstance(\n                    wrapper.extra(TLSAttribute.channel_binding_tls_unique), bytes\n                )\n                assert isinstance(wrapper.extra(TLSAttribute.cipher), tuple)\n                assert isinstance(wrapper.extra(TLSAttribute.peer_certificate), dict)\n                assert isinstance(\n                    wrapper.extra(TLSAttribute.peer_certificate_binary), bytes\n                )\n                assert wrapper.extra(TLSAttribute.server_side) is False\n                assert wrapper.extra(TLSAttribute.shared_ciphers) is None\n                assert isinstance(wrapper.extra(TLSAttribute.ssl_object), ssl.SSLObject)\n                assert wrapper.extra(TLSAttribute.standard_compatible) is False\n                assert wrapper.extra(TLSAttribute.tls_version).startswith(\"TLSv\")\n                await wrapper.send(b\"\\x00\")\n\n        server_thread.join()\n        server_sock.close()\n\n    async def test_unwrap(\n        self, server_context: ssl.SSLContext, client_context: ssl.SSLContext\n    ) -> None:\n        def serve_sync() -> None:\n            conn, addr = server_sock.accept()\n            conn.settimeout(1)\n            conn.send(b\"encrypted\")\n            unencrypted = conn.unwrap()\n            unencrypted.send(b\"unencrypted\")\n            unencrypted.close()\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=False\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync)\n        server_thread.start()\n\n        async with await connect_tcp(*server_sock.getsockname()) as stream:\n            wrapper = await TLSStream.wrap(\n                stream, hostname=\"localhost\", ssl_context=client_context\n            )\n            msg1 = await wrapper.receive()\n            unwrapped_stream, msg2 = await wrapper.unwrap()\n            if msg2 != b\"unencrypted\":\n                msg2 += await unwrapped_stream.receive()\n\n        server_thread.join()\n        server_sock.close()\n        assert msg1 == b\"encrypted\"\n        assert msg2 == b\"unencrypted\"\n\n    @pytest.mark.skipif(not ssl.HAS_ALPN, reason=\"ALPN support not available\")\n    async def test_alpn_negotiation(\n        self, server_context: ssl.SSLContext, client_context: ssl.SSLContext\n    ) -> None:\n        def serve_sync() -> None:\n            conn, addr = server_sock.accept()\n            conn.settimeout(1)\n            selected_alpn_protocol = conn.selected_alpn_protocol()\n            assert selected_alpn_protocol is not None\n            conn.send(selected_alpn_protocol.encode())\n            conn.close()\n\n        server_context.set_alpn_protocols([\"dummy1\", \"dummy2\"])\n        client_context.set_alpn_protocols([\"dummy2\", \"dummy3\"])\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=False\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync)\n        server_thread.start()\n\n        async with await connect_tcp(*server_sock.getsockname()) as stream:\n            wrapper = await TLSStream.wrap(\n                stream, hostname=\"localhost\", ssl_context=client_context\n            )\n            assert wrapper.extra(TLSAttribute.alpn_protocol) == \"dummy2\"\n            server_alpn_protocol = await wrapper.receive()\n\n        server_thread.join()\n        server_sock.close()\n        assert server_alpn_protocol == b\"dummy2\"\n\n    @pytest.mark.parametrize(\n        \"server_compatible, client_compatible\",\n        [\n            pytest.param(True, True, id=\"both_standard\"),\n            pytest.param(True, False, id=\"server_standard\"),\n            pytest.param(False, True, id=\"client_standard\"),\n            pytest.param(False, False, id=\"neither_standard\"),\n        ],\n    )\n    async def test_ragged_eofs(\n        self,\n        server_context: ssl.SSLContext,\n        client_context: ssl.SSLContext,\n        server_compatible: bool,\n        client_compatible: bool,\n    ) -> None:\n        server_exc = None\n\n        def serve_sync() -> None:\n            nonlocal server_exc\n            conn, addr = server_sock.accept()\n            try:\n                conn.settimeout(1)\n                conn.sendall(b\"hello\")\n                if server_compatible:\n                    conn.unwrap()\n            except BaseException as exc:\n                server_exc = exc\n            finally:\n                conn.close()\n\n        client_cm: ContextManager = ExitStack()\n        if client_compatible and not server_compatible:\n            client_cm = pytest.raises(BrokenResourceError)\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(),\n            server_side=True,\n            suppress_ragged_eofs=not server_compatible,\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync, daemon=True)\n        server_thread.start()\n\n        async with await connect_tcp(*server_sock.getsockname()) as stream:\n            wrapper = await TLSStream.wrap(\n                stream,\n                hostname=\"localhost\",\n                ssl_context=client_context,\n                standard_compatible=client_compatible,\n            )\n            with client_cm:\n                assert await wrapper.receive() == b\"hello\"\n                await wrapper.aclose()\n\n        server_thread.join()\n        server_sock.close()\n        if not client_compatible and server_compatible:\n            assert isinstance(server_exc, OSError)\n            assert not isinstance(server_exc, socket.timeout)\n        else:\n            assert server_exc is None\n\n    async def test_ragged_eof_on_receive(\n        self, server_context: ssl.SSLContext, client_context: ssl.SSLContext\n    ) -> None:\n        server_exc = None\n\n        def serve_sync() -> None:\n            nonlocal server_exc\n            conn, addr = server_sock.accept()\n            try:\n                conn.settimeout(1)\n                conn.sendall(b\"hello\")\n            except BaseException as exc:\n                server_exc = exc\n            finally:\n                conn.close()\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=True\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync, daemon=True)\n        server_thread.start()\n        try:\n            async with await connect_tcp(*server_sock.getsockname()) as stream:\n                wrapper = await TLSStream.wrap(\n                    stream,\n                    hostname=\"localhost\",\n                    ssl_context=client_context,\n                    standard_compatible=False,\n                )\n                assert await wrapper.receive() == b\"hello\"\n                with pytest.raises(EndOfStream):\n                    await wrapper.receive()\n        finally:\n            server_thread.join()\n            server_sock.close()\n\n        assert server_exc is None\n\n    async def test_receive_send_after_eof(\n        self, server_context: ssl.SSLContext, client_context: ssl.SSLContext\n    ) -> None:\n        def serve_sync() -> None:\n            conn, addr = server_sock.accept()\n            conn.sendall(b\"hello\")\n            conn.unwrap()\n            conn.close()\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=False\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync, daemon=True)\n        server_thread.start()\n\n        stream = await connect_tcp(*server_sock.getsockname())\n        async with await TLSStream.wrap(\n            stream, hostname=\"localhost\", ssl_context=client_context\n        ) as wrapper:\n            assert await wrapper.receive() == b\"hello\"\n            with pytest.raises(EndOfStream):\n                await wrapper.receive()\n\n        server_thread.join()\n        server_sock.close()\n\n    @pytest.mark.parametrize(\n        \"force_tlsv12\",\n        [\n            pytest.param(\n                False,\n                marks=[\n                    pytest.mark.skipif(\n                        not getattr(ssl, \"HAS_TLSv1_3\", False),\n                        reason=\"No TLS 1.3 support\",\n                    )\n                ],\n            ),\n            pytest.param(True),\n        ],\n        ids=[\"tlsv13\", \"tlsv12\"],\n    )\n    async def test_send_eof_not_implemented(\n        self, server_context: ssl.SSLContext, ca: CA, force_tlsv12: bool\n    ) -> None:\n        def serve_sync() -> None:\n            conn, addr = server_sock.accept()\n            conn.sendall(b\"hello\")\n            conn.unwrap()\n            conn.close()\n\n        client_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n        ca.configure_trust(client_context)\n        if force_tlsv12:\n            expected_pattern = r\"send_eof\\(\\) requires at least TLSv1.3\"\n            client_context.maximum_version = ssl.TLSVersion.TLSv1_2\n        else:\n            expected_pattern = (\n                r\"send_eof\\(\\) has not yet been implemented for TLS streams\"\n            )\n\n        server_sock = server_context.wrap_socket(\n            socket.socket(), server_side=True, suppress_ragged_eofs=False\n        )\n        server_sock.settimeout(1)\n        server_sock.bind((\"127.0.0.1\", 0))\n        server_sock.listen()\n        server_thread = Thread(target=serve_sync, daemon=True)\n        server_thread.start()\n\n        stream = await connect_tcp(*server_sock.getsockname())\n        async with await TLSStream.wrap(\n            stream, hostname=\"localhost\", ssl_context=client_context\n        ) as wrapper:\n            assert await wrapper.receive() == b\"hello\"\n            with pytest.raises(NotImplementedError) as exc:\n                await wrapper.send_eof()\n\n            exc.match(expected_pattern)\n\n        server_thread.join()\n        server_sock.close()\n\n    @pytest.mark.skipif(\n        not hasattr(ssl, \"OP_IGNORE_UNEXPECTED_EOF\"),\n        reason=\"The ssl module does not have the OP_IGNORE_UNEXPECTED_EOF attribute\",\n    )\n    async def test_default_context_ignore_unexpected_eof_flag_off(\n        self, mocker: MockerFixture\n    ) -> None:\n        send1, receive1 = create_memory_object_stream[bytes]()\n        client_stream = StapledObjectStream(send1, receive1)\n        mocker.patch.object(TLSStream, \"_call_sslobject_method\")\n        tls_stream = await TLSStream.wrap(client_stream)\n        ssl_context = tls_stream.extra(TLSAttribute.ssl_object).context\n        assert not ssl_context.options & ssl.OP_IGNORE_UNEXPECTED_EOF\n\n        send1.close()\n        receive1.close()\n\n\nclass TestTLSListener:\n    async def test_handshake_fail(\n        self, server_context: ssl.SSLContext, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        def handler(stream: object) -> NoReturn:\n            pytest.fail(\"This function should never be called in this scenario\")\n\n        exception = None\n\n        class CustomTLSListener(TLSListener):\n            @staticmethod\n            async def handle_handshake_error(\n                exc: BaseException, stream: AnyByteStream\n            ) -> None:\n                nonlocal exception\n                await TLSListener.handle_handshake_error(exc, stream)\n\n                # Regression test for #608\n                assert len(caplog.records) == 1\n                logged_exc_info = caplog.records[0].exc_info\n                logged_exc = logged_exc_info[1] if logged_exc_info is not None else None\n                assert logged_exc is exc\n\n                assert isinstance(stream, SocketStream)\n                exception = exc\n                event.set()\n\n        event = Event()\n        listener = await create_tcp_listener(local_host=\"127.0.0.1\")\n        tls_listener = CustomTLSListener(listener, server_context)\n        async with tls_listener, create_task_group() as tg:\n            tg.start_soon(tls_listener.serve, handler)\n            sock = socket.socket()\n            sock.connect(listener.extra(SocketAttribute.local_address))\n            sock.close()\n            await event.wait()\n            tg.cancel_scope.cancel()\n\n        assert isinstance(exception, BrokenResourceError)\n\n    async def test_extra_attributes(\n        self, client_context: ssl.SSLContext, server_context: ssl.SSLContext, ca: CA\n    ) -> None:\n        def connect_sync(addr: tuple[str, int]) -> None:\n            with socket.create_connection(addr) as plain_sock:\n                plain_sock.settimeout(2)\n                with client_context.wrap_socket(\n                    plain_sock,\n                    server_side=False,\n                    server_hostname=\"localhost\",\n                    suppress_ragged_eofs=False,\n                ) as conn:\n                    conn.recv(1)\n                    conn.unwrap()\n\n        class CustomTLSListener(TLSListener):\n            @staticmethod\n            async def handle_handshake_error(\n                exc: BaseException, stream: AnyByteStream\n            ) -> None:\n                await TLSListener.handle_handshake_error(exc, stream)\n                pytest.fail(\"TLS handshake failed\")\n\n        async def handler(stream: TLSStream) -> None:\n            async with stream:\n                try:\n                    assert stream.extra(TLSAttribute.alpn_protocol) == \"h2\"\n                    assert isinstance(\n                        stream.extra(TLSAttribute.channel_binding_tls_unique), bytes\n                    )\n                    assert isinstance(stream.extra(TLSAttribute.cipher), tuple)\n                    assert isinstance(stream.extra(TLSAttribute.peer_certificate), dict)\n                    assert isinstance(\n                        stream.extra(TLSAttribute.peer_certificate_binary), bytes\n                    )\n                    assert stream.extra(TLSAttribute.server_side) is True\n                    shared_ciphers = stream.extra(TLSAttribute.shared_ciphers)\n                    assert isinstance(shared_ciphers, list)\n                    assert len(shared_ciphers) > 1\n                    assert isinstance(\n                        stream.extra(TLSAttribute.ssl_object), ssl.SSLObject\n                    )\n                    assert stream.extra(TLSAttribute.standard_compatible) is True\n                    assert stream.extra(TLSAttribute.tls_version).startswith(\"TLSv\")\n                finally:\n                    event.set()\n                    await stream.send(b\"\\x00\")\n\n        # Issue a client certificate and make the server trust it\n        client_cert = ca.issue_cert(\"dummy-client\")\n        client_cert.configure_cert(client_context)\n        ca.configure_trust(server_context)\n        server_context.verify_mode = ssl.CERT_REQUIRED\n\n        event = Event()\n        server_context.set_alpn_protocols([\"h2\"])\n        client_context.set_alpn_protocols([\"h2\"])\n        listener = await create_tcp_listener(local_host=\"127.0.0.1\")\n        tls_listener = CustomTLSListener(listener, server_context)\n        async with tls_listener, create_task_group() as tg:\n            assert tls_listener.extra(TLSAttribute.standard_compatible) is True\n            tg.start_soon(tls_listener.serve, handler)\n            client_thread = Thread(\n                target=connect_sync,\n                args=[listener.extra(SocketAttribute.local_address)],\n            )\n            client_thread.start()\n            await event.wait()\n            await to_thread.run_sync(client_thread.join)\n            tg.cancel_scope.cancel()\n", "tests/streams/test_memory.py": "from __future__ import annotations\n\nimport gc\nimport sys\nfrom typing import NoReturn\n\nimport pytest\n\nfrom anyio import (\n    BrokenResourceError,\n    CancelScope,\n    ClosedResourceError,\n    EndOfStream,\n    WouldBlock,\n    create_memory_object_stream,\n    create_task_group,\n    fail_after,\n    wait_all_tasks_blocked,\n)\nfrom anyio.abc import ObjectReceiveStream, ObjectSendStream, TaskStatus\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import ExceptionGroup\n\npytestmark = pytest.mark.anyio\n\n\ndef test_invalid_max_buffer() -> None:\n    pytest.raises(ValueError, create_memory_object_stream, 1.0).match(\n        \"max_buffer_size must be either an integer or math.inf\"\n    )\n\n\ndef test_negative_max_buffer() -> None:\n    pytest.raises(ValueError, create_memory_object_stream, -1).match(\n        \"max_buffer_size cannot be negative\"\n    )\n\n\nasync def test_receive_then_send() -> None:\n    async def receiver() -> None:\n        received_objects.append(await receive.receive())\n        received_objects.append(await receive.receive())\n\n    send, receive = create_memory_object_stream[str](0)\n    received_objects: list[str] = []\n    async with create_task_group() as tg:\n        tg.start_soon(receiver)\n        await wait_all_tasks_blocked()\n        await send.send(\"hello\")\n        await send.send(\"anyio\")\n\n    assert received_objects == [\"hello\", \"anyio\"]\n\n    send.close()\n    receive.close()\n\n\nasync def test_receive_then_send_nowait() -> None:\n    async def receiver() -> None:\n        received_objects.append(await receive.receive())\n\n    send, receive = create_memory_object_stream[str](0)\n    received_objects: list[str] = []\n    async with create_task_group() as tg:\n        tg.start_soon(receiver)\n        tg.start_soon(receiver)\n        await wait_all_tasks_blocked()\n        send.send_nowait(\"hello\")\n        send.send_nowait(\"anyio\")\n\n    assert sorted(received_objects, reverse=True) == [\"hello\", \"anyio\"]\n\n    send.close()\n    receive.close()\n\n\nasync def test_send_then_receive_nowait() -> None:\n    send, receive = create_memory_object_stream[str](0)\n    async with create_task_group() as tg:\n        tg.start_soon(send.send, \"hello\")\n        await wait_all_tasks_blocked()\n        assert receive.receive_nowait() == \"hello\"\n\n    send.close()\n    receive.close()\n\n\nasync def test_send_is_unblocked_after_receive_nowait() -> None:\n    send, receive = create_memory_object_stream[str](1)\n    send.send_nowait(\"hello\")\n\n    with fail_after(1):\n        async with create_task_group() as tg:\n            tg.start_soon(send.send, \"anyio\")\n            await wait_all_tasks_blocked()\n            assert receive.receive_nowait() == \"hello\"\n\n    assert receive.receive_nowait() == \"anyio\"\n\n    send.close()\n    receive.close()\n\n\nasync def test_send_nowait_then_receive_nowait() -> None:\n    send, receive = create_memory_object_stream[str](2)\n    send.send_nowait(\"hello\")\n    send.send_nowait(\"anyio\")\n    assert receive.receive_nowait() == \"hello\"\n    assert receive.receive_nowait() == \"anyio\"\n\n    send.close()\n    receive.close()\n\n\nasync def test_iterate() -> None:\n    async def receiver() -> None:\n        async for item in receive:\n            received_objects.append(item)\n\n    send, receive = create_memory_object_stream[str]()\n    received_objects: list[str] = []\n    async with create_task_group() as tg:\n        tg.start_soon(receiver)\n        await send.send(\"hello\")\n        await send.send(\"anyio\")\n        await send.aclose()\n\n    assert received_objects == [\"hello\", \"anyio\"]\n\n    send.close()\n    receive.close()\n\n\nasync def test_receive_send_closed_send_stream() -> None:\n    send, receive = create_memory_object_stream[None]()\n    await send.aclose()\n    with pytest.raises(EndOfStream):\n        receive.receive_nowait()\n\n    with pytest.raises(ClosedResourceError):\n        await send.send(None)\n\n    receive.close()\n\n\nasync def test_receive_send_closed_receive_stream() -> None:\n    send, receive = create_memory_object_stream[None]()\n    await receive.aclose()\n    with pytest.raises(ClosedResourceError):\n        receive.receive_nowait()\n\n    with pytest.raises(BrokenResourceError):\n        await send.send(None)\n\n    send.close()\n\n\nasync def test_cancel_receive() -> None:\n    send, receive = create_memory_object_stream[str]()\n    async with create_task_group() as tg:\n        tg.start_soon(receive.receive)\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    with pytest.raises(WouldBlock):\n        send.send_nowait(\"hello\")\n\n    send.close()\n    receive.close()\n\n\nasync def test_cancel_send() -> None:\n    send, receive = create_memory_object_stream[str]()\n    async with create_task_group() as tg:\n        tg.start_soon(send.send, \"hello\")\n        await wait_all_tasks_blocked()\n        tg.cancel_scope.cancel()\n\n    with pytest.raises(WouldBlock):\n        receive.receive_nowait()\n\n    send.close()\n    receive.close()\n\n\nasync def test_clone() -> None:\n    send1, receive1 = create_memory_object_stream[str](1)\n    send2 = send1.clone()\n    receive2 = receive1.clone()\n    await send1.aclose()\n    await receive1.aclose()\n    send2.send_nowait(\"hello\")\n    assert receive2.receive_nowait() == \"hello\"\n\n    send1.close()\n    receive1.close()\n    send2.close()\n    receive2.close()\n\n\nasync def test_clone_closed() -> None:\n    send, receive = create_memory_object_stream[NoReturn](1)\n    await send.aclose()\n    await receive.aclose()\n    pytest.raises(ClosedResourceError, send.clone)\n    pytest.raises(ClosedResourceError, receive.clone)\n\n\nasync def test_close_send_while_receiving() -> None:\n    send, receive = create_memory_object_stream[NoReturn](1)\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(receive.receive)\n            await wait_all_tasks_blocked()\n            await send.aclose()\n\n    assert len(exc.value.exceptions) == 1\n    assert isinstance(exc.value.exceptions[0], EndOfStream)\n\n    send.close()\n    receive.close()\n\n\nasync def test_close_receive_while_sending() -> None:\n    # We send None here as a regression test for #731\n    send, receive = create_memory_object_stream[None](0)\n    with pytest.raises(ExceptionGroup) as exc:\n        async with create_task_group() as tg:\n            tg.start_soon(send.send, None)\n            await wait_all_tasks_blocked()\n            await receive.aclose()\n\n    assert len(exc.value.exceptions) == 1\n    assert isinstance(exc.value.exceptions[0], BrokenResourceError)\n\n    send.close()\n    receive.close()\n\n\nasync def test_receive_after_send_closed() -> None:\n    send, receive = create_memory_object_stream[str](1)\n    await send.send(\"hello\")\n    await send.aclose()\n    assert await receive.receive() == \"hello\"\n\n    send.close()\n    receive.close()\n\n\nasync def test_receive_when_cancelled() -> None:\n    \"\"\"\n    Test that calling receive() in a cancelled scope prevents it from going through with\n    the operation.\n\n    \"\"\"\n    send, receive = create_memory_object_stream[str]()\n    async with create_task_group() as tg:\n        tg.start_soon(send.send, \"hello\")\n        await wait_all_tasks_blocked()\n        tg.start_soon(send.send, \"world\")\n        await wait_all_tasks_blocked()\n\n        with CancelScope() as scope:\n            scope.cancel()\n            await receive.receive()\n\n        assert await receive.receive() == \"hello\"\n        assert await receive.receive() == \"world\"\n\n    send.close()\n    receive.close()\n\n\nasync def test_send_when_cancelled() -> None:\n    \"\"\"\n    Test that calling send() in a cancelled scope prevents it from going through with\n    the operation.\n\n    \"\"\"\n\n    async def receiver() -> None:\n        received.append(await receive.receive())\n\n    received: list[str] = []\n    send, receive = create_memory_object_stream[str]()\n    async with create_task_group() as tg:\n        tg.start_soon(receiver)\n        with CancelScope() as scope:\n            scope.cancel()\n            await send.send(\"hello\")\n\n        await send.send(\"world\")\n\n    assert received == [\"world\"]\n\n    send.close()\n    receive.close()\n\n\nasync def test_cancel_during_receive() -> None:\n    \"\"\"\n    Test that cancelling a pending receive() operation does not cause an item in the\n    stream to be lost.\n\n    \"\"\"\n\n    async def scoped_receiver(task_status: TaskStatus[CancelScope]) -> None:\n        with CancelScope() as cancel_scope:\n            task_status.started(cancel_scope)\n            received.append(await receive.receive())\n\n        assert cancel_scope.cancel_called\n\n    received: list[str] = []\n    send, receive = create_memory_object_stream[str]()\n    with send, receive:\n        async with create_task_group() as tg:\n            receiver_scope = await tg.start(scoped_receiver)\n            await wait_all_tasks_blocked()\n            send.send_nowait(\"hello\")\n            receiver_scope.cancel()\n\n    assert received == [\"hello\"]\n\n\nasync def test_cancel_during_receive_buffered() -> None:\n    \"\"\"\n    Test that sending an item to a memory object stream when the receiver that is next\n    in line has been cancelled will not result in the item being lost.\n    \"\"\"\n\n    async def scoped_receiver(\n        receive: MemoryObjectReceiveStream[str], task_status: TaskStatus[CancelScope]\n    ) -> None:\n        with CancelScope() as cancel_scope:\n            task_status.started(cancel_scope)\n            await receive.receive()\n\n    send, receive = create_memory_object_stream[str](1)\n    with send, receive:\n        async with create_task_group() as tg:\n            cancel_scope = await tg.start(scoped_receiver, receive)\n            await wait_all_tasks_blocked()\n            cancel_scope.cancel()\n            send.send_nowait(\"item\")\n\n        # Since the item was not sent to the cancelled task, it should be available here\n        assert receive.receive_nowait() == \"item\"\n\n\nasync def test_close_receive_after_send() -> None:\n    async def send() -> None:\n        async with send_stream:\n            await send_stream.send(\"test\")\n\n    async def receive() -> None:\n        async with receive_stream:\n            assert await receive_stream.receive() == \"test\"\n\n    send_stream, receive_stream = create_memory_object_stream[str]()\n    async with create_task_group() as tg:\n        tg.start_soon(send)\n        tg.start_soon(receive)\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_statistics() -> None:\n    send_stream, receive_stream = create_memory_object_stream[None](1)\n    streams: list[MemoryObjectReceiveStream[None] | MemoryObjectSendStream[None]] = [\n        send_stream,\n        receive_stream,\n    ]\n    for stream in streams:\n        statistics = stream.statistics()\n        assert statistics.max_buffer_size == 1\n        assert statistics.current_buffer_used == 0\n        assert statistics.open_send_streams == 1\n        assert statistics.open_receive_streams == 1\n        assert statistics.tasks_waiting_send == 0\n        assert statistics.tasks_waiting_receive == 0\n\n    for stream in streams:\n        async with create_task_group() as tg:\n            # Test tasks_waiting_send\n            send_stream.send_nowait(None)\n            assert stream.statistics().current_buffer_used == 1\n            tg.start_soon(send_stream.send, None)\n            await wait_all_tasks_blocked()\n            assert stream.statistics().current_buffer_used == 1\n            assert stream.statistics().tasks_waiting_send == 1\n            receive_stream.receive_nowait()\n            assert stream.statistics().current_buffer_used == 1\n            assert stream.statistics().tasks_waiting_send == 0\n            receive_stream.receive_nowait()\n            assert stream.statistics().current_buffer_used == 0\n\n            # Test tasks_waiting_receive\n            tg.start_soon(receive_stream.receive)\n            await wait_all_tasks_blocked()\n            assert stream.statistics().tasks_waiting_receive == 1\n            send_stream.send_nowait(None)\n            assert stream.statistics().tasks_waiting_receive == 0\n\n        async with create_task_group() as tg:\n            # Test tasks_waiting_send\n            send_stream.send_nowait(None)\n            assert stream.statistics().tasks_waiting_send == 0\n            for _ in range(3):\n                tg.start_soon(send_stream.send, None)\n\n            await wait_all_tasks_blocked()\n            assert stream.statistics().tasks_waiting_send == 3\n            for i in range(2, -1, -1):\n                receive_stream.receive_nowait()\n                assert stream.statistics().tasks_waiting_send == i\n\n            receive_stream.receive_nowait()\n\n        assert stream.statistics().current_buffer_used == 0\n        assert stream.statistics().tasks_waiting_send == 0\n        assert stream.statistics().tasks_waiting_receive == 0\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_sync_close() -> None:\n    send_stream, receive_stream = create_memory_object_stream[None](1)\n    with send_stream, receive_stream:\n        pass\n\n    with pytest.raises(ClosedResourceError):\n        send_stream.send_nowait(None)\n\n    with pytest.raises(ClosedResourceError):\n        receive_stream.receive_nowait()\n\n\nasync def test_type_variance() -> None:\n    \"\"\"\n    This test does not do anything at run time, but since the test suite is also checked\n    with a static type checker, it ensures that the memory object stream\n    co/contravariance works as intended. If it doesn't, one or both of the following\n    reassignments will trip the type checker.\n\n    \"\"\"\n    send, receive = create_memory_object_stream[float]()\n    receive1: MemoryObjectReceiveStream[complex] = receive  # noqa: F841\n    receive2: ObjectReceiveStream[complex] = receive  # noqa: F841\n    send1: MemoryObjectSendStream[int] = send  # noqa: F841\n    send2: ObjectSendStream[int] = send  # noqa: F841\n\n    send.close()\n    receive.close()\n\n\nasync def test_deprecated_item_type_parameter() -> None:\n    with pytest.warns(DeprecationWarning, match=\"item_type argument has been \"):\n        send, receive = create_memory_object_stream(item_type=int)  # type: ignore[var-annotated]\n\n        send.close()\n        receive.close()\n\n\nasync def test_not_closed_warning() -> None:\n    send, receive = create_memory_object_stream[int]()\n\n    with pytest.warns(\n        ResourceWarning, match=\"Unclosed <MemoryObjectSendStream at [0-9a-f]+>\"\n    ):\n        del send\n        gc.collect()\n\n    with pytest.warns(\n        ResourceWarning, match=\"Unclosed <MemoryObjectReceiveStream at [0-9a-f]+>\"\n    ):\n        del receive\n        gc.collect()\n\n\n@pytest.mark.parametrize(\"anyio_backend\", [\"asyncio\"], indirect=True)\nasync def test_send_to_natively_cancelled_receiver() -> None:\n    \"\"\"\n    Test that if a task waiting on receive.receive() is cancelled and then another\n    task sends an item, said item is not delivered to the task with a pending\n    cancellation, but rather to the next one in line.\n\n    \"\"\"\n    from asyncio import CancelledError, create_task\n\n    send, receive = create_memory_object_stream[str](1)\n    with send, receive:\n        receive_task = create_task(receive.receive())\n        await wait_all_tasks_blocked()  # ensure that the task is waiting to receive\n        receive_task.cancel()\n        send.send_nowait(\"hello\")\n        with pytest.raises(CancelledError):\n            await receive_task\n\n        assert receive.receive_nowait() == \"hello\"\n", "tests/streams/test_file.py": "from __future__ import annotations\n\nfrom pathlib import Path\n\nimport pytest\nfrom _pytest.fixtures import SubRequest\nfrom _pytest.tmpdir import TempPathFactory\n\nfrom anyio import ClosedResourceError, EndOfStream\nfrom anyio.abc import ByteReceiveStream\nfrom anyio.streams.file import FileReadStream, FileStreamAttribute, FileWriteStream\n\npytestmark = pytest.mark.anyio\n\n\nclass TestFileReadStream:\n    @pytest.fixture(scope=\"class\")\n    def file_path(self, tmp_path_factory: TempPathFactory) -> Path:\n        path = tmp_path_factory.mktemp(\"filestream\") / \"data.txt\"\n        path.write_text(\"Hello\")\n        return path\n\n    @pytest.fixture(params=[False, True], ids=[\"str\", \"path\"])\n    def file_path_or_str(self, request: SubRequest, file_path: Path) -> Path | str:\n        return file_path if request.param else str(file_path)\n\n    async def _run_filestream_test(self, stream: ByteReceiveStream) -> None:\n        assert await stream.receive(3) == b\"Hel\"\n        assert await stream.receive(3) == b\"lo\"\n        with pytest.raises(EndOfStream):\n            await stream.receive(1)\n\n    async def test_read_file_as_path(self, file_path_or_str: Path | str) -> None:\n        async with await FileReadStream.from_path(file_path_or_str) as stream:\n            await self._run_filestream_test(stream)\n\n    async def test_read_file(self, file_path: Path) -> None:\n        with file_path.open(\"rb\") as file:\n            async with FileReadStream(file) as stream:\n                await self._run_filestream_test(stream)\n\n    async def test_read_after_close(self, file_path: Path) -> None:\n        async with await FileReadStream.from_path(file_path) as stream:\n            pass\n\n        with pytest.raises(ClosedResourceError):\n            await stream.receive()\n\n    async def test_seek(self, file_path: Path) -> None:\n        with file_path.open(\"rb\") as file:\n            async with FileReadStream(file) as stream:\n                await stream.seek(2)\n                assert await stream.tell() == 2\n                data = await stream.receive()\n                assert data == b\"llo\"\n                assert await stream.tell() == 5\n\n    async def test_extra_attributes(self, file_path: Path) -> None:\n        async with await FileReadStream.from_path(file_path) as stream:\n            path = stream.extra(FileStreamAttribute.path)\n            assert path == file_path\n\n            fileno = stream.extra(FileStreamAttribute.fileno)\n            assert fileno > 2\n\n            file = stream.extra(FileStreamAttribute.file)\n            assert file.fileno() == fileno\n\n\nclass TestFileWriteStream:\n    @pytest.fixture\n    def file_path(self, tmp_path: Path) -> Path:\n        return tmp_path / \"written_data.txt\"\n\n    async def test_write_file(self, file_path: Path) -> None:\n        async with await FileWriteStream.from_path(file_path) as stream:\n            await stream.send(b\"Hel\")\n            await stream.send(b\"lo\")\n\n        assert file_path.read_text() == \"Hello\"\n\n    async def test_append_file(self, file_path: Path) -> None:\n        file_path.write_text(\"Hello\")\n        async with await FileWriteStream.from_path(file_path, True) as stream:\n            await stream.send(b\", World!\")\n\n        assert file_path.read_text() == \"Hello, World!\"\n\n    async def test_write_after_close(self, file_path: Path) -> None:\n        async with await FileWriteStream.from_path(file_path, True) as stream:\n            pass\n\n        with pytest.raises(ClosedResourceError):\n            await stream.send(b\"foo\")\n\n    async def test_extra_attributes(self, file_path: Path) -> None:\n        async with await FileWriteStream.from_path(file_path) as stream:\n            path = stream.extra(FileStreamAttribute.path)\n            assert path == file_path\n\n            fileno = stream.extra(FileStreamAttribute.fileno)\n            assert fileno > 2\n\n            file = stream.extra(FileStreamAttribute.file)\n            assert file.fileno() == fileno\n", "tests/streams/test_stapled.py": "from __future__ import annotations\n\nfrom collections import deque\nfrom dataclasses import InitVar, dataclass, field\nfrom typing import Iterable, TypeVar\n\nimport pytest\n\nfrom anyio import ClosedResourceError, EndOfStream\nfrom anyio.abc import (\n    ByteReceiveStream,\n    ByteSendStream,\n    ObjectReceiveStream,\n    ObjectSendStream,\n)\nfrom anyio.streams.stapled import StapledByteStream, StapledObjectStream\n\npytestmark = pytest.mark.anyio\n\n\n@dataclass\nclass DummyByteReceiveStream(ByteReceiveStream):\n    data: InitVar[bytes]\n    buffer: bytearray = field(init=False)\n    _closed: bool = field(init=False, default=False)\n\n    def __post_init__(self, data: bytes) -> None:\n        self.buffer = bytearray(data)\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        if self._closed:\n            raise ClosedResourceError\n\n        data = bytes(self.buffer[:max_bytes])\n        del self.buffer[:max_bytes]\n        return data\n\n    async def aclose(self) -> None:\n        self._closed = True\n\n\n@dataclass\nclass DummyByteSendStream(ByteSendStream):\n    buffer: bytearray = field(init=False, default_factory=bytearray)\n    _closed: bool = field(init=False, default=False)\n\n    async def send(self, item: bytes) -> None:\n        if self._closed:\n            raise ClosedResourceError\n\n        self.buffer.extend(item)\n\n    async def aclose(self) -> None:\n        self._closed = True\n\n\nclass TestStapledByteStream:\n    @pytest.fixture\n    def send_stream(self) -> DummyByteSendStream:\n        return DummyByteSendStream()\n\n    @pytest.fixture\n    def receive_stream(self) -> DummyByteReceiveStream:\n        return DummyByteReceiveStream(b\"hello, world\")\n\n    @pytest.fixture\n    def stapled(\n        self, send_stream: DummyByteSendStream, receive_stream: DummyByteReceiveStream\n    ) -> StapledByteStream:\n        return StapledByteStream(send_stream, receive_stream)\n\n    async def test_receive_send(\n        self, stapled: StapledByteStream, send_stream: DummyByteSendStream\n    ) -> None:\n        assert await stapled.receive(3) == b\"hel\"\n        assert await stapled.receive() == b\"lo, world\"\n        assert await stapled.receive() == b\"\"\n\n        await stapled.send(b\"how are you \")\n        await stapled.send(b\"today?\")\n        assert stapled.send_stream is send_stream\n        assert bytes(send_stream.buffer) == b\"how are you today?\"\n\n    async def test_send_eof(self, stapled: StapledByteStream) -> None:\n        await stapled.send_eof()\n        await stapled.send_eof()\n        with pytest.raises(ClosedResourceError):\n            await stapled.send(b\"world\")\n\n        assert await stapled.receive() == b\"hello, world\"\n\n    async def test_aclose(self, stapled: StapledByteStream) -> None:\n        await stapled.aclose()\n        with pytest.raises(ClosedResourceError):\n            await stapled.receive()\n        with pytest.raises(ClosedResourceError):\n            await stapled.send(b\"\")\n\n\nT_Item = TypeVar(\"T_Item\")\n\n\n@dataclass\nclass DummyObjectReceiveStream(ObjectReceiveStream[T_Item]):\n    data: InitVar[Iterable[T_Item]]\n    buffer: deque[T_Item] = field(init=False)\n    _closed: bool = field(init=False, default=False)\n\n    def __post_init__(self, data: Iterable[T_Item]) -> None:\n        self.buffer = deque(data)\n\n    async def receive(self) -> T_Item:\n        if self._closed:\n            raise ClosedResourceError\n        if not self.buffer:\n            raise EndOfStream\n\n        return self.buffer.popleft()\n\n    async def aclose(self) -> None:\n        self._closed = True\n\n\n@dataclass\nclass DummyObjectSendStream(ObjectSendStream[T_Item]):\n    buffer: list[T_Item] = field(init=False, default_factory=list)\n    _closed: bool = field(init=False, default=False)\n\n    async def send(self, item: T_Item) -> None:\n        if self._closed:\n            raise ClosedResourceError\n\n        self.buffer.append(item)\n\n    async def aclose(self) -> None:\n        self._closed = True\n\n\nclass TestStapledObjectStream:\n    @pytest.fixture\n    def receive_stream(self) -> DummyObjectReceiveStream[str]:\n        return DummyObjectReceiveStream([\"hello\", \"world\"])\n\n    @pytest.fixture\n    def send_stream(self) -> DummyObjectSendStream[str]:\n        return DummyObjectSendStream[str]()\n\n    @pytest.fixture\n    def stapled(\n        self,\n        receive_stream: DummyObjectReceiveStream[str],\n        send_stream: DummyObjectSendStream[str],\n    ) -> StapledObjectStream[str]:\n        return StapledObjectStream(send_stream, receive_stream)\n\n    async def test_receive_send(\n        self, stapled: StapledObjectStream[str], send_stream: DummyObjectSendStream[str]\n    ) -> None:\n        assert await stapled.receive() == \"hello\"\n        assert await stapled.receive() == \"world\"\n        with pytest.raises(EndOfStream):\n            await stapled.receive()\n\n        await stapled.send(\"how are you \")\n        await stapled.send(\"today?\")\n        assert stapled.send_stream is send_stream\n        assert send_stream.buffer == [\"how are you \", \"today?\"]\n\n    async def test_send_eof(self, stapled: StapledObjectStream[str]) -> None:\n        await stapled.send_eof()\n        await stapled.send_eof()\n        with pytest.raises(ClosedResourceError):\n            await stapled.send(\"world\")\n\n        assert await stapled.receive() == \"hello\"\n        assert await stapled.receive() == \"world\"\n\n    async def test_aclose(self, stapled: StapledObjectStream[str]) -> None:\n        await stapled.aclose()\n        with pytest.raises(ClosedResourceError):\n            await stapled.receive()\n        with pytest.raises(ClosedResourceError):\n            await stapled.send(b\"\")  # type: ignore[arg-type]\n", "tests/streams/test_buffered.py": "from __future__ import annotations\n\nimport pytest\n\nfrom anyio import IncompleteRead, create_memory_object_stream\nfrom anyio.streams.buffered import BufferedByteReceiveStream\n\npytestmark = pytest.mark.anyio\n\n\nasync def test_receive_exactly() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](2)\n    buffered_stream = BufferedByteReceiveStream(receive_stream)\n    await send_stream.send(b\"abcd\")\n    await send_stream.send(b\"efgh\")\n    result = await buffered_stream.receive_exactly(8)\n    assert result == b\"abcdefgh\"\n    assert isinstance(result, bytes)\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_receive_exactly_incomplete() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    buffered_stream = BufferedByteReceiveStream(receive_stream)\n    await send_stream.send(b\"abcd\")\n    await send_stream.aclose()\n    with pytest.raises(IncompleteRead):\n        await buffered_stream.receive_exactly(8)\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_receive_until() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](2)\n    buffered_stream = BufferedByteReceiveStream(receive_stream)\n    await send_stream.send(b\"abcd\")\n    await send_stream.send(b\"efgh\")\n\n    result = await buffered_stream.receive_until(b\"de\", 10)\n    assert result == b\"abc\"\n    assert isinstance(result, bytes)\n\n    result = await buffered_stream.receive_until(b\"h\", 10)\n    assert result == b\"fg\"\n    assert isinstance(result, bytes)\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_receive_until_incomplete() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    buffered_stream = BufferedByteReceiveStream(receive_stream)\n    await send_stream.send(b\"abcd\")\n    await send_stream.aclose()\n    with pytest.raises(IncompleteRead):\n        assert await buffered_stream.receive_until(b\"de\", 10)\n\n    assert buffered_stream.buffer == b\"abcd\"\n\n    send_stream.close()\n    receive_stream.close()\n", "tests/streams/test_text.py": "from __future__ import annotations\n\nimport platform\nimport sys\n\nimport pytest\n\nfrom anyio import create_memory_object_stream\nfrom anyio.streams.stapled import StapledObjectStream\nfrom anyio.streams.text import TextReceiveStream, TextSendStream, TextStream\n\npytestmark = pytest.mark.anyio\n\n\nasync def test_receive() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    text_stream = TextReceiveStream(receive_stream)\n    await send_stream.send(b\"\\xc3\\xa5\\xc3\\xa4\\xc3\")  # ends with half of the \"\u00f6\" letter\n    assert await text_stream.receive() == \"\u00e5\u00e4\"\n\n    # Send the missing byte for \"\u00f6\"\n    await send_stream.send(b\"\\xb6\")\n    assert await text_stream.receive() == \"\u00f6\"\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_send() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    text_stream = TextSendStream(send_stream)\n    await text_stream.send(\"\u00e5\u00e4\u00f6\")\n    assert await receive_stream.receive() == b\"\\xc3\\xa5\\xc3\\xa4\\xc3\\xb6\"\n\n    send_stream.close()\n    receive_stream.close()\n\n\n@pytest.mark.xfail(\n    platform.python_implementation() == \"PyPy\" and sys.pypy_version_info < (7, 3, 2),  # type: ignore[attr-defined]\n    reason=\"PyPy has a bug in its incremental UTF-8 decoder (#3274)\",\n)\nasync def test_receive_encoding_error() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    text_stream = TextReceiveStream(receive_stream, errors=\"replace\")\n    await send_stream.send(b\"\\xe5\\xe4\\xf6\")  # \"\u00e5\u00e4\u00f6\" in latin-1\n    assert await text_stream.receive() == \"\ufffd\ufffd\ufffd\"\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_send_encoding_error() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    text_stream = TextSendStream(send_stream, encoding=\"iso-8859-1\", errors=\"replace\")\n    await text_stream.send(\"\u20ac\")\n    assert await receive_stream.receive() == b\"?\"\n\n    send_stream.close()\n    receive_stream.close()\n\n\nasync def test_bidirectional_stream() -> None:\n    send_stream, receive_stream = create_memory_object_stream[bytes](1)\n    stapled_stream = StapledObjectStream(send_stream, receive_stream)\n    text_stream = TextStream(stapled_stream)\n\n    await text_stream.send(\"\u00e5\u00e4\u00f6\")\n    assert await receive_stream.receive() == b\"\\xc3\\xa5\\xc3\\xa4\\xc3\\xb6\"\n\n    await send_stream.send(b\"\\xc3\\xa6\\xc3\\xb8\")\n    assert await text_stream.receive() == \"\u00e6\u00f8\"\n    assert text_stream.extra_attributes == {}\n\n    send_stream.close()\n    receive_stream.close()\n", "tests/streams/__init__.py": "", "src/anyio/to_process.py": "from __future__ import annotations\n\nimport os\nimport pickle\nimport subprocess\nimport sys\nfrom collections import deque\nfrom collections.abc import Callable\nfrom importlib.util import module_from_spec, spec_from_file_location\nfrom typing import TypeVar, cast\n\nfrom ._core._eventloop import current_time, get_async_backend, get_cancelled_exc_class\nfrom ._core._exceptions import BrokenWorkerProcess\nfrom ._core._subprocesses import open_process\nfrom ._core._synchronization import CapacityLimiter\nfrom ._core._tasks import CancelScope, fail_after\nfrom .abc import ByteReceiveStream, ByteSendStream, Process\nfrom .lowlevel import RunVar, checkpoint_if_cancelled\nfrom .streams.buffered import BufferedByteReceiveStream\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nWORKER_MAX_IDLE_TIME = 300  # 5 minutes\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n_process_pool_workers: RunVar[set[Process]] = RunVar(\"_process_pool_workers\")\n_process_pool_idle_workers: RunVar[deque[tuple[Process, float]]] = RunVar(\n    \"_process_pool_idle_workers\"\n)\n_default_process_limiter: RunVar[CapacityLimiter] = RunVar(\"_default_process_limiter\")\n\n\nasync def run_sync(\n    func: Callable[[Unpack[PosArgsT]], T_Retval],\n    *args: Unpack[PosArgsT],\n    cancellable: bool = False,\n    limiter: CapacityLimiter | None = None,\n) -> T_Retval:\n    \"\"\"\n    Call the given function with the given arguments in a worker process.\n\n    If the ``cancellable`` option is enabled and the task waiting for its completion is\n    cancelled, the worker process running it will be abruptly terminated using SIGKILL\n    (or ``terminateProcess()`` on Windows).\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :param cancellable: ``True`` to allow cancellation of the operation while it's\n        running\n    :param limiter: capacity limiter to use to limit the total amount of processes\n        running (if omitted, the default limiter is used)\n    :return: an awaitable that yields the return value of the function.\n\n    \"\"\"\n\n    async def send_raw_command(pickled_cmd: bytes) -> object:\n        try:\n            await stdin.send(pickled_cmd)\n            response = await buffered.receive_until(b\"\\n\", 50)\n            status, length = response.split(b\" \")\n            if status not in (b\"RETURN\", b\"EXCEPTION\"):\n                raise RuntimeError(\n                    f\"Worker process returned unexpected response: {response!r}\"\n                )\n\n            pickled_response = await buffered.receive_exactly(int(length))\n        except BaseException as exc:\n            workers.discard(process)\n            try:\n                process.kill()\n                with CancelScope(shield=True):\n                    await process.aclose()\n            except ProcessLookupError:\n                pass\n\n            if isinstance(exc, get_cancelled_exc_class()):\n                raise\n            else:\n                raise BrokenWorkerProcess from exc\n\n        retval = pickle.loads(pickled_response)\n        if status == b\"EXCEPTION\":\n            assert isinstance(retval, BaseException)\n            raise retval\n        else:\n            return retval\n\n    # First pickle the request before trying to reserve a worker process\n    await checkpoint_if_cancelled()\n    request = pickle.dumps((\"run\", func, args), protocol=pickle.HIGHEST_PROTOCOL)\n\n    # If this is the first run in this event loop thread, set up the necessary variables\n    try:\n        workers = _process_pool_workers.get()\n        idle_workers = _process_pool_idle_workers.get()\n    except LookupError:\n        workers = set()\n        idle_workers = deque()\n        _process_pool_workers.set(workers)\n        _process_pool_idle_workers.set(idle_workers)\n        get_async_backend().setup_process_pool_exit_at_shutdown(workers)\n\n    async with limiter or current_default_process_limiter():\n        # Pop processes from the pool (starting from the most recently used) until we\n        # find one that hasn't exited yet\n        process: Process\n        while idle_workers:\n            process, idle_since = idle_workers.pop()\n            if process.returncode is None:\n                stdin = cast(ByteSendStream, process.stdin)\n                buffered = BufferedByteReceiveStream(\n                    cast(ByteReceiveStream, process.stdout)\n                )\n\n                # Prune any other workers that have been idle for WORKER_MAX_IDLE_TIME\n                # seconds or longer\n                now = current_time()\n                killed_processes: list[Process] = []\n                while idle_workers:\n                    if now - idle_workers[0][1] < WORKER_MAX_IDLE_TIME:\n                        break\n\n                    process_to_kill, idle_since = idle_workers.popleft()\n                    process_to_kill.kill()\n                    workers.remove(process_to_kill)\n                    killed_processes.append(process_to_kill)\n\n                with CancelScope(shield=True):\n                    for killed_process in killed_processes:\n                        await killed_process.aclose()\n\n                break\n\n            workers.remove(process)\n        else:\n            command = [sys.executable, \"-u\", \"-m\", __name__]\n            process = await open_process(\n                command, stdin=subprocess.PIPE, stdout=subprocess.PIPE\n            )\n            try:\n                stdin = cast(ByteSendStream, process.stdin)\n                buffered = BufferedByteReceiveStream(\n                    cast(ByteReceiveStream, process.stdout)\n                )\n                with fail_after(20):\n                    message = await buffered.receive(6)\n\n                if message != b\"READY\\n\":\n                    raise BrokenWorkerProcess(\n                        f\"Worker process returned unexpected response: {message!r}\"\n                    )\n\n                main_module_path = getattr(sys.modules[\"__main__\"], \"__file__\", None)\n                pickled = pickle.dumps(\n                    (\"init\", sys.path, main_module_path),\n                    protocol=pickle.HIGHEST_PROTOCOL,\n                )\n                await send_raw_command(pickled)\n            except (BrokenWorkerProcess, get_cancelled_exc_class()):\n                raise\n            except BaseException as exc:\n                process.kill()\n                raise BrokenWorkerProcess(\n                    \"Error during worker process initialization\"\n                ) from exc\n\n            workers.add(process)\n\n        with CancelScope(shield=not cancellable):\n            try:\n                return cast(T_Retval, await send_raw_command(request))\n            finally:\n                if process in workers:\n                    idle_workers.append((process, current_time()))\n\n\ndef current_default_process_limiter() -> CapacityLimiter:\n    \"\"\"\n    Return the capacity limiter that is used by default to limit the number of worker\n    processes.\n\n    :return: a capacity limiter object\n\n    \"\"\"\n    try:\n        return _default_process_limiter.get()\n    except LookupError:\n        limiter = CapacityLimiter(os.cpu_count() or 2)\n        _default_process_limiter.set(limiter)\n        return limiter\n\n\ndef process_worker() -> None:\n    # Redirect standard streams to os.devnull so that user code won't interfere with the\n    # parent-worker communication\n    stdin = sys.stdin\n    stdout = sys.stdout\n    sys.stdin = open(os.devnull)\n    sys.stdout = open(os.devnull, \"w\")\n\n    stdout.buffer.write(b\"READY\\n\")\n    while True:\n        retval = exception = None\n        try:\n            command, *args = pickle.load(stdin.buffer)\n        except EOFError:\n            return\n        except BaseException as exc:\n            exception = exc\n        else:\n            if command == \"run\":\n                func, args = args\n                try:\n                    retval = func(*args)\n                except BaseException as exc:\n                    exception = exc\n            elif command == \"init\":\n                main_module_path: str | None\n                sys.path, main_module_path = args\n                del sys.modules[\"__main__\"]\n                if main_module_path and os.path.isfile(main_module_path):\n                    # Load the parent's main module but as __mp_main__ instead of\n                    # __main__ (like multiprocessing does) to avoid infinite recursion\n                    try:\n                        spec = spec_from_file_location(\"__mp_main__\", main_module_path)\n                        if spec and spec.loader:\n                            main = module_from_spec(spec)\n                            spec.loader.exec_module(main)\n                            sys.modules[\"__main__\"] = main\n                    except BaseException as exc:\n                        exception = exc\n        try:\n            if exception is not None:\n                status = b\"EXCEPTION\"\n                pickled = pickle.dumps(exception, pickle.HIGHEST_PROTOCOL)\n            else:\n                status = b\"RETURN\"\n                pickled = pickle.dumps(retval, pickle.HIGHEST_PROTOCOL)\n        except BaseException as exc:\n            exception = exc\n            status = b\"EXCEPTION\"\n            pickled = pickle.dumps(exc, pickle.HIGHEST_PROTOCOL)\n\n        stdout.buffer.write(b\"%s %d\\n\" % (status, len(pickled)))\n        stdout.buffer.write(pickled)\n\n        # Respect SIGTERM\n        if isinstance(exception, SystemExit):\n            raise exception\n\n\nif __name__ == \"__main__\":\n    process_worker()\n", "src/anyio/pytest_plugin.py": "from __future__ import annotations\n\nfrom collections.abc import Iterator\nfrom contextlib import ExitStack, contextmanager\nfrom inspect import isasyncgenfunction, iscoroutinefunction\nfrom typing import Any, Dict, Tuple, cast\n\nimport pytest\nimport sniffio\n\nfrom ._core._eventloop import get_all_backends, get_async_backend\nfrom .abc import TestRunner\n\n_current_runner: TestRunner | None = None\n_runner_stack: ExitStack | None = None\n_runner_leases = 0\n\n\ndef extract_backend_and_options(backend: object) -> tuple[str, dict[str, Any]]:\n    if isinstance(backend, str):\n        return backend, {}\n    elif isinstance(backend, tuple) and len(backend) == 2:\n        if isinstance(backend[0], str) and isinstance(backend[1], dict):\n            return cast(Tuple[str, Dict[str, Any]], backend)\n\n    raise TypeError(\"anyio_backend must be either a string or tuple of (string, dict)\")\n\n\n@contextmanager\ndef get_runner(\n    backend_name: str, backend_options: dict[str, Any]\n) -> Iterator[TestRunner]:\n    global _current_runner, _runner_leases, _runner_stack\n    if _current_runner is None:\n        asynclib = get_async_backend(backend_name)\n        _runner_stack = ExitStack()\n        if sniffio.current_async_library_cvar.get(None) is None:\n            # Since we're in control of the event loop, we can cache the name of the\n            # async library\n            token = sniffio.current_async_library_cvar.set(backend_name)\n            _runner_stack.callback(sniffio.current_async_library_cvar.reset, token)\n\n        backend_options = backend_options or {}\n        _current_runner = _runner_stack.enter_context(\n            asynclib.create_test_runner(backend_options)\n        )\n\n    _runner_leases += 1\n    try:\n        yield _current_runner\n    finally:\n        _runner_leases -= 1\n        if not _runner_leases:\n            assert _runner_stack is not None\n            _runner_stack.close()\n            _runner_stack = _current_runner = None\n\n\ndef pytest_configure(config: Any) -> None:\n    config.addinivalue_line(\n        \"markers\",\n        \"anyio: mark the (coroutine function) test to be run \"\n        \"asynchronously via anyio.\",\n    )\n\n\ndef pytest_fixture_setup(fixturedef: Any, request: Any) -> None:\n    def wrapper(*args, anyio_backend, **kwargs):  # type: ignore[no-untyped-def]\n        backend_name, backend_options = extract_backend_and_options(anyio_backend)\n        if has_backend_arg:\n            kwargs[\"anyio_backend\"] = anyio_backend\n\n        with get_runner(backend_name, backend_options) as runner:\n            if isasyncgenfunction(func):\n                yield from runner.run_asyncgen_fixture(func, kwargs)\n            else:\n                yield runner.run_fixture(func, kwargs)\n\n    # Only apply this to coroutine functions and async generator functions in requests\n    # that involve the anyio_backend fixture\n    func = fixturedef.func\n    if isasyncgenfunction(func) or iscoroutinefunction(func):\n        if \"anyio_backend\" in request.fixturenames:\n            has_backend_arg = \"anyio_backend\" in fixturedef.argnames\n            fixturedef.func = wrapper\n            if not has_backend_arg:\n                fixturedef.argnames += (\"anyio_backend\",)\n\n\n@pytest.hookimpl(tryfirst=True)\ndef pytest_pycollect_makeitem(collector: Any, name: Any, obj: Any) -> None:\n    if collector.istestfunction(obj, name):\n        inner_func = obj.hypothesis.inner_test if hasattr(obj, \"hypothesis\") else obj\n        if iscoroutinefunction(inner_func):\n            marker = collector.get_closest_marker(\"anyio\")\n            own_markers = getattr(obj, \"pytestmark\", ())\n            if marker or any(marker.name == \"anyio\" for marker in own_markers):\n                pytest.mark.usefixtures(\"anyio_backend\")(obj)\n\n\n@pytest.hookimpl(tryfirst=True)\ndef pytest_pyfunc_call(pyfuncitem: Any) -> bool | None:\n    def run_with_hypothesis(**kwargs: Any) -> None:\n        with get_runner(backend_name, backend_options) as runner:\n            runner.run_test(original_func, kwargs)\n\n    backend = pyfuncitem.funcargs.get(\"anyio_backend\")\n    if backend:\n        backend_name, backend_options = extract_backend_and_options(backend)\n\n        if hasattr(pyfuncitem.obj, \"hypothesis\"):\n            # Wrap the inner test function unless it's already wrapped\n            original_func = pyfuncitem.obj.hypothesis.inner_test\n            if original_func.__qualname__ != run_with_hypothesis.__qualname__:\n                if iscoroutinefunction(original_func):\n                    pyfuncitem.obj.hypothesis.inner_test = run_with_hypothesis\n\n            return None\n\n        if iscoroutinefunction(pyfuncitem.obj):\n            funcargs = pyfuncitem.funcargs\n            testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}\n            with get_runner(backend_name, backend_options) as runner:\n                runner.run_test(pyfuncitem.obj, testargs)\n\n            return True\n\n    return None\n\n\n@pytest.fixture(scope=\"module\", params=get_all_backends())\ndef anyio_backend(request: Any) -> Any:\n    return request.param\n\n\n@pytest.fixture\ndef anyio_backend_name(anyio_backend: Any) -> str:\n    if isinstance(anyio_backend, str):\n        return anyio_backend\n    else:\n        return anyio_backend[0]\n\n\n@pytest.fixture\ndef anyio_backend_options(anyio_backend: Any) -> dict[str, Any]:\n    if isinstance(anyio_backend, str):\n        return {}\n    else:\n        return anyio_backend[1]\n", "src/anyio/from_thread.py": "from __future__ import annotations\n\nimport sys\nimport threading\nfrom collections.abc import Awaitable, Callable, Generator\nfrom concurrent.futures import FIRST_COMPLETED, Future, ThreadPoolExecutor, wait\nfrom contextlib import AbstractContextManager, contextmanager\nfrom dataclasses import dataclass, field\nfrom inspect import isawaitable\nfrom types import TracebackType\nfrom typing import (\n    Any,\n    AsyncContextManager,\n    ContextManager,\n    Generic,\n    Iterable,\n    TypeVar,\n    cast,\n    overload,\n)\n\nfrom ._core import _eventloop\nfrom ._core._eventloop import get_async_backend, get_cancelled_exc_class, threadlocals\nfrom ._core._synchronization import Event\nfrom ._core._tasks import CancelScope, create_task_group\nfrom .abc import AsyncBackend\nfrom .abc._tasks import TaskStatus\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nT_Retval = TypeVar(\"T_Retval\")\nT_co = TypeVar(\"T_co\", covariant=True)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\ndef run(\n    func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]], *args: Unpack[PosArgsT]\n) -> T_Retval:\n    \"\"\"\n    Call a coroutine function from a worker thread.\n\n    :param func: a coroutine function\n    :param args: positional arguments for the callable\n    :return: the return value of the coroutine function\n\n    \"\"\"\n    try:\n        async_backend = threadlocals.current_async_backend\n        token = threadlocals.current_token\n    except AttributeError:\n        raise RuntimeError(\n            \"This function can only be run from an AnyIO worker thread\"\n        ) from None\n\n    return async_backend.run_async_from_thread(func, args, token=token)\n\n\ndef run_sync(\n    func: Callable[[Unpack[PosArgsT]], T_Retval], *args: Unpack[PosArgsT]\n) -> T_Retval:\n    \"\"\"\n    Call a function in the event loop thread from a worker thread.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :return: the return value of the callable\n\n    \"\"\"\n    try:\n        async_backend = threadlocals.current_async_backend\n        token = threadlocals.current_token\n    except AttributeError:\n        raise RuntimeError(\n            \"This function can only be run from an AnyIO worker thread\"\n        ) from None\n\n    return async_backend.run_sync_from_thread(func, args, token=token)\n\n\nclass _BlockingAsyncContextManager(Generic[T_co], AbstractContextManager):\n    _enter_future: Future[T_co]\n    _exit_future: Future[bool | None]\n    _exit_event: Event\n    _exit_exc_info: tuple[\n        type[BaseException] | None, BaseException | None, TracebackType | None\n    ] = (None, None, None)\n\n    def __init__(self, async_cm: AsyncContextManager[T_co], portal: BlockingPortal):\n        self._async_cm = async_cm\n        self._portal = portal\n\n    async def run_async_cm(self) -> bool | None:\n        try:\n            self._exit_event = Event()\n            value = await self._async_cm.__aenter__()\n        except BaseException as exc:\n            self._enter_future.set_exception(exc)\n            raise\n        else:\n            self._enter_future.set_result(value)\n\n        try:\n            # Wait for the sync context manager to exit.\n            # This next statement can raise `get_cancelled_exc_class()` if\n            # something went wrong in a task group in this async context\n            # manager.\n            await self._exit_event.wait()\n        finally:\n            # In case of cancellation, it could be that we end up here before\n            # `_BlockingAsyncContextManager.__exit__` is called, and an\n            # `_exit_exc_info` has been set.\n            result = await self._async_cm.__aexit__(*self._exit_exc_info)\n            return result\n\n    def __enter__(self) -> T_co:\n        self._enter_future = Future()\n        self._exit_future = self._portal.start_task_soon(self.run_async_cm)\n        return self._enter_future.result()\n\n    def __exit__(\n        self,\n        __exc_type: type[BaseException] | None,\n        __exc_value: BaseException | None,\n        __traceback: TracebackType | None,\n    ) -> bool | None:\n        self._exit_exc_info = __exc_type, __exc_value, __traceback\n        self._portal.call(self._exit_event.set)\n        return self._exit_future.result()\n\n\nclass _BlockingPortalTaskStatus(TaskStatus):\n    def __init__(self, future: Future):\n        self._future = future\n\n    def started(self, value: object = None) -> None:\n        self._future.set_result(value)\n\n\nclass BlockingPortal:\n    \"\"\"An object that lets external threads run code in an asynchronous event loop.\"\"\"\n\n    def __new__(cls) -> BlockingPortal:\n        return get_async_backend().create_blocking_portal()\n\n    def __init__(self) -> None:\n        self._event_loop_thread_id: int | None = threading.get_ident()\n        self._stop_event = Event()\n        self._task_group = create_task_group()\n        self._cancelled_exc_class = get_cancelled_exc_class()\n\n    async def __aenter__(self) -> BlockingPortal:\n        await self._task_group.__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        await self.stop()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    def _check_running(self) -> None:\n        if self._event_loop_thread_id is None:\n            raise RuntimeError(\"This portal is not running\")\n        if self._event_loop_thread_id == threading.get_ident():\n            raise RuntimeError(\n                \"This method cannot be called from the event loop thread\"\n            )\n\n    async def sleep_until_stopped(self) -> None:\n        \"\"\"Sleep until :meth:`stop` is called.\"\"\"\n        await self._stop_event.wait()\n\n    async def stop(self, cancel_remaining: bool = False) -> None:\n        \"\"\"\n        Signal the portal to shut down.\n\n        This marks the portal as no longer accepting new calls and exits from\n        :meth:`sleep_until_stopped`.\n\n        :param cancel_remaining: ``True`` to cancel all the remaining tasks, ``False``\n            to let them finish before returning\n\n        \"\"\"\n        self._event_loop_thread_id = None\n        self._stop_event.set()\n        if cancel_remaining:\n            self._task_group.cancel_scope.cancel()\n\n    async def _call_func(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        future: Future[T_Retval],\n    ) -> None:\n        def callback(f: Future[T_Retval]) -> None:\n            if f.cancelled() and self._event_loop_thread_id not in (\n                None,\n                threading.get_ident(),\n            ):\n                self.call(scope.cancel)\n\n        try:\n            retval_or_awaitable = func(*args, **kwargs)\n            if isawaitable(retval_or_awaitable):\n                with CancelScope() as scope:\n                    if future.cancelled():\n                        scope.cancel()\n                    else:\n                        future.add_done_callback(callback)\n\n                    retval = await retval_or_awaitable\n            else:\n                retval = retval_or_awaitable\n        except self._cancelled_exc_class:\n            future.cancel()\n            future.set_running_or_notify_cancel()\n        except BaseException as exc:\n            if not future.cancelled():\n                future.set_exception(exc)\n\n            # Let base exceptions fall through\n            if not isinstance(exc, Exception):\n                raise\n        else:\n            if not future.cancelled():\n                future.set_result(retval)\n        finally:\n            scope = None  # type: ignore[assignment]\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future[T_Retval],\n    ) -> None:\n        \"\"\"\n        Spawn a new task using the given callable.\n\n        Implementors must ensure that the future is resolved when the task finishes.\n\n        :param func: a callable\n        :param args: positional arguments to be passed to the callable\n        :param kwargs: keyword arguments to be passed to the callable\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :param future: a future that will resolve to the return value of the callable,\n            or the exception raised during its execution\n\n        \"\"\"\n        raise NotImplementedError\n\n    @overload\n    def call(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        *args: Unpack[PosArgsT],\n    ) -> T_Retval: ...\n\n    @overload\n    def call(\n        self, func: Callable[[Unpack[PosArgsT]], T_Retval], *args: Unpack[PosArgsT]\n    ) -> T_Retval: ...\n\n    def call(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        *args: Unpack[PosArgsT],\n    ) -> T_Retval:\n        \"\"\"\n        Call the given function in the event loop thread.\n\n        If the callable returns a coroutine object, it is awaited on.\n\n        :param func: any callable\n        :raises RuntimeError: if the portal is not running or if this method is called\n            from within the event loop thread\n\n        \"\"\"\n        return cast(T_Retval, self.start_task_soon(func, *args).result())\n\n    @overload\n    def start_task_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> Future[T_Retval]: ...\n\n    @overload\n    def start_task_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> Future[T_Retval]: ...\n\n    def start_task_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> Future[T_Retval]:\n        \"\"\"\n        Start a task in the portal's task group.\n\n        The task will be run inside a cancel scope which can be cancelled by cancelling\n        the returned future.\n\n        :param func: the target function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a future that resolves with the return value of the callable if the\n            task completes successfully, or with the exception raised in the task\n        :raises RuntimeError: if the portal is not running or if this method is called\n            from within the event loop thread\n        :rtype: concurrent.futures.Future[T_Retval]\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        self._check_running()\n        f: Future[T_Retval] = Future()\n        self._spawn_task_from_thread(func, args, {}, name, f)\n        return f\n\n    def start_task(\n        self,\n        func: Callable[..., Awaitable[T_Retval]],\n        *args: object,\n        name: object = None,\n    ) -> tuple[Future[T_Retval], Any]:\n        \"\"\"\n        Start a task in the portal's task group and wait until it signals for readiness.\n\n        This method works the same way as :meth:`.abc.TaskGroup.start`.\n\n        :param func: the target function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a tuple of (future, task_status_value) where the ``task_status_value``\n            is the value passed to ``task_status.started()`` from within the target\n            function\n        :rtype: tuple[concurrent.futures.Future[T_Retval], Any]\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n\n        def task_done(future: Future[T_Retval]) -> None:\n            if not task_status_future.done():\n                if future.cancelled():\n                    task_status_future.cancel()\n                elif future.exception():\n                    task_status_future.set_exception(future.exception())\n                else:\n                    exc = RuntimeError(\n                        \"Task exited without calling task_status.started()\"\n                    )\n                    task_status_future.set_exception(exc)\n\n        self._check_running()\n        task_status_future: Future = Future()\n        task_status = _BlockingPortalTaskStatus(task_status_future)\n        f: Future = Future()\n        f.add_done_callback(task_done)\n        self._spawn_task_from_thread(func, args, {\"task_status\": task_status}, name, f)\n        return f, task_status_future.result()\n\n    def wrap_async_context_manager(\n        self, cm: AsyncContextManager[T_co]\n    ) -> ContextManager[T_co]:\n        \"\"\"\n        Wrap an async context manager as a synchronous context manager via this portal.\n\n        Spawns a task that will call both ``__aenter__()`` and ``__aexit__()``, stopping\n        in the middle until the synchronous context manager exits.\n\n        :param cm: an asynchronous context manager\n        :return: a synchronous context manager\n\n        .. versionadded:: 2.1\n\n        \"\"\"\n        return _BlockingAsyncContextManager(cm, self)\n\n\n@dataclass\nclass BlockingPortalProvider:\n    \"\"\"\n    A manager for a blocking portal. Used as a context manager. The first thread to\n    enter this context manager causes a blocking portal to be started with the specific\n    parameters, and the last thread to exit causes the portal to be shut down. Thus,\n    there will be exactly one blocking portal running in this context as long as at\n    least one thread has entered this context manager.\n\n    The parameters are the same as for :func:`~anyio.run`.\n\n    :param backend: name of the backend\n    :param backend_options: backend options\n\n    .. versionadded:: 4.4\n    \"\"\"\n\n    backend: str = \"asyncio\"\n    backend_options: dict[str, Any] | None = None\n    _lock: threading.Lock = field(init=False, default_factory=threading.Lock)\n    _leases: int = field(init=False, default=0)\n    _portal: BlockingPortal = field(init=False)\n    _portal_cm: AbstractContextManager[BlockingPortal] | None = field(\n        init=False, default=None\n    )\n\n    def __enter__(self) -> BlockingPortal:\n        with self._lock:\n            if self._portal_cm is None:\n                self._portal_cm = start_blocking_portal(\n                    self.backend, self.backend_options\n                )\n                self._portal = self._portal_cm.__enter__()\n\n            self._leases += 1\n            return self._portal\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        portal_cm: AbstractContextManager[BlockingPortal] | None = None\n        with self._lock:\n            assert self._portal_cm\n            assert self._leases > 0\n            self._leases -= 1\n            if not self._leases:\n                portal_cm = self._portal_cm\n                self._portal_cm = None\n                del self._portal\n\n        if portal_cm:\n            portal_cm.__exit__(None, None, None)\n\n\n@contextmanager\ndef start_blocking_portal(\n    backend: str = \"asyncio\", backend_options: dict[str, Any] | None = None\n) -> Generator[BlockingPortal, Any, None]:\n    \"\"\"\n    Start a new event loop in a new thread and run a blocking portal in its main task.\n\n    The parameters are the same as for :func:`~anyio.run`.\n\n    :param backend: name of the backend\n    :param backend_options: backend options\n    :return: a context manager that yields a blocking portal\n\n    .. versionchanged:: 3.0\n        Usage as a context manager is now required.\n\n    \"\"\"\n\n    async def run_portal() -> None:\n        async with BlockingPortal() as portal_:\n            if future.set_running_or_notify_cancel():\n                future.set_result(portal_)\n                await portal_.sleep_until_stopped()\n\n    future: Future[BlockingPortal] = Future()\n    with ThreadPoolExecutor(1) as executor:\n        run_future = executor.submit(\n            _eventloop.run,  # type: ignore[arg-type]\n            run_portal,\n            backend=backend,\n            backend_options=backend_options,\n        )\n        try:\n            wait(\n                cast(Iterable[Future], [run_future, future]),\n                return_when=FIRST_COMPLETED,\n            )\n        except BaseException:\n            future.cancel()\n            run_future.cancel()\n            raise\n\n        if future.done():\n            portal = future.result()\n            cancel_remaining_tasks = False\n            try:\n                yield portal\n            except BaseException:\n                cancel_remaining_tasks = True\n                raise\n            finally:\n                try:\n                    portal.call(portal.stop, cancel_remaining_tasks)\n                except RuntimeError:\n                    pass\n\n        run_future.result()\n\n\ndef check_cancelled() -> None:\n    \"\"\"\n    Check if the cancel scope of the host task's running the current worker thread has\n    been cancelled.\n\n    If the host task's current cancel scope has indeed been cancelled, the\n    backend-specific cancellation exception will be raised.\n\n    :raises RuntimeError: if the current thread was not spawned by\n        :func:`.to_thread.run_sync`\n\n    \"\"\"\n    try:\n        async_backend: AsyncBackend = threadlocals.current_async_backend\n    except AttributeError:\n        raise RuntimeError(\n            \"This function can only be run from an AnyIO worker thread\"\n        ) from None\n\n    async_backend.check_cancelled()\n", "src/anyio/to_thread.py": "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Callable\nfrom typing import TypeVar\nfrom warnings import warn\n\nfrom ._core._eventloop import get_async_backend\nfrom .abc import CapacityLimiter\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\nasync def run_sync(\n    func: Callable[[Unpack[PosArgsT]], T_Retval],\n    *args: Unpack[PosArgsT],\n    abandon_on_cancel: bool = False,\n    cancellable: bool | None = None,\n    limiter: CapacityLimiter | None = None,\n) -> T_Retval:\n    \"\"\"\n    Call the given function with the given arguments in a worker thread.\n\n    If the ``cancellable`` option is enabled and the task waiting for its completion is\n    cancelled, the thread will still run its course but its return value (or any raised\n    exception) will be ignored.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :param abandon_on_cancel: ``True`` to abandon the thread (leaving it to run\n        unchecked on own) if the host task is cancelled, ``False`` to ignore\n        cancellations in the host task until the operation has completed in the worker\n        thread\n    :param cancellable: deprecated alias of ``abandon_on_cancel``; will override\n        ``abandon_on_cancel`` if both parameters are passed\n    :param limiter: capacity limiter to use to limit the total amount of threads running\n        (if omitted, the default limiter is used)\n    :return: an awaitable that yields the return value of the function.\n\n    \"\"\"\n    if cancellable is not None:\n        abandon_on_cancel = cancellable\n        warn(\n            \"The `cancellable=` keyword argument to `anyio.to_thread.run_sync` is \"\n            \"deprecated since AnyIO 4.1.0; use `abandon_on_cancel=` instead\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    return await get_async_backend().run_sync_in_worker_thread(\n        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n    )\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    \"\"\"\n    Return the capacity limiter that is used by default to limit the number of\n    concurrent threads.\n\n    :return: a capacity limiter object\n\n    \"\"\"\n    return get_async_backend().current_default_thread_limiter()\n", "src/anyio/__init__.py": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom ._core._eventloop import current_time as current_time\nfrom ._core._eventloop import get_all_backends as get_all_backends\nfrom ._core._eventloop import get_cancelled_exc_class as get_cancelled_exc_class\nfrom ._core._eventloop import run as run\nfrom ._core._eventloop import sleep as sleep\nfrom ._core._eventloop import sleep_forever as sleep_forever\nfrom ._core._eventloop import sleep_until as sleep_until\nfrom ._core._exceptions import BrokenResourceError as BrokenResourceError\nfrom ._core._exceptions import BrokenWorkerProcess as BrokenWorkerProcess\nfrom ._core._exceptions import BusyResourceError as BusyResourceError\nfrom ._core._exceptions import ClosedResourceError as ClosedResourceError\nfrom ._core._exceptions import DelimiterNotFound as DelimiterNotFound\nfrom ._core._exceptions import EndOfStream as EndOfStream\nfrom ._core._exceptions import IncompleteRead as IncompleteRead\nfrom ._core._exceptions import TypedAttributeLookupError as TypedAttributeLookupError\nfrom ._core._exceptions import WouldBlock as WouldBlock\nfrom ._core._fileio import AsyncFile as AsyncFile\nfrom ._core._fileio import Path as Path\nfrom ._core._fileio import open_file as open_file\nfrom ._core._fileio import wrap_file as wrap_file\nfrom ._core._resources import aclose_forcefully as aclose_forcefully\nfrom ._core._signals import open_signal_receiver as open_signal_receiver\nfrom ._core._sockets import connect_tcp as connect_tcp\nfrom ._core._sockets import connect_unix as connect_unix\nfrom ._core._sockets import create_connected_udp_socket as create_connected_udp_socket\nfrom ._core._sockets import (\n    create_connected_unix_datagram_socket as create_connected_unix_datagram_socket,\n)\nfrom ._core._sockets import create_tcp_listener as create_tcp_listener\nfrom ._core._sockets import create_udp_socket as create_udp_socket\nfrom ._core._sockets import create_unix_datagram_socket as create_unix_datagram_socket\nfrom ._core._sockets import create_unix_listener as create_unix_listener\nfrom ._core._sockets import getaddrinfo as getaddrinfo\nfrom ._core._sockets import getnameinfo as getnameinfo\nfrom ._core._sockets import wait_socket_readable as wait_socket_readable\nfrom ._core._sockets import wait_socket_writable as wait_socket_writable\nfrom ._core._streams import create_memory_object_stream as create_memory_object_stream\nfrom ._core._subprocesses import open_process as open_process\nfrom ._core._subprocesses import run_process as run_process\nfrom ._core._synchronization import CapacityLimiter as CapacityLimiter\nfrom ._core._synchronization import (\n    CapacityLimiterStatistics as CapacityLimiterStatistics,\n)\nfrom ._core._synchronization import Condition as Condition\nfrom ._core._synchronization import ConditionStatistics as ConditionStatistics\nfrom ._core._synchronization import Event as Event\nfrom ._core._synchronization import EventStatistics as EventStatistics\nfrom ._core._synchronization import Lock as Lock\nfrom ._core._synchronization import LockStatistics as LockStatistics\nfrom ._core._synchronization import ResourceGuard as ResourceGuard\nfrom ._core._synchronization import Semaphore as Semaphore\nfrom ._core._synchronization import SemaphoreStatistics as SemaphoreStatistics\nfrom ._core._tasks import TASK_STATUS_IGNORED as TASK_STATUS_IGNORED\nfrom ._core._tasks import CancelScope as CancelScope\nfrom ._core._tasks import create_task_group as create_task_group\nfrom ._core._tasks import current_effective_deadline as current_effective_deadline\nfrom ._core._tasks import fail_after as fail_after\nfrom ._core._tasks import move_on_after as move_on_after\nfrom ._core._testing import TaskInfo as TaskInfo\nfrom ._core._testing import get_current_task as get_current_task\nfrom ._core._testing import get_running_tasks as get_running_tasks\nfrom ._core._testing import wait_all_tasks_blocked as wait_all_tasks_blocked\nfrom ._core._typedattr import TypedAttributeProvider as TypedAttributeProvider\nfrom ._core._typedattr import TypedAttributeSet as TypedAttributeSet\nfrom ._core._typedattr import typed_attribute as typed_attribute\n\n# Re-export imports so they look like they live directly in this package\nkey: str\nvalue: Any\nfor key, value in list(locals().items()):\n    if getattr(value, \"__module__\", \"\").startswith(\"anyio.\"):\n        value.__module__ = __name__\n", "src/anyio/lowlevel.py": "from __future__ import annotations\n\nimport enum\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Literal, TypeVar, overload\nfrom weakref import WeakKeyDictionary\n\nfrom ._core._eventloop import get_async_backend\n\nT = TypeVar(\"T\")\nD = TypeVar(\"D\")\n\n\nasync def checkpoint() -> None:\n    \"\"\"\n    Check for cancellation and allow the scheduler to switch to another task.\n\n    Equivalent to (but more efficient than)::\n\n        await checkpoint_if_cancelled()\n        await cancel_shielded_checkpoint()\n\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().checkpoint()\n\n\nasync def checkpoint_if_cancelled() -> None:\n    \"\"\"\n    Enter a checkpoint if the enclosing cancel scope has been cancelled.\n\n    This does not allow the scheduler to switch to a different task.\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().checkpoint_if_cancelled()\n\n\nasync def cancel_shielded_checkpoint() -> None:\n    \"\"\"\n    Allow the scheduler to switch to another task but without checking for cancellation.\n\n    Equivalent to (but potentially more efficient than)::\n\n        with CancelScope(shield=True):\n            await checkpoint()\n\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().cancel_shielded_checkpoint()\n\n\ndef current_token() -> object:\n    \"\"\"\n    Return a backend specific token object that can be used to get back to the event\n    loop.\n\n    \"\"\"\n    return get_async_backend().current_token()\n\n\n_run_vars: WeakKeyDictionary[Any, dict[str, Any]] = WeakKeyDictionary()\n_token_wrappers: dict[Any, _TokenWrapper] = {}\n\n\n@dataclass(frozen=True)\nclass _TokenWrapper:\n    __slots__ = \"_token\", \"__weakref__\"\n    _token: object\n\n\nclass _NoValueSet(enum.Enum):\n    NO_VALUE_SET = enum.auto()\n\n\nclass RunvarToken(Generic[T]):\n    __slots__ = \"_var\", \"_value\", \"_redeemed\"\n\n    def __init__(self, var: RunVar[T], value: T | Literal[_NoValueSet.NO_VALUE_SET]):\n        self._var = var\n        self._value: T | Literal[_NoValueSet.NO_VALUE_SET] = value\n        self._redeemed = False\n\n\nclass RunVar(Generic[T]):\n    \"\"\"\n    Like a :class:`~contextvars.ContextVar`, except scoped to the running event loop.\n    \"\"\"\n\n    __slots__ = \"_name\", \"_default\"\n\n    NO_VALUE_SET: Literal[_NoValueSet.NO_VALUE_SET] = _NoValueSet.NO_VALUE_SET\n\n    _token_wrappers: set[_TokenWrapper] = set()\n\n    def __init__(\n        self, name: str, default: T | Literal[_NoValueSet.NO_VALUE_SET] = NO_VALUE_SET\n    ):\n        self._name = name\n        self._default = default\n\n    @property\n    def _current_vars(self) -> dict[str, T]:\n        token = current_token()\n        try:\n            return _run_vars[token]\n        except KeyError:\n            run_vars = _run_vars[token] = {}\n            return run_vars\n\n    @overload\n    def get(self, default: D) -> T | D: ...\n\n    @overload\n    def get(self) -> T: ...\n\n    def get(\n        self, default: D | Literal[_NoValueSet.NO_VALUE_SET] = NO_VALUE_SET\n    ) -> T | D:\n        try:\n            return self._current_vars[self._name]\n        except KeyError:\n            if default is not RunVar.NO_VALUE_SET:\n                return default\n            elif self._default is not RunVar.NO_VALUE_SET:\n                return self._default\n\n        raise LookupError(\n            f'Run variable \"{self._name}\" has no value and no default set'\n        )\n\n    def set(self, value: T) -> RunvarToken[T]:\n        current_vars = self._current_vars\n        token = RunvarToken(self, current_vars.get(self._name, RunVar.NO_VALUE_SET))\n        current_vars[self._name] = value\n        return token\n\n    def reset(self, token: RunvarToken[T]) -> None:\n        if token._var is not self:\n            raise ValueError(\"This token does not belong to this RunVar\")\n\n        if token._redeemed:\n            raise ValueError(\"This token has already been used\")\n\n        if token._value is _NoValueSet.NO_VALUE_SET:\n            try:\n                del self._current_vars[self._name]\n            except KeyError:\n                pass\n        else:\n            self._current_vars[self._name] = token._value\n\n        token._redeemed = True\n\n    def __repr__(self) -> str:\n        return f\"<RunVar name={self._name!r}>\"\n", "src/anyio/_backends/_asyncio.py": "from __future__ import annotations\n\nimport array\nimport asyncio\nimport concurrent.futures\nimport math\nimport socket\nimport sys\nimport threading\nimport weakref\nfrom asyncio import (\n    AbstractEventLoop,\n    CancelledError,\n    all_tasks,\n    create_task,\n    current_task,\n    get_running_loop,\n    sleep,\n)\nfrom asyncio.base_events import _run_until_complete_cb  # type: ignore[attr-defined]\nfrom collections import OrderedDict, deque\nfrom collections.abc import AsyncIterator, Generator, Iterable\nfrom concurrent.futures import Future\nfrom contextlib import suppress\nfrom contextvars import Context, copy_context\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom inspect import (\n    CORO_RUNNING,\n    CORO_SUSPENDED,\n    getcoroutinestate,\n    iscoroutine,\n)\nfrom io import IOBase\nfrom os import PathLike\nfrom queue import Queue\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Collection,\n    ContextManager,\n    Coroutine,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    cast,\n)\nfrom weakref import WeakKeyDictionary\n\nimport sniffio\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._eventloop import claim_worker_thread, threadlocals\nfrom .._core._exceptions import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    WouldBlock,\n)\nfrom .._core._sockets import convert_ipv6_sockaddr\nfrom .._core._streams import create_memory_object_stream\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import (\n    AsyncBackend,\n    IPSockAddrType,\n    SocketListener,\n    UDPPacketType,\n    UNIXDatagramPacketType,\n)\nfrom ..lowlevel import RunVar\nfrom ..streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\nif sys.version_info >= (3, 11):\n    from asyncio import Runner\n    from typing import TypeVarTuple, Unpack\nelse:\n    import contextvars\n    import enum\n    import signal\n    from asyncio import coroutines, events, exceptions, tasks\n\n    from exceptiongroup import BaseExceptionGroup\n    from typing_extensions import TypeVarTuple, Unpack\n\n    class _State(enum.Enum):\n        CREATED = \"created\"\n        INITIALIZED = \"initialized\"\n        CLOSED = \"closed\"\n\n    class Runner:\n        # Copied from CPython 3.11\n        def __init__(\n            self,\n            *,\n            debug: bool | None = None,\n            loop_factory: Callable[[], AbstractEventLoop] | None = None,\n        ):\n            self._state = _State.CREATED\n            self._debug = debug\n            self._loop_factory = loop_factory\n            self._loop: AbstractEventLoop | None = None\n            self._context = None\n            self._interrupt_count = 0\n            self._set_event_loop = False\n\n        def __enter__(self) -> Runner:\n            self._lazy_init()\n            return self\n\n        def __exit__(\n            self,\n            exc_type: type[BaseException],\n            exc_val: BaseException,\n            exc_tb: TracebackType,\n        ) -> None:\n            self.close()\n\n        def close(self) -> None:\n            \"\"\"Shutdown and close event loop.\"\"\"\n            if self._state is not _State.INITIALIZED:\n                return\n            try:\n                loop = self._loop\n                _cancel_all_tasks(loop)\n                loop.run_until_complete(loop.shutdown_asyncgens())\n                if hasattr(loop, \"shutdown_default_executor\"):\n                    loop.run_until_complete(loop.shutdown_default_executor())\n                else:\n                    loop.run_until_complete(_shutdown_default_executor(loop))\n            finally:\n                if self._set_event_loop:\n                    events.set_event_loop(None)\n                loop.close()\n                self._loop = None\n                self._state = _State.CLOSED\n\n        def get_loop(self) -> AbstractEventLoop:\n            \"\"\"Return embedded event loop.\"\"\"\n            self._lazy_init()\n            return self._loop\n\n        def run(self, coro: Coroutine[T_Retval], *, context=None) -> T_Retval:\n            \"\"\"Run a coroutine inside the embedded event loop.\"\"\"\n            if not coroutines.iscoroutine(coro):\n                raise ValueError(f\"a coroutine was expected, got {coro!r}\")\n\n            if events._get_running_loop() is not None:\n                # fail fast with short traceback\n                raise RuntimeError(\n                    \"Runner.run() cannot be called from a running event loop\"\n                )\n\n            self._lazy_init()\n\n            if context is None:\n                context = self._context\n            task = context.run(self._loop.create_task, coro)\n\n            if (\n                threading.current_thread() is threading.main_thread()\n                and signal.getsignal(signal.SIGINT) is signal.default_int_handler\n            ):\n                sigint_handler = partial(self._on_sigint, main_task=task)\n                try:\n                    signal.signal(signal.SIGINT, sigint_handler)\n                except ValueError:\n                    # `signal.signal` may throw if `threading.main_thread` does\n                    # not support signals (e.g. embedded interpreter with signals\n                    # not registered - see gh-91880)\n                    sigint_handler = None\n            else:\n                sigint_handler = None\n\n            self._interrupt_count = 0\n            try:\n                return self._loop.run_until_complete(task)\n            except exceptions.CancelledError:\n                if self._interrupt_count > 0:\n                    uncancel = getattr(task, \"uncancel\", None)\n                    if uncancel is not None and uncancel() == 0:\n                        raise KeyboardInterrupt()\n                raise  # CancelledError\n            finally:\n                if (\n                    sigint_handler is not None\n                    and signal.getsignal(signal.SIGINT) is sigint_handler\n                ):\n                    signal.signal(signal.SIGINT, signal.default_int_handler)\n\n        def _lazy_init(self) -> None:\n            if self._state is _State.CLOSED:\n                raise RuntimeError(\"Runner is closed\")\n            if self._state is _State.INITIALIZED:\n                return\n            if self._loop_factory is None:\n                self._loop = events.new_event_loop()\n                if not self._set_event_loop:\n                    # Call set_event_loop only once to avoid calling\n                    # attach_loop multiple times on child watchers\n                    events.set_event_loop(self._loop)\n                    self._set_event_loop = True\n            else:\n                self._loop = self._loop_factory()\n            if self._debug is not None:\n                self._loop.set_debug(self._debug)\n            self._context = contextvars.copy_context()\n            self._state = _State.INITIALIZED\n\n        def _on_sigint(self, signum, frame, main_task: asyncio.Task) -> None:\n            self._interrupt_count += 1\n            if self._interrupt_count == 1 and not main_task.done():\n                main_task.cancel()\n                # wakeup loop if it is blocked by select() with long timeout\n                self._loop.call_soon_threadsafe(lambda: None)\n                return\n            raise KeyboardInterrupt()\n\n    def _cancel_all_tasks(loop: AbstractEventLoop) -> None:\n        to_cancel = tasks.all_tasks(loop)\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler(\n                    {\n                        \"message\": \"unhandled exception during asyncio.run() shutdown\",\n                        \"exception\": task.exception(),\n                        \"task\": task,\n                    }\n                )\n\n    async def _shutdown_default_executor(loop: AbstractEventLoop) -> None:\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n\n        def _do_shutdown(future: asyncio.futures.Future) -> None:\n            try:\n                loop._default_executor.shutdown(wait=True)  # type: ignore[attr-defined]\n                loop.call_soon_threadsafe(future.set_result, None)\n            except Exception as ex:\n                loop.call_soon_threadsafe(future.set_exception, ex)\n\n        loop._executor_shutdown_called = True\n        if loop._default_executor is None:\n            return\n        future = loop.create_future()\n        thread = threading.Thread(target=_do_shutdown, args=(future,))\n        thread.start()\n        try:\n            await future\n        finally:\n            thread.join()\n\n\nT_Retval = TypeVar(\"T_Retval\")\nT_contra = TypeVar(\"T_contra\", contravariant=True)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\nP = ParamSpec(\"P\")\n\n_root_task: RunVar[asyncio.Task | None] = RunVar(\"_root_task\")\n\n\ndef find_root_task() -> asyncio.Task:\n    root_task = _root_task.get(None)\n    if root_task is not None and not root_task.done():\n        return root_task\n\n    # Look for a task that has been started via run_until_complete()\n    for task in all_tasks():\n        if task._callbacks and not task.done():\n            callbacks = [cb for cb, context in task._callbacks]\n            for cb in callbacks:\n                if (\n                    cb is _run_until_complete_cb\n                    or getattr(cb, \"__module__\", None) == \"uvloop.loop\"\n                ):\n                    _root_task.set(task)\n                    return task\n\n    # Look up the topmost task in the AnyIO task tree, if possible\n    task = cast(asyncio.Task, current_task())\n    state = _task_states.get(task)\n    if state:\n        cancel_scope = state.cancel_scope\n        while cancel_scope and cancel_scope._parent_scope is not None:\n            cancel_scope = cancel_scope._parent_scope\n\n        if cancel_scope is not None:\n            return cast(asyncio.Task, cancel_scope._host_task)\n\n    return task\n\n\ndef get_callable_name(func: Callable) -> str:\n    module = getattr(func, \"__module__\", None)\n    qualname = getattr(func, \"__qualname__\", None)\n    return \".\".join([x for x in (module, qualname) if x])\n\n\n#\n# Event loop\n#\n\n_run_vars: WeakKeyDictionary[asyncio.AbstractEventLoop, Any] = WeakKeyDictionary()\n\n\ndef _task_started(task: asyncio.Task) -> bool:\n    \"\"\"Return ``True`` if the task has been started and has not finished.\"\"\"\n    try:\n        return getcoroutinestate(task.get_coro()) in (CORO_RUNNING, CORO_SUSPENDED)\n    except AttributeError:\n        # task coro is async_genenerator_asend https://bugs.python.org/issue37771\n        raise Exception(f\"Cannot determine if task {task} has started or not\") from None\n\n\n#\n# Timeouts and cancellation\n#\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return object.__new__(cls)\n\n    def __init__(self, deadline: float = math.inf, shield: bool = False):\n        self._deadline = deadline\n        self._shield = shield\n        self._parent_scope: CancelScope | None = None\n        self._child_scopes: set[CancelScope] = set()\n        self._cancel_called = False\n        self._cancelled_caught = False\n        self._active = False\n        self._timeout_handle: asyncio.TimerHandle | None = None\n        self._cancel_handle: asyncio.Handle | None = None\n        self._tasks: set[asyncio.Task] = set()\n        self._host_task: asyncio.Task | None = None\n        self._cancel_calls: int = 0\n        self._cancelling: int | None = None\n\n    def __enter__(self) -> CancelScope:\n        if self._active:\n            raise RuntimeError(\n                \"Each CancelScope may only be used for a single 'with' block\"\n            )\n\n        self._host_task = host_task = cast(asyncio.Task, current_task())\n        self._tasks.add(host_task)\n        try:\n            task_state = _task_states[host_task]\n        except KeyError:\n            task_state = TaskState(None, self)\n            _task_states[host_task] = task_state\n        else:\n            self._parent_scope = task_state.cancel_scope\n            task_state.cancel_scope = self\n            if self._parent_scope is not None:\n                self._parent_scope._child_scopes.add(self)\n                self._parent_scope._tasks.remove(host_task)\n\n        self._timeout()\n        self._active = True\n        if sys.version_info >= (3, 11):\n            self._cancelling = self._host_task.cancelling()\n\n        # Start cancelling the host task if the scope was cancelled before entering\n        if self._cancel_called:\n            self._deliver_cancellation(self)\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        if not self._active:\n            raise RuntimeError(\"This cancel scope is not active\")\n        if current_task() is not self._host_task:\n            raise RuntimeError(\n                \"Attempted to exit cancel scope in a different task than it was \"\n                \"entered in\"\n            )\n\n        assert self._host_task is not None\n        host_task_state = _task_states.get(self._host_task)\n        if host_task_state is None or host_task_state.cancel_scope is not self:\n            raise RuntimeError(\n                \"Attempted to exit a cancel scope that isn't the current tasks's \"\n                \"current cancel scope\"\n            )\n\n        self._active = False\n        if self._timeout_handle:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        self._tasks.remove(self._host_task)\n        if self._parent_scope is not None:\n            self._parent_scope._child_scopes.remove(self)\n            self._parent_scope._tasks.add(self._host_task)\n\n        host_task_state.cancel_scope = self._parent_scope\n\n        # Restart the cancellation effort in the closest directly cancelled parent\n        # scope if this one was shielded\n        self._restart_cancellation_in_parent()\n\n        if self._cancel_called and exc_val is not None:\n            for exc in iterate_exceptions(exc_val):\n                if isinstance(exc, CancelledError):\n                    self._cancelled_caught = self._uncancel(exc)\n                    if self._cancelled_caught:\n                        break\n\n            return self._cancelled_caught\n\n        return None\n\n    def _uncancel(self, cancelled_exc: CancelledError) -> bool:\n        if sys.version_info < (3, 9) or self._host_task is None:\n            self._cancel_calls = 0\n            return True\n\n        # Undo all cancellations done by this scope\n        if self._cancelling is not None:\n            while self._cancel_calls:\n                self._cancel_calls -= 1\n                if self._host_task.uncancel() <= self._cancelling:\n                    return True\n\n        self._cancel_calls = 0\n        return f\"Cancelled by cancel scope {id(self):x}\" in cancelled_exc.args\n\n    def _timeout(self) -> None:\n        if self._deadline != math.inf:\n            loop = get_running_loop()\n            if loop.time() >= self._deadline:\n                self.cancel()\n            else:\n                self._timeout_handle = loop.call_at(self._deadline, self._timeout)\n\n    def _deliver_cancellation(self, origin: CancelScope) -> bool:\n        \"\"\"\n        Deliver cancellation to directly contained tasks and nested cancel scopes.\n\n        Schedule another run at the end if we still have tasks eligible for\n        cancellation.\n\n        :param origin: the cancel scope that originated the cancellation\n        :return: ``True`` if the delivery needs to be retried on the next cycle\n\n        \"\"\"\n        should_retry = False\n        current = current_task()\n        for task in self._tasks:\n            if task._must_cancel:  # type: ignore[attr-defined]\n                continue\n\n            # The task is eligible for cancellation if it has started\n            should_retry = True\n            if task is not current and (task is self._host_task or _task_started(task)):\n                waiter = task._fut_waiter  # type: ignore[attr-defined]\n                if not isinstance(waiter, asyncio.Future) or not waiter.done():\n                    origin._cancel_calls += 1\n                    if sys.version_info >= (3, 9):\n                        task.cancel(f\"Cancelled by cancel scope {id(origin):x}\")\n                    else:\n                        task.cancel()\n\n        # Deliver cancellation to child scopes that aren't shielded or running their own\n        # cancellation callbacks\n        for scope in self._child_scopes:\n            if not scope._shield and not scope.cancel_called:\n                should_retry = scope._deliver_cancellation(origin) or should_retry\n\n        # Schedule another callback if there are still tasks left\n        if origin is self:\n            if should_retry:\n                self._cancel_handle = get_running_loop().call_soon(\n                    self._deliver_cancellation, origin\n                )\n            else:\n                self._cancel_handle = None\n\n        return should_retry\n\n    def _restart_cancellation_in_parent(self) -> None:\n        \"\"\"\n        Restart the cancellation effort in the closest directly cancelled parent scope.\n\n        \"\"\"\n        scope = self._parent_scope\n        while scope is not None:\n            if scope._cancel_called:\n                if scope._cancel_handle is None:\n                    scope._deliver_cancellation(scope)\n\n                break\n\n            # No point in looking beyond any shielded scope\n            if scope._shield:\n                break\n\n            scope = scope._parent_scope\n\n    def _parent_cancelled(self) -> bool:\n        # Check whether any parent has been cancelled\n        cancel_scope = self._parent_scope\n        while cancel_scope is not None and not cancel_scope._shield:\n            if cancel_scope._cancel_called:\n                return True\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return False\n\n    def cancel(self) -> None:\n        if not self._cancel_called:\n            if self._timeout_handle:\n                self._timeout_handle.cancel()\n                self._timeout_handle = None\n\n            self._cancel_called = True\n            if self._host_task is not None:\n                self._deliver_cancellation(self)\n\n    @property\n    def deadline(self) -> float:\n        return self._deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self._deadline = float(value)\n        if self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        if self._active and not self._cancel_called:\n            self._timeout()\n\n    @property\n    def cancel_called(self) -> bool:\n        return self._cancel_called\n\n    @property\n    def cancelled_caught(self) -> bool:\n        return self._cancelled_caught\n\n    @property\n    def shield(self) -> bool:\n        return self._shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        if self._shield != value:\n            self._shield = value\n            if not value:\n                self._restart_cancellation_in_parent()\n\n\n#\n# Task states\n#\n\n\nclass TaskState:\n    \"\"\"\n    Encapsulates auxiliary task information that cannot be added to the Task instance\n    itself because there are no guarantees about its implementation.\n    \"\"\"\n\n    __slots__ = \"parent_id\", \"cancel_scope\", \"__weakref__\"\n\n    def __init__(self, parent_id: int | None, cancel_scope: CancelScope | None):\n        self.parent_id = parent_id\n        self.cancel_scope = cancel_scope\n\n\n_task_states: WeakKeyDictionary[asyncio.Task, TaskState] = WeakKeyDictionary()\n\n\n#\n# Task groups\n#\n\n\nclass _AsyncioTaskStatus(abc.TaskStatus):\n    def __init__(self, future: asyncio.Future, parent_id: int):\n        self._future = future\n        self._parent_id = parent_id\n\n    def started(self, value: T_contra | None = None) -> None:\n        try:\n            self._future.set_result(value)\n        except asyncio.InvalidStateError:\n            if not self._future.cancelled():\n                raise RuntimeError(\n                    \"called 'started' twice on the same task status\"\n                ) from None\n\n        task = cast(asyncio.Task, current_task())\n        _task_states[task].parent_id = self._parent_id\n\n\ndef iterate_exceptions(\n    exception: BaseException,\n) -> Generator[BaseException, None, None]:\n    if isinstance(exception, BaseExceptionGroup):\n        for exc in exception.exceptions:\n            yield from iterate_exceptions(exc)\n    else:\n        yield exception\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self.cancel_scope: CancelScope = CancelScope()\n        self._active = False\n        self._exceptions: list[BaseException] = []\n        self._tasks: set[asyncio.Task] = set()\n\n    async def __aenter__(self) -> TaskGroup:\n        self.cancel_scope.__enter__()\n        self._active = True\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        ignore_exception = self.cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n        if exc_val is not None:\n            self.cancel_scope.cancel()\n            if not isinstance(exc_val, CancelledError):\n                self._exceptions.append(exc_val)\n\n        cancelled_exc_while_waiting_tasks: CancelledError | None = None\n        while self._tasks:\n            try:\n                await asyncio.wait(self._tasks)\n            except CancelledError as exc:\n                # This task was cancelled natively; reraise the CancelledError later\n                # unless this task was already interrupted by another exception\n                self.cancel_scope.cancel()\n                if cancelled_exc_while_waiting_tasks is None:\n                    cancelled_exc_while_waiting_tasks = exc\n\n        self._active = False\n        if self._exceptions:\n            raise BaseExceptionGroup(\n                \"unhandled errors in a TaskGroup\", self._exceptions\n            )\n\n        # Raise the CancelledError received while waiting for child tasks to exit,\n        # unless the context manager itself was previously exited with another\n        # exception, or if any of the  child tasks raised an exception other than\n        # CancelledError\n        if cancelled_exc_while_waiting_tasks:\n            if exc_val is None or ignore_exception:\n                raise cancelled_exc_while_waiting_tasks\n\n        return ignore_exception\n\n    def _spawn(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        args: tuple[Unpack[PosArgsT]],\n        name: object,\n        task_status_future: asyncio.Future | None = None,\n    ) -> asyncio.Task:\n        def task_done(_task: asyncio.Task) -> None:\n            task_state = _task_states[_task]\n            assert task_state.cancel_scope is not None\n            assert _task in task_state.cancel_scope._tasks\n            task_state.cancel_scope._tasks.remove(_task)\n            self._tasks.remove(task)\n            del _task_states[_task]\n\n            try:\n                exc = _task.exception()\n            except CancelledError as e:\n                while isinstance(e.__context__, CancelledError):\n                    e = e.__context__\n\n                exc = e\n\n            if exc is not None:\n                # The future can only be in the cancelled state if the host task was\n                # cancelled, so return immediately instead of adding one more\n                # CancelledError to the exceptions list\n                if task_status_future is not None and task_status_future.cancelled():\n                    return\n\n                if task_status_future is None or task_status_future.done():\n                    if not isinstance(exc, CancelledError):\n                        self._exceptions.append(exc)\n\n                    if not self.cancel_scope._parent_cancelled():\n                        self.cancel_scope.cancel()\n                else:\n                    task_status_future.set_exception(exc)\n            elif task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError(\"Child exited without calling task_status.started()\")\n                )\n\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        kwargs = {}\n        if task_status_future:\n            parent_id = id(current_task())\n            kwargs[\"task_status\"] = _AsyncioTaskStatus(\n                task_status_future, id(self.cancel_scope._host_task)\n            )\n        else:\n            parent_id = id(self.cancel_scope._host_task)\n\n        coro = func(*args, **kwargs)\n        if not iscoroutine(coro):\n            prefix = f\"{func.__module__}.\" if hasattr(func, \"__module__\") else \"\"\n            raise TypeError(\n                f\"Expected {prefix}{func.__qualname__}() to return a coroutine, but \"\n                f\"the return value ({coro!r}) is not a coroutine object\"\n            )\n\n        name = get_callable_name(func) if name is None else str(name)\n        task = create_task(coro, name=name)\n        task.add_done_callback(task_done)\n\n        # Make the spawned task inherit the task group's cancel scope\n        _task_states[task] = TaskState(\n            parent_id=parent_id, cancel_scope=self.cancel_scope\n        )\n        self.cancel_scope._tasks.add(task)\n        self._tasks.add(task)\n        return task\n\n    def start_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> None:\n        self._spawn(func, args, name)\n\n    async def start(\n        self, func: Callable[..., Awaitable[Any]], *args: object, name: object = None\n    ) -> Any:\n        future: asyncio.Future = asyncio.Future()\n        task = self._spawn(func, args, name, future)\n\n        # If the task raises an exception after sending a start value without a switch\n        # point between, the task group is cancelled and this method never proceeds to\n        # process the completed future. That's why we have to have a shielded cancel\n        # scope here.\n        try:\n            return await future\n        except CancelledError:\n            # Cancel the task and wait for it to exit before returning\n            task.cancel()\n            with CancelScope(shield=True), suppress(CancelledError):\n                await task\n\n            raise\n\n\n#\n# Threads\n#\n\n_Retval_Queue_Type = Tuple[Optional[T_Retval], Optional[BaseException]]\n\n\nclass WorkerThread(Thread):\n    MAX_IDLE_TIME = 10  # seconds\n\n    def __init__(\n        self,\n        root_task: asyncio.Task,\n        workers: set[WorkerThread],\n        idle_workers: deque[WorkerThread],\n    ):\n        super().__init__(name=\"AnyIO worker thread\")\n        self.root_task = root_task\n        self.workers = workers\n        self.idle_workers = idle_workers\n        self.loop = root_task._loop\n        self.queue: Queue[\n            tuple[Context, Callable, tuple, asyncio.Future, CancelScope] | None\n        ] = Queue(2)\n        self.idle_since = AsyncIOBackend.current_time()\n        self.stopping = False\n\n    def _report_result(\n        self, future: asyncio.Future, result: Any, exc: BaseException | None\n    ) -> None:\n        self.idle_since = AsyncIOBackend.current_time()\n        if not self.stopping:\n            self.idle_workers.append(self)\n\n        if not future.cancelled():\n            if exc is not None:\n                if isinstance(exc, StopIteration):\n                    new_exc = RuntimeError(\"coroutine raised StopIteration\")\n                    new_exc.__cause__ = exc\n                    exc = new_exc\n\n                future.set_exception(exc)\n            else:\n                future.set_result(result)\n\n    def run(self) -> None:\n        with claim_worker_thread(AsyncIOBackend, self.loop):\n            while True:\n                item = self.queue.get()\n                if item is None:\n                    # Shutdown command received\n                    return\n\n                context, func, args, future, cancel_scope = item\n                if not future.cancelled():\n                    result = None\n                    exception: BaseException | None = None\n                    threadlocals.current_cancel_scope = cancel_scope\n                    try:\n                        result = context.run(func, *args)\n                    except BaseException as exc:\n                        exception = exc\n                    finally:\n                        del threadlocals.current_cancel_scope\n\n                    if not self.loop.is_closed():\n                        self.loop.call_soon_threadsafe(\n                            self._report_result, future, result, exception\n                        )\n\n                self.queue.task_done()\n\n    def stop(self, f: asyncio.Task | None = None) -> None:\n        self.stopping = True\n        self.queue.put_nowait(None)\n        self.workers.discard(self)\n        try:\n            self.idle_workers.remove(self)\n        except ValueError:\n            pass\n\n\n_threadpool_idle_workers: RunVar[deque[WorkerThread]] = RunVar(\n    \"_threadpool_idle_workers\"\n)\n_threadpool_workers: RunVar[set[WorkerThread]] = RunVar(\"_threadpool_workers\")\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> BlockingPortal:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._loop = get_running_loop()\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future[T_Retval],\n    ) -> None:\n        AsyncIOBackend.run_sync_from_thread(\n            partial(self._task_group.start_soon, name=name),\n            (self._call_func, func, args, kwargs, future),\n            self._loop,\n        )\n\n\n#\n# Subprocesses\n#\n\n\n@dataclass(eq=False)\nclass StreamReaderWrapper(abc.ByteReceiveStream):\n    _stream: asyncio.StreamReader\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._stream.read(max_bytes)\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        self._stream.feed_eof()\n        await AsyncIOBackend.checkpoint()\n\n\n@dataclass(eq=False)\nclass StreamWriterWrapper(abc.ByteSendStream):\n    _stream: asyncio.StreamWriter\n\n    async def send(self, item: bytes) -> None:\n        self._stream.write(item)\n        await self._stream.drain()\n\n    async def aclose(self) -> None:\n        self._stream.close()\n        await AsyncIOBackend.checkpoint()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: asyncio.subprocess.Process\n    _stdin: StreamWriterWrapper | None\n    _stdout: StreamReaderWrapper | None\n    _stderr: StreamReaderWrapper | None\n\n    async def aclose(self) -> None:\n        with CancelScope(shield=True):\n            if self._stdin:\n                await self._stdin.aclose()\n            if self._stdout:\n                await self._stdout.aclose()\n            if self._stderr:\n                await self._stderr.aclose()\n\n        try:\n            await self.wait()\n        except BaseException:\n            self.kill()\n            with CancelScope(shield=True):\n                await self.wait()\n\n            raise\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: int) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> int | None:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> abc.ByteSendStream | None:\n        return self._stdin\n\n    @property\n    def stdout(self) -> abc.ByteReceiveStream | None:\n        return self._stdout\n\n    @property\n    def stderr(self) -> abc.ByteReceiveStream | None:\n        return self._stderr\n\n\ndef _forcibly_shutdown_process_pool_on_exit(\n    workers: set[Process], _task: object\n) -> None:\n    \"\"\"\n    Forcibly shuts down worker processes belonging to this event loop.\"\"\"\n    child_watcher: asyncio.AbstractChildWatcher | None = None\n    if sys.version_info < (3, 12):\n        try:\n            child_watcher = asyncio.get_event_loop_policy().get_child_watcher()\n        except NotImplementedError:\n            pass\n\n    # Close as much as possible (w/o async/await) to avoid warnings\n    for process in workers:\n        if process.returncode is None:\n            continue\n\n        process._stdin._stream._transport.close()  # type: ignore[union-attr]\n        process._stdout._stream._transport.close()  # type: ignore[union-attr]\n        process._stderr._stream._transport.close()  # type: ignore[union-attr]\n        process.kill()\n        if child_watcher:\n            child_watcher.remove_child_handler(process.pid)\n\n\nasync def _shutdown_process_pool_on_exit(workers: set[abc.Process]) -> None:\n    \"\"\"\n    Shuts down worker processes belonging to this event loop.\n\n    NOTE: this only works when the event loop was started using asyncio.run() or\n    anyio.run().\n\n    \"\"\"\n    process: abc.Process\n    try:\n        await sleep(math.inf)\n    except asyncio.CancelledError:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        for process in workers:\n            await process.aclose()\n\n\n#\n# Sockets and networking\n#\n\n\nclass StreamProtocol(asyncio.Protocol):\n    read_queue: deque[bytes]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Exception | None = None\n    is_at_eof: bool = False\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque()\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n        cast(asyncio.Transport, transport).set_write_buffer_limits(0)\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        if exc:\n            self.exception = BrokenResourceError()\n            self.exception.__cause__ = exc\n\n        self.read_event.set()\n        self.write_event.set()\n\n    def data_received(self, data: bytes) -> None:\n        self.read_queue.append(data)\n        self.read_event.set()\n\n    def eof_received(self) -> bool | None:\n        self.is_at_eof = True\n        self.read_event.set()\n        return True\n\n    def pause_writing(self) -> None:\n        self.write_event = asyncio.Event()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass DatagramProtocol(asyncio.DatagramProtocol):\n    read_queue: deque[tuple[bytes, IPSockAddrType]]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Exception | None = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque(maxlen=100)  # arbitrary value\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        self.read_event.set()\n        self.write_event.set()\n\n    def datagram_received(self, data: bytes, addr: IPSockAddrType) -> None:\n        addr = convert_ipv6_sockaddr(addr)\n        self.read_queue.append((data, addr))\n        self.read_event.set()\n\n    def error_received(self, exc: Exception) -> None:\n        self.exception = exc\n\n    def pause_writing(self) -> None:\n        self.write_event.clear()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass SocketStream(abc.SocketStream):\n    def __init__(self, transport: asyncio.Transport, protocol: StreamProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            if (\n                not self._protocol.read_event.is_set()\n                and not self._transport.is_closing()\n                and not self._protocol.is_at_eof\n            ):\n                self._transport.resume_reading()\n                await self._protocol.read_event.wait()\n                self._transport.pause_reading()\n            else:\n                await AsyncIOBackend.checkpoint()\n\n            try:\n                chunk = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                elif self._protocol.exception:\n                    raise self._protocol.exception from None\n                else:\n                    raise EndOfStream from None\n\n            if len(chunk) > max_bytes:\n                # Split the oversized chunk\n                chunk, leftover = chunk[:max_bytes], chunk[max_bytes:]\n                self._protocol.read_queue.appendleft(leftover)\n\n            # If the read queue is empty, clear the flag so that the next call will\n            # block until data is available\n            if not self._protocol.read_queue:\n                self._protocol.read_event.clear()\n\n        return chunk\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n\n            if self._closed:\n                raise ClosedResourceError\n            elif self._protocol.exception is not None:\n                raise self._protocol.exception\n\n            try:\n                self._transport.write(item)\n            except RuntimeError as exc:\n                if self._transport.is_closing():\n                    raise BrokenResourceError from exc\n                else:\n                    raise\n\n            await self._protocol.write_event.wait()\n\n    async def send_eof(self) -> None:\n        try:\n            self._transport.write_eof()\n        except OSError:\n            pass\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            try:\n                self._transport.write_eof()\n            except OSError:\n                pass\n\n            self._transport.close()\n            await sleep(0)\n            self._transport.abort()\n\n\nclass _RawSocketMixin:\n    _receive_future: asyncio.Future | None = None\n    _send_future: asyncio.Future | None = None\n    _closing = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    def _wait_until_readable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._receive_future\n            loop.remove_reader(self.__raw_socket)\n\n        f = self._receive_future = asyncio.Future()\n        loop.add_reader(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    def _wait_until_writable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._send_future\n            loop.remove_writer(self.__raw_socket)\n\n        f = self._send_future = asyncio.Future()\n        loop.add_writer(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    async def aclose(self) -> None:\n        if not self._closing:\n            self._closing = True\n            if self.__raw_socket.fileno() != -1:\n                self.__raw_socket.close()\n\n            if self._receive_future:\n                self._receive_future.set_result(None)\n            if self._send_future:\n                self._send_future.set_result(None)\n\n\nclass UNIXSocketStream(_RawSocketMixin, abc.UNIXSocketStream):\n    async def send_eof(self) -> None:\n        with self._send_guard:\n            self._raw_socket.shutdown(socket.SHUT_WR)\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recv(max_bytes)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not data:\n                        raise EndOfStream\n\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = self._raw_socket.send(view)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    view = view[bytes_sent:]\n\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError(\"msglen must be a non-negative integer\")\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError(\"maxfds must be a positive integer\")\n\n        loop = get_running_loop()\n        fds = array.array(\"i\")\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = self._raw_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize)\n                    )\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(\n                    f\"Received unexpected ancillary data; message = {message!r}, \"\n                    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n                )\n\n            fds.frombytes(cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        if not message:\n            raise ValueError(\"message must not be empty\")\n        if not fds:\n            raise ValueError(\"fds must not be empty\")\n\n        loop = get_running_loop()\n        filenos: list[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    # The ignore can be removed after mypy picks up\n                    # https://github.com/python/typeshed/pull/5545\n                    self._raw_socket.sendmsg(\n                        [message], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fdarray)]\n                    )\n                    break\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n\nclass TCPSocketListener(abc.SocketListener):\n    _accept_scope: CancelScope | None = None\n    _closed = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = cast(asyncio.BaseEventLoop, get_running_loop())\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    async def accept(self) -> abc.SocketStream:\n        if self._closed:\n            raise ClosedResourceError\n\n        with self._accept_guard:\n            await AsyncIOBackend.checkpoint()\n            with CancelScope() as self._accept_scope:\n                try:\n                    client_sock, _addr = await self._loop.sock_accept(self._raw_socket)\n                except asyncio.CancelledError:\n                    # Workaround for https://bugs.python.org/issue41317\n                    try:\n                        self._loop.remove_reader(self._raw_socket)\n                    except (ValueError, NotImplementedError):\n                        pass\n\n                    if self._closed:\n                        raise ClosedResourceError from None\n\n                    raise\n                finally:\n                    self._accept_scope = None\n\n        client_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        transport, protocol = await self._loop.connect_accepted_socket(\n            StreamProtocol, client_sock\n        )\n        return SocketStream(transport, protocol)\n\n    async def aclose(self) -> None:\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._accept_scope:\n            # Workaround for https://bugs.python.org/issue41317\n            try:\n                self._loop.remove_reader(self._raw_socket)\n            except (ValueError, NotImplementedError):\n                pass\n\n            self._accept_scope.cancel()\n            await sleep(0)\n\n        self._raw_socket.close()\n\n\nclass UNIXSocketListener(abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n        self._closed = False\n\n    async def accept(self) -> abc.SocketStream:\n        await AsyncIOBackend.checkpoint()\n        with self._accept_guard:\n            while True:\n                try:\n                    client_sock, _ = self.__raw_socket.accept()\n                    client_sock.setblocking(False)\n                    return UNIXSocketStream(client_sock)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    self._loop.add_reader(self.__raw_socket, f.set_result, None)\n                    f.add_done_callback(\n                        lambda _: self._loop.remove_reader(self.__raw_socket)\n                    )\n                    await f\n                except OSError as exc:\n                    if self._closed:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        self._closed = True\n        self.__raw_socket.close()\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n\nclass UDPSocket(abc.UDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                return self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(*item)\n\n\nclass ConnectedUDPSocket(abc.ConnectedUDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                packet = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n            return packet[0]\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(item)\n\n\nclass UNIXDatagramSocket(_RawSocketMixin, abc.UNIXDatagramSocket):\n    async def receive(self) -> UNIXDatagramPacketType:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recvfrom(65536)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return data\n\n    async def send(self, item: UNIXDatagramPacketType) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    self._raw_socket.sendto(*item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return\n\n\nclass ConnectedUNIXDatagramSocket(_RawSocketMixin, abc.ConnectedUNIXDatagramSocket):\n    async def receive(self) -> bytes:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recv(65536)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    self._raw_socket.send(item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return\n\n\n_read_events: RunVar[dict[Any, asyncio.Event]] = RunVar(\"read_events\")\n_write_events: RunVar[dict[Any, asyncio.Event]] = RunVar(\"write_events\")\n\n\n#\n# Synchronization\n#\n\n\nclass Event(BaseEvent):\n    def __new__(cls) -> Event:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self._event = asyncio.Event()\n\n    def set(self) -> None:\n        self._event.set()\n\n    def is_set(self) -> bool:\n        return self._event.is_set()\n\n    async def wait(self) -> None:\n        if self.is_set():\n            await AsyncIOBackend.checkpoint()\n        else:\n            await self._event.wait()\n\n    def statistics(self) -> EventStatistics:\n        return EventStatistics(len(self._event._waiters))\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    _total_tokens: float = 0\n\n    def __new__(cls, total_tokens: float) -> CapacityLimiter:\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float):\n        self._borrowers: set[Any] = set()\n        self._wait_queue: OrderedDict[Any, asyncio.Event] = OrderedDict()\n        self.total_tokens = total_tokens\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    @property\n    def total_tokens(self) -> float:\n        return self._total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and not math.isinf(value):\n            raise TypeError(\"total_tokens must be an int or math.inf\")\n        if value < 1:\n            raise ValueError(\"total_tokens must be >= 1\")\n\n        waiters_to_notify = max(value - self._total_tokens, 0)\n        self._total_tokens = value\n\n        # Notify waiting tasks that they have acquired the limiter\n        while self._wait_queue and waiters_to_notify:\n            event = self._wait_queue.popitem(last=False)[1]\n            event.set()\n            waiters_to_notify -= 1\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return len(self._borrowers)\n\n    @property\n    def available_tokens(self) -> float:\n        return self._total_tokens - len(self._borrowers)\n\n    def acquire_nowait(self) -> None:\n        self.acquire_on_behalf_of_nowait(current_task())\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        if borrower in self._borrowers:\n            raise RuntimeError(\n                \"this borrower is already holding one of this CapacityLimiter's \"\n                \"tokens\"\n            )\n\n        if self._wait_queue or len(self._borrowers) >= self._total_tokens:\n            raise WouldBlock\n\n        self._borrowers.add(borrower)\n\n    async def acquire(self) -> None:\n        return await self.acquire_on_behalf_of(current_task())\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await AsyncIOBackend.checkpoint_if_cancelled()\n        try:\n            self.acquire_on_behalf_of_nowait(borrower)\n        except WouldBlock:\n            event = asyncio.Event()\n            self._wait_queue[borrower] = event\n            try:\n                await event.wait()\n            except BaseException:\n                self._wait_queue.pop(borrower, None)\n                raise\n\n            self._borrowers.add(borrower)\n        else:\n            try:\n                await AsyncIOBackend.cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def release(self) -> None:\n        self.release_on_behalf_of(current_task())\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        try:\n            self._borrowers.remove(borrower)\n        except KeyError:\n            raise RuntimeError(\n                \"this borrower isn't holding any of this CapacityLimiter's tokens\"\n            ) from None\n\n        # Notify the next task in line if this limiter has free capacity now\n        if self._wait_queue and len(self._borrowers) < self._total_tokens:\n            event = self._wait_queue.popitem(last=False)[1]\n            event.set()\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        return CapacityLimiterStatistics(\n            self.borrowed_tokens,\n            self.total_tokens,\n            tuple(self._borrowers),\n            len(self._wait_queue),\n        )\n\n\n_default_thread_limiter: RunVar[CapacityLimiter] = RunVar(\"_default_thread_limiter\")\n\n\n#\n# Operating system signals\n#\n\n\nclass _SignalReceiver:\n    def __init__(self, signals: tuple[Signals, ...]):\n        self._signals = signals\n        self._loop = get_running_loop()\n        self._signal_queue: deque[Signals] = deque()\n        self._future: asyncio.Future = asyncio.Future()\n        self._handled_signals: set[Signals] = set()\n\n    def _deliver(self, signum: Signals) -> None:\n        self._signal_queue.append(signum)\n        if not self._future.done():\n            self._future.set_result(None)\n\n    def __enter__(self) -> _SignalReceiver:\n        for sig in set(self._signals):\n            self._loop.add_signal_handler(sig, self._deliver, sig)\n            self._handled_signals.add(sig)\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        for sig in self._handled_signals:\n            self._loop.remove_signal_handler(sig)\n        return None\n\n    def __aiter__(self) -> _SignalReceiver:\n        return self\n\n    async def __anext__(self) -> Signals:\n        await AsyncIOBackend.checkpoint()\n        if not self._signal_queue:\n            self._future = asyncio.Future()\n            await self._future\n\n        return self._signal_queue.popleft()\n\n\n#\n# Testing and debugging\n#\n\n\nclass AsyncIOTaskInfo(TaskInfo):\n    def __init__(self, task: asyncio.Task):\n        task_state = _task_states.get(task)\n        if task_state is None:\n            parent_id = None\n        else:\n            parent_id = task_state.parent_id\n\n        super().__init__(id(task), parent_id, task.get_name(), task.get_coro())\n        self._task = weakref.ref(task)\n\n    def has_pending_cancellation(self) -> bool:\n        if not (task := self._task()):\n            # If the task isn't around anymore, it won't have a pending cancellation\n            return False\n\n        if sys.version_info >= (3, 11):\n            if task.cancelling():\n                return True\n        elif (\n            isinstance(task._fut_waiter, asyncio.Future)\n            and task._fut_waiter.cancelled()\n        ):\n            return True\n\n        if task_state := _task_states.get(task):\n            if cancel_scope := task_state.cancel_scope:\n                return cancel_scope.cancel_called or cancel_scope._parent_cancelled()\n\n        return False\n\n\nclass TestRunner(abc.TestRunner):\n    _send_stream: MemoryObjectSendStream[tuple[Awaitable[Any], asyncio.Future[Any]]]\n\n    def __init__(\n        self,\n        *,\n        debug: bool | None = None,\n        use_uvloop: bool = False,\n        loop_factory: Callable[[], AbstractEventLoop] | None = None,\n    ) -> None:\n        if use_uvloop and loop_factory is None:\n            import uvloop\n\n            loop_factory = uvloop.new_event_loop\n\n        self._runner = Runner(debug=debug, loop_factory=loop_factory)\n        self._exceptions: list[BaseException] = []\n        self._runner_task: asyncio.Task | None = None\n\n    def __enter__(self) -> TestRunner:\n        self._runner.__enter__()\n        self.get_loop().set_exception_handler(self._exception_handler)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self._runner.__exit__(exc_type, exc_val, exc_tb)\n\n    def get_loop(self) -> AbstractEventLoop:\n        return self._runner.get_loop()\n\n    def _exception_handler(\n        self, loop: asyncio.AbstractEventLoop, context: dict[str, Any]\n    ) -> None:\n        if isinstance(context.get(\"exception\"), Exception):\n            self._exceptions.append(context[\"exception\"])\n        else:\n            loop.default_exception_handler(context)\n\n    def _raise_async_exceptions(self) -> None:\n        # Re-raise any exceptions raised in asynchronous callbacks\n        if self._exceptions:\n            exceptions, self._exceptions = self._exceptions, []\n            if len(exceptions) == 1:\n                raise exceptions[0]\n            elif exceptions:\n                raise BaseExceptionGroup(\n                    \"Multiple exceptions occurred in asynchronous callbacks\", exceptions\n                )\n\n    async def _run_tests_and_fixtures(\n        self,\n        receive_stream: MemoryObjectReceiveStream[\n            tuple[Awaitable[T_Retval], asyncio.Future[T_Retval]]\n        ],\n    ) -> None:\n        with receive_stream, self._send_stream:\n            async for coro, future in receive_stream:\n                try:\n                    retval = await coro\n                except BaseException as exc:\n                    if not future.cancelled():\n                        future.set_exception(exc)\n                else:\n                    if not future.cancelled():\n                        future.set_result(retval)\n\n    async def _call_in_runner_task(\n        self,\n        func: Callable[P, Awaitable[T_Retval]],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> T_Retval:\n        if not self._runner_task:\n            self._send_stream, receive_stream = create_memory_object_stream[\n                Tuple[Awaitable[Any], asyncio.Future]\n            ](1)\n            self._runner_task = self.get_loop().create_task(\n                self._run_tests_and_fixtures(receive_stream)\n            )\n\n        coro = func(*args, **kwargs)\n        future: asyncio.Future[T_Retval] = self.get_loop().create_future()\n        self._send_stream.send_nowait((coro, future))\n        return await future\n\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[T_Retval, Any]],\n        kwargs: dict[str, Any],\n    ) -> Iterable[T_Retval]:\n        asyncgen = fixture_func(**kwargs)\n        fixturevalue: T_Retval = self.get_loop().run_until_complete(\n            self._call_in_runner_task(asyncgen.asend, None)\n        )\n        self._raise_async_exceptions()\n\n        yield fixturevalue\n\n        try:\n            self.get_loop().run_until_complete(\n                self._call_in_runner_task(asyncgen.asend, None)\n            )\n        except StopAsyncIteration:\n            self._raise_async_exceptions()\n        else:\n            self.get_loop().run_until_complete(asyncgen.aclose())\n            raise RuntimeError(\"Async generator fixture did not stop\")\n\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        kwargs: dict[str, Any],\n    ) -> T_Retval:\n        retval = self.get_loop().run_until_complete(\n            self._call_in_runner_task(fixture_func, **kwargs)\n        )\n        self._raise_async_exceptions()\n        return retval\n\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: dict[str, Any]\n    ) -> None:\n        try:\n            self.get_loop().run_until_complete(\n                self._call_in_runner_task(test_func, **kwargs)\n            )\n        except Exception as exc:\n            self._exceptions.append(exc)\n\n        self._raise_async_exceptions()\n\n\nclass AsyncIOBackend(AsyncBackend):\n    @classmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        @wraps(func)\n        async def wrapper() -> T_Retval:\n            task = cast(asyncio.Task, current_task())\n            task.set_name(get_callable_name(func))\n            _task_states[task] = TaskState(None, None)\n\n            try:\n                return await func(*args)\n            finally:\n                del _task_states[task]\n\n        debug = options.get(\"debug\", None)\n        loop_factory = options.get(\"loop_factory\", None)\n        if loop_factory is None and options.get(\"use_uvloop\", False):\n            import uvloop\n\n            loop_factory = uvloop.new_event_loop\n\n        with Runner(debug=debug, loop_factory=loop_factory) as runner:\n            return runner.run(wrapper())\n\n    @classmethod\n    def current_token(cls) -> object:\n        return get_running_loop()\n\n    @classmethod\n    def current_time(cls) -> float:\n        return get_running_loop().time()\n\n    @classmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        return CancelledError\n\n    @classmethod\n    async def checkpoint(cls) -> None:\n        await sleep(0)\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        task = current_task()\n        if task is None:\n            return\n\n        try:\n            cancel_scope = _task_states[task].cancel_scope\n        except KeyError:\n            return\n\n        while cancel_scope:\n            if cancel_scope.cancel_called:\n                await sleep(0)\n            elif cancel_scope.shield:\n                break\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        with CancelScope(shield=True):\n            await sleep(0)\n\n    @classmethod\n    async def sleep(cls, delay: float) -> None:\n        await sleep(delay)\n\n    @classmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return CancelScope(deadline=deadline, shield=shield)\n\n    @classmethod\n    def current_effective_deadline(cls) -> float:\n        try:\n            cancel_scope = _task_states[\n                current_task()  # type: ignore[index]\n            ].cancel_scope\n        except KeyError:\n            return math.inf\n\n        deadline = math.inf\n        while cancel_scope:\n            deadline = min(deadline, cancel_scope.deadline)\n            if cancel_scope._cancel_called:\n                deadline = -math.inf\n                break\n            elif cancel_scope.shield:\n                break\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return deadline\n\n    @classmethod\n    def create_task_group(cls) -> abc.TaskGroup:\n        return TaskGroup()\n\n    @classmethod\n    def create_event(cls) -> abc.Event:\n        return Event()\n\n    @classmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> abc.CapacityLimiter:\n        return CapacityLimiter(total_tokens)\n\n    @classmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        abandon_on_cancel: bool = False,\n        limiter: abc.CapacityLimiter | None = None,\n    ) -> T_Retval:\n        await cls.checkpoint()\n\n        # If this is the first run in this event loop thread, set up the necessary\n        # variables\n        try:\n            idle_workers = _threadpool_idle_workers.get()\n            workers = _threadpool_workers.get()\n        except LookupError:\n            idle_workers = deque()\n            workers = set()\n            _threadpool_idle_workers.set(idle_workers)\n            _threadpool_workers.set(workers)\n\n        async with limiter or cls.current_default_thread_limiter():\n            with CancelScope(shield=not abandon_on_cancel) as scope:\n                future: asyncio.Future = asyncio.Future()\n                root_task = find_root_task()\n                if not idle_workers:\n                    worker = WorkerThread(root_task, workers, idle_workers)\n                    worker.start()\n                    workers.add(worker)\n                    root_task.add_done_callback(worker.stop)\n                else:\n                    worker = idle_workers.pop()\n\n                    # Prune any other workers that have been idle for MAX_IDLE_TIME\n                    # seconds or longer\n                    now = cls.current_time()\n                    while idle_workers:\n                        if (\n                            now - idle_workers[0].idle_since\n                            < WorkerThread.MAX_IDLE_TIME\n                        ):\n                            break\n\n                        expired_worker = idle_workers.popleft()\n                        expired_worker.root_task.remove_done_callback(\n                            expired_worker.stop\n                        )\n                        expired_worker.stop()\n\n                context = copy_context()\n                context.run(sniffio.current_async_library_cvar.set, None)\n                if abandon_on_cancel or scope._parent_scope is None:\n                    worker_scope = scope\n                else:\n                    worker_scope = scope._parent_scope\n\n                worker.queue.put_nowait((context, func, args, future, worker_scope))\n                return await future\n\n    @classmethod\n    def check_cancelled(cls) -> None:\n        scope: CancelScope | None = threadlocals.current_cancel_scope\n        while scope is not None:\n            if scope.cancel_called:\n                raise CancelledError(f\"Cancelled by cancel scope {id(scope):x}\")\n\n            if scope.shield:\n                return\n\n            scope = scope._parent_scope\n\n    @classmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        async def task_wrapper(scope: CancelScope) -> T_Retval:\n            __tracebackhide__ = True\n            task = cast(asyncio.Task, current_task())\n            _task_states[task] = TaskState(None, scope)\n            scope._tasks.add(task)\n            try:\n                return await func(*args)\n            except CancelledError as exc:\n                raise concurrent.futures.CancelledError(str(exc)) from None\n            finally:\n                scope._tasks.discard(task)\n\n        loop = cast(AbstractEventLoop, token)\n        context = copy_context()\n        context.run(sniffio.current_async_library_cvar.set, \"asyncio\")\n        wrapper = task_wrapper(threadlocals.current_cancel_scope)\n        f: concurrent.futures.Future[T_Retval] = context.run(\n            asyncio.run_coroutine_threadsafe, wrapper, loop\n        )\n        return f.result()\n\n    @classmethod\n    def run_sync_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        @wraps(func)\n        def wrapper() -> None:\n            try:\n                sniffio.current_async_library_cvar.set(\"asyncio\")\n                f.set_result(func(*args))\n            except BaseException as exc:\n                f.set_exception(exc)\n                if not isinstance(exc, Exception):\n                    raise\n\n        f: concurrent.futures.Future[T_Retval] = Future()\n        loop = cast(AbstractEventLoop, token)\n        loop.call_soon_threadsafe(wrapper)\n        return f.result()\n\n    @classmethod\n    def create_blocking_portal(cls) -> abc.BlockingPortal:\n        return BlockingPortal()\n\n    @classmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        await cls.checkpoint()\n        if shell:\n            process = await asyncio.create_subprocess_shell(\n                cast(\"str | bytes\", command),\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                cwd=cwd,\n                env=env,\n                start_new_session=start_new_session,\n            )\n        else:\n            process = await asyncio.create_subprocess_exec(\n                *command,\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                cwd=cwd,\n                env=env,\n                start_new_session=start_new_session,\n            )\n\n        stdin_stream = StreamWriterWrapper(process.stdin) if process.stdin else None\n        stdout_stream = StreamReaderWrapper(process.stdout) if process.stdout else None\n        stderr_stream = StreamReaderWrapper(process.stderr) if process.stderr else None\n        return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n    @classmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[abc.Process]) -> None:\n        create_task(\n            _shutdown_process_pool_on_exit(workers),\n            name=\"AnyIO process pool shutdown task\",\n        )\n        find_root_task().add_done_callback(\n            partial(_forcibly_shutdown_process_pool_on_exit, workers)\n        )\n\n    @classmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> abc.SocketStream:\n        transport, protocol = cast(\n            Tuple[asyncio.Transport, StreamProtocol],\n            await get_running_loop().create_connection(\n                StreamProtocol, host, port, local_addr=local_address\n            ),\n        )\n        transport.pause_reading()\n        return SocketStream(transport, protocol)\n\n    @classmethod\n    async def connect_unix(cls, path: str | bytes) -> abc.UNIXSocketStream:\n        await cls.checkpoint()\n        loop = get_running_loop()\n        raw_socket = socket.socket(socket.AF_UNIX)\n        raw_socket.setblocking(False)\n        while True:\n            try:\n                raw_socket.connect(path)\n            except BlockingIOError:\n                f: asyncio.Future = asyncio.Future()\n                loop.add_writer(raw_socket, f.set_result, None)\n                f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n                await f\n            except BaseException:\n                raw_socket.close()\n                raise\n            else:\n                return UNIXSocketStream(raw_socket)\n\n    @classmethod\n    def create_tcp_listener(cls, sock: socket.socket) -> SocketListener:\n        return TCPSocketListener(sock)\n\n    @classmethod\n    def create_unix_listener(cls, sock: socket.socket) -> SocketListener:\n        return UNIXSocketListener(sock)\n\n    @classmethod\n    async def create_udp_socket(\n        cls,\n        family: AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        transport, protocol = await get_running_loop().create_datagram_endpoint(\n            DatagramProtocol,\n            local_addr=local_address,\n            remote_addr=remote_address,\n            family=family,\n            reuse_port=reuse_port,\n        )\n        if protocol.exception:\n            transport.close()\n            raise protocol.exception\n\n        if not remote_address:\n            return UDPSocket(transport, protocol)\n        else:\n            return ConnectedUDPSocket(transport, protocol)\n\n    @classmethod\n    async def create_unix_datagram_socket(  # type: ignore[override]\n        cls, raw_socket: socket.socket, remote_path: str | bytes | None\n    ) -> abc.UNIXDatagramSocket | abc.ConnectedUNIXDatagramSocket:\n        await cls.checkpoint()\n        loop = get_running_loop()\n\n        if remote_path:\n            while True:\n                try:\n                    raw_socket.connect(remote_path)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    loop.add_writer(raw_socket, f.set_result, None)\n                    f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n                    await f\n                except BaseException:\n                    raw_socket.close()\n                    raise\n                else:\n                    return ConnectedUNIXDatagramSocket(raw_socket)\n        else:\n            return UNIXDatagramSocket(raw_socket)\n\n    @classmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        return await get_running_loop().getaddrinfo(\n            host, port, family=family, type=type, proto=proto, flags=flags\n        )\n\n    @classmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        return await get_running_loop().getnameinfo(sockaddr, flags)\n\n    @classmethod\n    async def wait_socket_readable(cls, sock: socket.socket) -> None:\n        await cls.checkpoint()\n        try:\n            read_events = _read_events.get()\n        except LookupError:\n            read_events = {}\n            _read_events.set(read_events)\n\n        if read_events.get(sock):\n            raise BusyResourceError(\"reading from\") from None\n\n        loop = get_running_loop()\n        event = read_events[sock] = asyncio.Event()\n        loop.add_reader(sock, event.set)\n        try:\n            await event.wait()\n        finally:\n            if read_events.pop(sock, None) is not None:\n                loop.remove_reader(sock)\n                readable = True\n            else:\n                readable = False\n\n        if not readable:\n            raise ClosedResourceError\n\n    @classmethod\n    async def wait_socket_writable(cls, sock: socket.socket) -> None:\n        await cls.checkpoint()\n        try:\n            write_events = _write_events.get()\n        except LookupError:\n            write_events = {}\n            _write_events.set(write_events)\n\n        if write_events.get(sock):\n            raise BusyResourceError(\"writing to\") from None\n\n        loop = get_running_loop()\n        event = write_events[sock] = asyncio.Event()\n        loop.add_writer(sock.fileno(), event.set)\n        try:\n            await event.wait()\n        finally:\n            if write_events.pop(sock, None) is not None:\n                loop.remove_writer(sock)\n                writable = True\n            else:\n                writable = False\n\n        if not writable:\n            raise ClosedResourceError\n\n    @classmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        try:\n            return _default_thread_limiter.get()\n        except LookupError:\n            limiter = CapacityLimiter(40)\n            _default_thread_limiter.set(limiter)\n            return limiter\n\n    @classmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        return _SignalReceiver(signals)\n\n    @classmethod\n    def get_current_task(cls) -> TaskInfo:\n        return AsyncIOTaskInfo(current_task())  # type: ignore[arg-type]\n\n    @classmethod\n    def get_running_tasks(cls) -> Sequence[TaskInfo]:\n        return [AsyncIOTaskInfo(task) for task in all_tasks() if not task.done()]\n\n    @classmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        await cls.checkpoint()\n        this_task = current_task()\n        while True:\n            for task in all_tasks():\n                if task is this_task:\n                    continue\n\n                waiter = task._fut_waiter  # type: ignore[attr-defined]\n                if waiter is None or waiter.done():\n                    await sleep(0.1)\n                    break\n            else:\n                return\n\n    @classmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        return TestRunner(**options)\n\n\nbackend_class = AsyncIOBackend\n", "src/anyio/_backends/_trio.py": "from __future__ import annotations\n\nimport array\nimport math\nimport socket\nimport sys\nimport types\nimport weakref\nfrom collections.abc import AsyncIterator, Iterable\nfrom concurrent.futures import Future\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom io import IOBase\nfrom os import PathLike\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Collection,\n    ContextManager,\n    Coroutine,\n    Generic,\n    Mapping,\n    NoReturn,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport trio.from_thread\nimport trio.lowlevel\nfrom outcome import Error, Outcome, Value\nfrom trio.lowlevel import (\n    current_root_task,\n    current_task,\n    wait_readable,\n    wait_writable,\n)\nfrom trio.socket import SocketType as TrioSocketType\nfrom trio.to_thread import run_sync\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._eventloop import claim_worker_thread\nfrom .._core._exceptions import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n)\nfrom .._core._sockets import convert_ipv6_sockaddr\nfrom .._core._streams import create_memory_object_stream\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import IPSockAddrType, UDPPacketType, UNIXDatagramPacketType\nfrom ..abc._eventloop import AsyncBackend\nfrom ..streams.memory import MemoryObjectSendStream\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from exceptiongroup import BaseExceptionGroup\n    from typing_extensions import TypeVarTuple, Unpack\n\nT = TypeVar(\"T\")\nT_Retval = TypeVar(\"T_Retval\")\nT_SockAddr = TypeVar(\"T_SockAddr\", str, IPSockAddrType)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\nP = ParamSpec(\"P\")\n\n\n#\n# Event loop\n#\n\nRunVar = trio.lowlevel.RunVar\n\n\n#\n# Timeouts and cancellation\n#\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(\n        cls, original: trio.CancelScope | None = None, **kwargs: object\n    ) -> CancelScope:\n        return object.__new__(cls)\n\n    def __init__(self, original: trio.CancelScope | None = None, **kwargs: Any) -> None:\n        self.__original = original or trio.CancelScope(**kwargs)\n\n    def __enter__(self) -> CancelScope:\n        self.__original.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        # https://github.com/python-trio/trio-typing/pull/79\n        return self.__original.__exit__(exc_type, exc_val, exc_tb)\n\n    def cancel(self) -> None:\n        self.__original.cancel()\n\n    @property\n    def deadline(self) -> float:\n        return self.__original.deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self.__original.deadline = value\n\n    @property\n    def cancel_called(self) -> bool:\n        return self.__original.cancel_called\n\n    @property\n    def cancelled_caught(self) -> bool:\n        return self.__original.cancelled_caught\n\n    @property\n    def shield(self) -> bool:\n        return self.__original.shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        self.__original.shield = value\n\n\n#\n# Task groups\n#\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self._active = False\n        self._nursery_manager = trio.open_nursery(strict_exception_groups=True)\n        self.cancel_scope = None  # type: ignore[assignment]\n\n    async def __aenter__(self) -> TaskGroup:\n        self._active = True\n        self._nursery = await self._nursery_manager.__aenter__()\n        self.cancel_scope = CancelScope(self._nursery.cancel_scope)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        try:\n            return await self._nursery_manager.__aexit__(exc_type, exc_val, exc_tb)\n        except BaseExceptionGroup as exc:\n            _, rest = exc.split(trio.Cancelled)\n            if not rest:\n                cancelled_exc = trio.Cancelled._create()\n                raise cancelled_exc from exc\n\n            raise\n        finally:\n            self._active = False\n\n    def start_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> None:\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        self._nursery.start_soon(func, *args, name=name)\n\n    async def start(\n        self, func: Callable[..., Awaitable[Any]], *args: object, name: object = None\n    ) -> Any:\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        return await self._nursery.start(func, *args, name=name)\n\n\n#\n# Threads\n#\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> BlockingPortal:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._token = trio.lowlevel.current_trio_token()\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future[T_Retval],\n    ) -> None:\n        trio.from_thread.run_sync(\n            partial(self._task_group.start_soon, name=name),\n            self._call_func,\n            func,\n            args,\n            kwargs,\n            future,\n            trio_token=self._token,\n        )\n\n\n#\n# Subprocesses\n#\n\n\n@dataclass(eq=False)\nclass ReceiveStreamWrapper(abc.ByteReceiveStream):\n    _stream: trio.abc.ReceiveStream\n\n    async def receive(self, max_bytes: int | None = None) -> bytes:\n        try:\n            data = await self._stream.receive_some(max_bytes)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError from exc.__cause__\n        except trio.BrokenResourceError as exc:\n            raise BrokenResourceError from exc.__cause__\n\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        await self._stream.aclose()\n\n\n@dataclass(eq=False)\nclass SendStreamWrapper(abc.ByteSendStream):\n    _stream: trio.abc.SendStream\n\n    async def send(self, item: bytes) -> None:\n        try:\n            await self._stream.send_all(item)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError from exc.__cause__\n        except trio.BrokenResourceError as exc:\n            raise BrokenResourceError from exc.__cause__\n\n    async def aclose(self) -> None:\n        await self._stream.aclose()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: trio.Process\n    _stdin: abc.ByteSendStream | None\n    _stdout: abc.ByteReceiveStream | None\n    _stderr: abc.ByteReceiveStream | None\n\n    async def aclose(self) -> None:\n        with CancelScope(shield=True):\n            if self._stdin:\n                await self._stdin.aclose()\n            if self._stdout:\n                await self._stdout.aclose()\n            if self._stderr:\n                await self._stderr.aclose()\n\n        try:\n            await self.wait()\n        except BaseException:\n            self.kill()\n            with CancelScope(shield=True):\n                await self.wait()\n            raise\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: Signals) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> int | None:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> abc.ByteSendStream | None:\n        return self._stdin\n\n    @property\n    def stdout(self) -> abc.ByteReceiveStream | None:\n        return self._stdout\n\n    @property\n    def stderr(self) -> abc.ByteReceiveStream | None:\n        return self._stderr\n\n\nclass _ProcessPoolShutdownInstrument(trio.abc.Instrument):\n    def after_run(self) -> None:\n        super().after_run()\n\n\ncurrent_default_worker_process_limiter: trio.lowlevel.RunVar = RunVar(\n    \"current_default_worker_process_limiter\"\n)\n\n\nasync def _shutdown_process_pool(workers: set[abc.Process]) -> None:\n    try:\n        await trio.sleep(math.inf)\n    except trio.Cancelled:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        with CancelScope(shield=True):\n            for process in workers:\n                await process.aclose()\n\n\n#\n# Sockets and networking\n#\n\n\nclass _TrioSocketMixin(Generic[T_SockAddr]):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        self._trio_socket = trio_socket\n        self._closed = False\n\n    def _check_closed(self) -> None:\n        if self._closed:\n            raise ClosedResourceError\n        if self._trio_socket.fileno() < 0:\n            raise BrokenResourceError\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._trio_socket._sock  # type: ignore[attr-defined]\n\n    async def aclose(self) -> None:\n        if self._trio_socket.fileno() >= 0:\n            self._closed = True\n            self._trio_socket.close()\n\n    def _convert_socket_error(self, exc: BaseException) -> NoReturn:\n        if isinstance(exc, trio.ClosedResourceError):\n            raise ClosedResourceError from exc\n        elif self._trio_socket.fileno() < 0 and self._closed:\n            raise ClosedResourceError from None\n        elif isinstance(exc, OSError):\n            raise BrokenResourceError from exc\n        else:\n            raise exc\n\n\nclass SocketStream(_TrioSocketMixin, abc.SocketStream):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            try:\n                data = await self._trio_socket.recv(max_bytes)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n            if data:\n                return data\n            else:\n                raise EndOfStream\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = await self._trio_socket.send(view)\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n\n                view = view[bytes_sent:]\n\n    async def send_eof(self) -> None:\n        self._trio_socket.shutdown(socket.SHUT_WR)\n\n\nclass UNIXSocketStream(SocketStream, abc.UNIXSocketStream):\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError(\"msglen must be a non-negative integer\")\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError(\"maxfds must be a positive integer\")\n\n        fds = array.array(\"i\")\n        await trio.lowlevel.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = await self._trio_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize)\n                    )\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(\n                    f\"Received unexpected ancillary data; message = {message!r}, \"\n                    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n                )\n\n            fds.frombytes(cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        if not message:\n            raise ValueError(\"message must not be empty\")\n        if not fds:\n            raise ValueError(\"fds must not be empty\")\n\n        filenos: list[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await trio.lowlevel.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    await self._trio_socket.sendmsg(\n                        [message],\n                        [\n                            (\n                                socket.SOL_SOCKET,\n                                socket.SCM_RIGHTS,\n                                fdarray,\n                            )\n                        ],\n                    )\n                    break\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n\n\nclass TCPSocketListener(_TrioSocketMixin, abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    async def accept(self) -> SocketStream:\n        with self._accept_guard:\n            try:\n                trio_socket, _addr = await self._trio_socket.accept()\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n        trio_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        return SocketStream(trio_socket)\n\n\nclass UNIXSocketListener(_TrioSocketMixin, abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    async def accept(self) -> UNIXSocketStream:\n        with self._accept_guard:\n            try:\n                trio_socket, _addr = await self._trio_socket.accept()\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n        return UNIXSocketStream(trio_socket)\n\n\nclass UDPSocket(_TrioSocketMixin[IPSockAddrType], abc.UDPSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            try:\n                data, addr = await self._trio_socket.recvfrom(65536)\n                return data, convert_ipv6_sockaddr(addr)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.sendto(*item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass ConnectedUDPSocket(_TrioSocketMixin[IPSockAddrType], abc.ConnectedUDPSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            try:\n                return await self._trio_socket.recv(65536)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.send(item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass UNIXDatagramSocket(_TrioSocketMixin[str], abc.UNIXDatagramSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> UNIXDatagramPacketType:\n        with self._receive_guard:\n            try:\n                data, addr = await self._trio_socket.recvfrom(65536)\n                return data, addr\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: UNIXDatagramPacketType) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.sendto(*item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass ConnectedUNIXDatagramSocket(\n    _TrioSocketMixin[str], abc.ConnectedUNIXDatagramSocket\n):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            try:\n                return await self._trio_socket.recv(65536)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.send(item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\n#\n# Synchronization\n#\n\n\nclass Event(BaseEvent):\n    def __new__(cls) -> Event:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self.__original = trio.Event()\n\n    def is_set(self) -> bool:\n        return self.__original.is_set()\n\n    async def wait(self) -> None:\n        return await self.__original.wait()\n\n    def statistics(self) -> EventStatistics:\n        orig_statistics = self.__original.statistics()\n        return EventStatistics(tasks_waiting=orig_statistics.tasks_waiting)\n\n    def set(self) -> None:\n        self.__original.set()\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    def __new__(\n        cls,\n        total_tokens: float | None = None,\n        *,\n        original: trio.CapacityLimiter | None = None,\n    ) -> CapacityLimiter:\n        return object.__new__(cls)\n\n    def __init__(\n        self,\n        total_tokens: float | None = None,\n        *,\n        original: trio.CapacityLimiter | None = None,\n    ) -> None:\n        if original is not None:\n            self.__original = original\n        else:\n            assert total_tokens is not None\n            self.__original = trio.CapacityLimiter(total_tokens)\n\n    async def __aenter__(self) -> None:\n        return await self.__original.__aenter__()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.__original.__aexit__(exc_type, exc_val, exc_tb)\n\n    @property\n    def total_tokens(self) -> float:\n        return self.__original.total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        self.__original.total_tokens = value\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return self.__original.borrowed_tokens\n\n    @property\n    def available_tokens(self) -> float:\n        return self.__original.available_tokens\n\n    def acquire_nowait(self) -> None:\n        self.__original.acquire_nowait()\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        self.__original.acquire_on_behalf_of_nowait(borrower)\n\n    async def acquire(self) -> None:\n        await self.__original.acquire()\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await self.__original.acquire_on_behalf_of(borrower)\n\n    def release(self) -> None:\n        return self.__original.release()\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        return self.__original.release_on_behalf_of(borrower)\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        orig = self.__original.statistics()\n        return CapacityLimiterStatistics(\n            borrowed_tokens=orig.borrowed_tokens,\n            total_tokens=orig.total_tokens,\n            borrowers=tuple(orig.borrowers),\n            tasks_waiting=orig.tasks_waiting,\n        )\n\n\n_capacity_limiter_wrapper: trio.lowlevel.RunVar = RunVar(\"_capacity_limiter_wrapper\")\n\n\n#\n# Signal handling\n#\n\n\nclass _SignalReceiver:\n    _iterator: AsyncIterator[int]\n\n    def __init__(self, signals: tuple[Signals, ...]):\n        self._signals = signals\n\n    def __enter__(self) -> _SignalReceiver:\n        self._cm = trio.open_signal_receiver(*self._signals)\n        self._iterator = self._cm.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        return self._cm.__exit__(exc_type, exc_val, exc_tb)\n\n    def __aiter__(self) -> _SignalReceiver:\n        return self\n\n    async def __anext__(self) -> Signals:\n        signum = await self._iterator.__anext__()\n        return Signals(signum)\n\n\n#\n# Testing and debugging\n#\n\n\nclass TestRunner(abc.TestRunner):\n    def __init__(self, **options: Any) -> None:\n        from queue import Queue\n\n        self._call_queue: Queue[Callable[[], object]] = Queue()\n        self._send_stream: MemoryObjectSendStream | None = None\n        self._options = options\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -> None:\n        if self._send_stream:\n            self._send_stream.close()\n            while self._send_stream is not None:\n                self._call_queue.get()()\n\n    async def _run_tests_and_fixtures(self) -> None:\n        self._send_stream, receive_stream = create_memory_object_stream(1)\n        with receive_stream:\n            async for coro, outcome_holder in receive_stream:\n                try:\n                    retval = await coro\n                except BaseException as exc:\n                    outcome_holder.append(Error(exc))\n                else:\n                    outcome_holder.append(Value(retval))\n\n    def _main_task_finished(self, outcome: object) -> None:\n        self._send_stream = None\n\n    def _call_in_runner_task(\n        self,\n        func: Callable[P, Awaitable[T_Retval]],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> T_Retval:\n        if self._send_stream is None:\n            trio.lowlevel.start_guest_run(\n                self._run_tests_and_fixtures,\n                run_sync_soon_threadsafe=self._call_queue.put,\n                done_callback=self._main_task_finished,\n                **self._options,\n            )\n            while self._send_stream is None:\n                self._call_queue.get()()\n\n        outcome_holder: list[Outcome] = []\n        self._send_stream.send_nowait((func(*args, **kwargs), outcome_holder))\n        while not outcome_holder:\n            self._call_queue.get()()\n\n        return outcome_holder[0].unwrap()\n\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[T_Retval, Any]],\n        kwargs: dict[str, Any],\n    ) -> Iterable[T_Retval]:\n        asyncgen = fixture_func(**kwargs)\n        fixturevalue: T_Retval = self._call_in_runner_task(asyncgen.asend, None)\n\n        yield fixturevalue\n\n        try:\n            self._call_in_runner_task(asyncgen.asend, None)\n        except StopAsyncIteration:\n            pass\n        else:\n            self._call_in_runner_task(asyncgen.aclose)\n            raise RuntimeError(\"Async generator fixture did not stop\")\n\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        kwargs: dict[str, Any],\n    ) -> T_Retval:\n        return self._call_in_runner_task(fixture_func, **kwargs)\n\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: dict[str, Any]\n    ) -> None:\n        self._call_in_runner_task(test_func, **kwargs)\n\n\nclass TrioTaskInfo(TaskInfo):\n    def __init__(self, task: trio.lowlevel.Task):\n        parent_id = None\n        if task.parent_nursery and task.parent_nursery.parent_task:\n            parent_id = id(task.parent_nursery.parent_task)\n\n        super().__init__(id(task), parent_id, task.name, task.coro)\n        self._task = weakref.proxy(task)\n\n    def has_pending_cancellation(self) -> bool:\n        try:\n            return self._task._cancel_status.effectively_cancelled\n        except ReferenceError:\n            # If the task is no longer around, it surely doesn't have a cancellation\n            # pending\n            return False\n\n\nclass TrioBackend(AsyncBackend):\n    @classmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        return trio.run(func, *args)\n\n    @classmethod\n    def current_token(cls) -> object:\n        return trio.lowlevel.current_trio_token()\n\n    @classmethod\n    def current_time(cls) -> float:\n        return trio.current_time()\n\n    @classmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        return trio.Cancelled\n\n    @classmethod\n    async def checkpoint(cls) -> None:\n        await trio.lowlevel.checkpoint()\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        await trio.lowlevel.checkpoint_if_cancelled()\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        await trio.lowlevel.cancel_shielded_checkpoint()\n\n    @classmethod\n    async def sleep(cls, delay: float) -> None:\n        await trio.sleep(delay)\n\n    @classmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> abc.CancelScope:\n        return CancelScope(deadline=deadline, shield=shield)\n\n    @classmethod\n    def current_effective_deadline(cls) -> float:\n        return trio.current_effective_deadline()\n\n    @classmethod\n    def create_task_group(cls) -> abc.TaskGroup:\n        return TaskGroup()\n\n    @classmethod\n    def create_event(cls) -> abc.Event:\n        return Event()\n\n    @classmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> CapacityLimiter:\n        return CapacityLimiter(total_tokens)\n\n    @classmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        abandon_on_cancel: bool = False,\n        limiter: abc.CapacityLimiter | None = None,\n    ) -> T_Retval:\n        def wrapper() -> T_Retval:\n            with claim_worker_thread(TrioBackend, token):\n                return func(*args)\n\n        token = TrioBackend.current_token()\n        return await run_sync(\n            wrapper,\n            abandon_on_cancel=abandon_on_cancel,\n            limiter=cast(trio.CapacityLimiter, limiter),\n        )\n\n    @classmethod\n    def check_cancelled(cls) -> None:\n        trio.from_thread.check_cancelled()\n\n    @classmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        return trio.from_thread.run(func, *args)\n\n    @classmethod\n    def run_sync_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        return trio.from_thread.run_sync(func, *args)\n\n    @classmethod\n    def create_blocking_portal(cls) -> abc.BlockingPortal:\n        return BlockingPortal()\n\n    @classmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        process = await trio.lowlevel.open_process(  # type: ignore[misc]\n            command,  # type: ignore[arg-type]\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            shell=shell,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n        stdin_stream = SendStreamWrapper(process.stdin) if process.stdin else None\n        stdout_stream = ReceiveStreamWrapper(process.stdout) if process.stdout else None\n        stderr_stream = ReceiveStreamWrapper(process.stderr) if process.stderr else None\n        return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n    @classmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[abc.Process]) -> None:\n        trio.lowlevel.spawn_system_task(_shutdown_process_pool, workers)\n\n    @classmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> SocketStream:\n        family = socket.AF_INET6 if \":\" in host else socket.AF_INET\n        trio_socket = trio.socket.socket(family)\n        trio_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        if local_address:\n            await trio_socket.bind(local_address)\n\n        try:\n            await trio_socket.connect((host, port))\n        except BaseException:\n            trio_socket.close()\n            raise\n\n        return SocketStream(trio_socket)\n\n    @classmethod\n    async def connect_unix(cls, path: str | bytes) -> abc.UNIXSocketStream:\n        trio_socket = trio.socket.socket(socket.AF_UNIX)\n        try:\n            await trio_socket.connect(path)\n        except BaseException:\n            trio_socket.close()\n            raise\n\n        return UNIXSocketStream(trio_socket)\n\n    @classmethod\n    def create_tcp_listener(cls, sock: socket.socket) -> abc.SocketListener:\n        return TCPSocketListener(sock)\n\n    @classmethod\n    def create_unix_listener(cls, sock: socket.socket) -> abc.SocketListener:\n        return UNIXSocketListener(sock)\n\n    @classmethod\n    async def create_udp_socket(\n        cls,\n        family: socket.AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        trio_socket = trio.socket.socket(family=family, type=socket.SOCK_DGRAM)\n\n        if reuse_port:\n            trio_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n\n        if local_address:\n            await trio_socket.bind(local_address)\n\n        if remote_address:\n            await trio_socket.connect(remote_address)\n            return ConnectedUDPSocket(trio_socket)\n        else:\n            return UDPSocket(trio_socket)\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket.socket, remote_path: None\n    ) -> abc.UNIXDatagramSocket: ...\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket.socket, remote_path: str | bytes\n    ) -> abc.ConnectedUNIXDatagramSocket: ...\n\n    @classmethod\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket.socket, remote_path: str | bytes | None\n    ) -> abc.UNIXDatagramSocket | abc.ConnectedUNIXDatagramSocket:\n        trio_socket = trio.socket.from_stdlib_socket(raw_socket)\n\n        if remote_path:\n            await trio_socket.connect(remote_path)\n            return ConnectedUNIXDatagramSocket(trio_socket)\n        else:\n            return UNIXDatagramSocket(trio_socket)\n\n    @classmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        return await trio.socket.getaddrinfo(host, port, family, type, proto, flags)\n\n    @classmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        return await trio.socket.getnameinfo(sockaddr, flags)\n\n    @classmethod\n    async def wait_socket_readable(cls, sock: socket.socket) -> None:\n        try:\n            await wait_readable(sock)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError().with_traceback(exc.__traceback__) from None\n        except trio.BusyResourceError:\n            raise BusyResourceError(\"reading from\") from None\n\n    @classmethod\n    async def wait_socket_writable(cls, sock: socket.socket) -> None:\n        try:\n            await wait_writable(sock)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError().with_traceback(exc.__traceback__) from None\n        except trio.BusyResourceError:\n            raise BusyResourceError(\"writing to\") from None\n\n    @classmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        try:\n            return _capacity_limiter_wrapper.get()\n        except LookupError:\n            limiter = CapacityLimiter(\n                original=trio.to_thread.current_default_thread_limiter()\n            )\n            _capacity_limiter_wrapper.set(limiter)\n            return limiter\n\n    @classmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        return _SignalReceiver(signals)\n\n    @classmethod\n    def get_current_task(cls) -> TaskInfo:\n        task = current_task()\n        return TrioTaskInfo(task)\n\n    @classmethod\n    def get_running_tasks(cls) -> Sequence[TaskInfo]:\n        root_task = current_root_task()\n        assert root_task\n        task_infos = [TrioTaskInfo(root_task)]\n        nurseries = root_task.child_nurseries\n        while nurseries:\n            new_nurseries: list[trio.Nursery] = []\n            for nursery in nurseries:\n                for task in nursery.child_tasks:\n                    task_infos.append(TrioTaskInfo(task))\n                    new_nurseries.extend(task.child_nurseries)\n\n            nurseries = new_nurseries\n\n        return task_infos\n\n    @classmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        from trio.testing import wait_all_tasks_blocked\n\n        await wait_all_tasks_blocked()\n\n    @classmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        return TestRunner(**options)\n\n\nbackend_class = TrioBackend\n", "src/anyio/_backends/__init__.py": "", "src/anyio/abc/_tasks.py": "from __future__ import annotations\n\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom collections.abc import Awaitable, Callable\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Any, Protocol, TypeVar, overload\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nif TYPE_CHECKING:\n    from .._core._tasks import CancelScope\n\nT_Retval = TypeVar(\"T_Retval\")\nT_contra = TypeVar(\"T_contra\", contravariant=True)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\nclass TaskStatus(Protocol[T_contra]):\n    @overload\n    def started(self: TaskStatus[None]) -> None: ...\n\n    @overload\n    def started(self, value: T_contra) -> None: ...\n\n    def started(self, value: T_contra | None = None) -> None:\n        \"\"\"\n        Signal that the task has started.\n\n        :param value: object passed back to the starter of the task\n        \"\"\"\n\n\nclass TaskGroup(metaclass=ABCMeta):\n    \"\"\"\n    Groups several asynchronous tasks together.\n\n    :ivar cancel_scope: the cancel scope inherited by all child tasks\n    :vartype cancel_scope: CancelScope\n    \"\"\"\n\n    cancel_scope: CancelScope\n\n    @abstractmethod\n    def start_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> None:\n        \"\"\"\n        Start a new task in this task group.\n\n        :param func: a coroutine function\n        :param args: positional arguments to call the function with\n        :param name: name of the task, for the purposes of introspection and debugging\n\n        .. versionadded:: 3.0\n        \"\"\"\n\n    @abstractmethod\n    async def start(\n        self,\n        func: Callable[..., Awaitable[Any]],\n        *args: object,\n        name: object = None,\n    ) -> Any:\n        \"\"\"\n        Start a new task and wait until it signals for readiness.\n\n        :param func: a coroutine function\n        :param args: positional arguments to call the function with\n        :param name: name of the task, for the purposes of introspection and debugging\n        :return: the value passed to ``task_status.started()``\n        :raises RuntimeError: if the task finishes without calling\n            ``task_status.started()``\n\n        .. versionadded:: 3.0\n        \"\"\"\n\n    @abstractmethod\n    async def __aenter__(self) -> TaskGroup:\n        \"\"\"Enter the task group context and allow starting new tasks.\"\"\"\n\n    @abstractmethod\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        \"\"\"Exit the task group context waiting for all tasks to finish.\"\"\"\n", "src/anyio/abc/_testing.py": "from __future__ import annotations\n\nimport types\nfrom abc import ABCMeta, abstractmethod\nfrom collections.abc import AsyncGenerator, Callable, Coroutine, Iterable\nfrom typing import Any, TypeVar\n\n_T = TypeVar(\"_T\")\n\n\nclass TestRunner(metaclass=ABCMeta):\n    \"\"\"\n    Encapsulates a running event loop. Every call made through this object will use the\n    same event loop.\n    \"\"\"\n\n    def __enter__(self) -> TestRunner:\n        return self\n\n    @abstractmethod\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -> bool | None: ...\n\n    @abstractmethod\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[_T, Any]],\n        kwargs: dict[str, Any],\n    ) -> Iterable[_T]:\n        \"\"\"\n        Run an async generator fixture.\n\n        :param fixture_func: the fixture function\n        :param kwargs: keyword arguments to call the fixture function with\n        :return: an iterator yielding the value yielded from the async generator\n        \"\"\"\n\n    @abstractmethod\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, _T]],\n        kwargs: dict[str, Any],\n    ) -> _T:\n        \"\"\"\n        Run an async fixture.\n\n        :param fixture_func: the fixture function\n        :param kwargs: keyword arguments to call the fixture function with\n        :return: the return value of the fixture function\n        \"\"\"\n\n    @abstractmethod\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: dict[str, Any]\n    ) -> None:\n        \"\"\"\n        Run an async test function.\n\n        :param test_func: the test function\n        :param kwargs: keyword arguments to call the test function with\n        \"\"\"\n", "src/anyio/abc/_sockets.py": "from __future__ import annotations\n\nimport socket\nfrom abc import abstractmethod\nfrom collections.abc import Callable, Collection, Mapping\nfrom contextlib import AsyncExitStack\nfrom io import IOBase\nfrom ipaddress import IPv4Address, IPv6Address\nfrom socket import AddressFamily\nfrom types import TracebackType\nfrom typing import Any, Tuple, TypeVar, Union\n\nfrom .._core._typedattr import (\n    TypedAttributeProvider,\n    TypedAttributeSet,\n    typed_attribute,\n)\nfrom ._streams import ByteStream, Listener, UnreliableObjectStream\nfrom ._tasks import TaskGroup\n\nIPAddressType = Union[str, IPv4Address, IPv6Address]\nIPSockAddrType = Tuple[str, int]\nSockAddrType = Union[IPSockAddrType, str]\nUDPPacketType = Tuple[bytes, IPSockAddrType]\nUNIXDatagramPacketType = Tuple[bytes, str]\nT_Retval = TypeVar(\"T_Retval\")\n\n\nclass _NullAsyncContextManager:\n    async def __aenter__(self) -> None:\n        pass\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        return None\n\n\nclass SocketAttribute(TypedAttributeSet):\n    #: the address family of the underlying socket\n    family: AddressFamily = typed_attribute()\n    #: the local socket address of the underlying socket\n    local_address: SockAddrType = typed_attribute()\n    #: for IP addresses, the local port the underlying socket is bound to\n    local_port: int = typed_attribute()\n    #: the underlying stdlib socket object\n    raw_socket: socket.socket = typed_attribute()\n    #: the remote address the underlying socket is connected to\n    remote_address: SockAddrType = typed_attribute()\n    #: for IP addresses, the remote port the underlying socket is connected to\n    remote_port: int = typed_attribute()\n\n\nclass _SocketProvider(TypedAttributeProvider):\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        from .._core._sockets import convert_ipv6_sockaddr as convert\n\n        attributes: dict[Any, Callable[[], Any]] = {\n            SocketAttribute.family: lambda: self._raw_socket.family,\n            SocketAttribute.local_address: lambda: convert(\n                self._raw_socket.getsockname()\n            ),\n            SocketAttribute.raw_socket: lambda: self._raw_socket,\n        }\n        try:\n            peername: tuple[str, int] | None = convert(self._raw_socket.getpeername())\n        except OSError:\n            peername = None\n\n        # Provide the remote address for connected sockets\n        if peername is not None:\n            attributes[SocketAttribute.remote_address] = lambda: peername\n\n        # Provide local and remote ports for IP based sockets\n        if self._raw_socket.family in (AddressFamily.AF_INET, AddressFamily.AF_INET6):\n            attributes[SocketAttribute.local_port] = (\n                lambda: self._raw_socket.getsockname()[1]\n            )\n            if peername is not None:\n                remote_port = peername[1]\n                attributes[SocketAttribute.remote_port] = lambda: remote_port\n\n        return attributes\n\n    @property\n    @abstractmethod\n    def _raw_socket(self) -> socket.socket:\n        pass\n\n\nclass SocketStream(ByteStream, _SocketProvider):\n    \"\"\"\n    Transports bytes over a socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n\nclass UNIXSocketStream(SocketStream):\n    @abstractmethod\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        \"\"\"\n        Send file descriptors along with a message to the peer.\n\n        :param message: a non-empty bytestring\n        :param fds: a collection of files (either numeric file descriptors or open file\n            or socket objects)\n        \"\"\"\n\n    @abstractmethod\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        \"\"\"\n        Receive file descriptors along with a message from the peer.\n\n        :param msglen: length of the message to expect from the peer\n        :param maxfds: maximum number of file descriptors to expect from the peer\n        :return: a tuple of (message, file descriptors)\n        \"\"\"\n\n\nclass SocketListener(Listener[SocketStream], _SocketProvider):\n    \"\"\"\n    Listens to incoming socket connections.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    @abstractmethod\n    async def accept(self) -> SocketStream:\n        \"\"\"Accept an incoming connection.\"\"\"\n\n    async def serve(\n        self,\n        handler: Callable[[SocketStream], Any],\n        task_group: TaskGroup | None = None,\n    ) -> None:\n        from .. import create_task_group\n\n        async with AsyncExitStack() as stack:\n            if task_group is None:\n                task_group = await stack.enter_async_context(create_task_group())\n\n            while True:\n                stream = await self.accept()\n                task_group.start_soon(handler, stream)\n\n\nclass UDPSocket(UnreliableObjectStream[UDPPacketType], _SocketProvider):\n    \"\"\"\n    Represents an unconnected UDP socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    async def sendto(self, data: bytes, host: str, port: int) -> None:\n        \"\"\"\n        Alias for :meth:`~.UnreliableObjectSendStream.send` ((data, (host, port))).\n\n        \"\"\"\n        return await self.send((data, (host, port)))\n\n\nclass ConnectedUDPSocket(UnreliableObjectStream[bytes], _SocketProvider):\n    \"\"\"\n    Represents an connected UDP socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n\nclass UNIXDatagramSocket(\n    UnreliableObjectStream[UNIXDatagramPacketType], _SocketProvider\n):\n    \"\"\"\n    Represents an unconnected Unix datagram socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    async def sendto(self, data: bytes, path: str) -> None:\n        \"\"\"Alias for :meth:`~.UnreliableObjectSendStream.send` ((data, path)).\"\"\"\n        return await self.send((data, path))\n\n\nclass ConnectedUNIXDatagramSocket(UnreliableObjectStream[bytes], _SocketProvider):\n    \"\"\"\n    Represents a connected Unix datagram socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n", "src/anyio/abc/_resources.py": "from __future__ import annotations\n\nfrom abc import ABCMeta, abstractmethod\nfrom types import TracebackType\nfrom typing import TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass AsyncResource(metaclass=ABCMeta):\n    \"\"\"\n    Abstract base class for all closeable asynchronous resources.\n\n    Works as an asynchronous context manager which returns the instance itself on enter,\n    and calls :meth:`aclose` on exit.\n    \"\"\"\n\n    __slots__ = ()\n\n    async def __aenter__(self: T) -> T:\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.aclose()\n\n    @abstractmethod\n    async def aclose(self) -> None:\n        \"\"\"Close the resource.\"\"\"\n", "src/anyio/abc/_eventloop.py": "from __future__ import annotations\n\nimport math\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom collections.abc import AsyncIterator, Awaitable, Mapping\nfrom os import PathLike\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind, socket\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ContextManager,\n    Sequence,\n    TypeVar,\n    overload,\n)\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from .._core._synchronization import CapacityLimiter, Event\n    from .._core._tasks import CancelScope\n    from .._core._testing import TaskInfo\n    from ..from_thread import BlockingPortal\n    from ._sockets import (\n        ConnectedUDPSocket,\n        ConnectedUNIXDatagramSocket,\n        IPSockAddrType,\n        SocketListener,\n        SocketStream,\n        UDPSocket,\n        UNIXDatagramSocket,\n        UNIXSocketStream,\n    )\n    from ._subprocesses import Process\n    from ._tasks import TaskGroup\n    from ._testing import TestRunner\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\nclass AsyncBackend(metaclass=ABCMeta):\n    @classmethod\n    @abstractmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        \"\"\"\n        Run the given coroutine function in an asynchronous event loop.\n\n        The current thread must not be already running an event loop.\n\n        :param func: a coroutine function\n        :param args: positional arguments to ``func``\n        :param kwargs: positional arguments to ``func``\n        :param options: keyword arguments to call the backend ``run()`` implementation\n            with\n        :return: the return value of the coroutine function\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def current_token(cls) -> object:\n        \"\"\"\n\n        :return:\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def current_time(cls) -> float:\n        \"\"\"\n        Return the current value of the event loop's internal clock.\n\n        :return: the clock value (seconds)\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        \"\"\"Return the exception class that is raised in a task if it's cancelled.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    async def checkpoint(cls) -> None:\n        \"\"\"\n        Check if the task has been cancelled, and allow rescheduling of other tasks.\n\n        This is effectively the same as running :meth:`checkpoint_if_cancelled` and then\n        :meth:`cancel_shielded_checkpoint`.\n        \"\"\"\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        \"\"\"\n        Check if the current task group has been cancelled.\n\n        This will check if the task has been cancelled, but will not allow other tasks\n        to be scheduled if not.\n\n        \"\"\"\n        if cls.current_effective_deadline() == -math.inf:\n            await cls.checkpoint()\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        \"\"\"\n        Allow the rescheduling of other tasks.\n\n        This will give other tasks the opportunity to run, but without checking if the\n        current task group has been cancelled, unlike with :meth:`checkpoint`.\n\n        \"\"\"\n        with cls.create_cancel_scope(shield=True):\n            await cls.sleep(0)\n\n    @classmethod\n    @abstractmethod\n    async def sleep(cls, delay: float) -> None:\n        \"\"\"\n        Pause the current task for the specified duration.\n\n        :param delay: the duration, in seconds\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def current_effective_deadline(cls) -> float:\n        \"\"\"\n        Return the nearest deadline among all the cancel scopes effective for the\n        current task.\n\n        :return:\n            - a clock value from the event loop's internal clock\n            - ``inf`` if there is no deadline in effect\n            - ``-inf`` if the current scope has been cancelled\n        :rtype: float\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def create_task_group(cls) -> TaskGroup:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_event(cls) -> Event:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> CapacityLimiter:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        abandon_on_cancel: bool = False,\n        limiter: CapacityLimiter | None = None,\n    ) -> T_Retval:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def check_cancelled(cls) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def run_sync_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_blocking_portal(cls) -> BlockingPortal:\n        pass\n\n    @classmethod\n    @overload\n    async def open_process(\n        cls,\n        command: str | bytes,\n        *,\n        shell: Literal[True],\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike[str] | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        pass\n\n    @classmethod\n    @overload\n    async def open_process(\n        cls,\n        command: Sequence[str | bytes],\n        *,\n        shell: Literal[False],\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike[str] | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike[str] | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[Process]) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> SocketStream:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def connect_unix(cls, path: str | bytes) -> UNIXSocketStream:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_tcp_listener(cls, sock: socket) -> SocketListener:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_unix_listener(cls, sock: socket) -> SocketListener:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def create_udp_socket(\n        cls,\n        family: AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        pass\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket, remote_path: None\n    ) -> UNIXDatagramSocket: ...\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket, remote_path: str | bytes\n    ) -> ConnectedUNIXDatagramSocket: ...\n\n    @classmethod\n    @abstractmethod\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket, remote_path: str | bytes | None\n    ) -> UNIXDatagramSocket | ConnectedUNIXDatagramSocket:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def wait_socket_readable(cls, sock: socket) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def wait_socket_writable(cls, sock: socket) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def get_current_task(cls) -> TaskInfo:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def get_running_tasks(cls) -> Sequence[TaskInfo]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        pass\n", "src/anyio/abc/_streams.py": "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom collections.abc import Callable\nfrom typing import Any, Generic, TypeVar, Union\n\nfrom .._core._exceptions import EndOfStream\nfrom .._core._typedattr import TypedAttributeProvider\nfrom ._resources import AsyncResource\nfrom ._tasks import TaskGroup\n\nT_Item = TypeVar(\"T_Item\")\nT_co = TypeVar(\"T_co\", covariant=True)\nT_contra = TypeVar(\"T_contra\", contravariant=True)\n\n\nclass UnreliableObjectReceiveStream(\n    Generic[T_co], AsyncResource, TypedAttributeProvider\n):\n    \"\"\"\n    An interface for receiving objects.\n\n    This interface makes no guarantees that the received messages arrive in the order in\n    which they were sent, or that no messages are missed.\n\n    Asynchronously iterating over objects of this type will yield objects matching the\n    given type parameter.\n    \"\"\"\n\n    def __aiter__(self) -> UnreliableObjectReceiveStream[T_co]:\n        return self\n\n    async def __anext__(self) -> T_co:\n        try:\n            return await self.receive()\n        except EndOfStream:\n            raise StopAsyncIteration\n\n    @abstractmethod\n    async def receive(self) -> T_co:\n        \"\"\"\n        Receive the next item.\n\n        :raises ~anyio.ClosedResourceError: if the receive stream has been explicitly\n            closed\n        :raises ~anyio.EndOfStream: if this stream has been closed from the other end\n        :raises ~anyio.BrokenResourceError: if this stream has been rendered unusable\n            due to external causes\n        \"\"\"\n\n\nclass UnreliableObjectSendStream(\n    Generic[T_contra], AsyncResource, TypedAttributeProvider\n):\n    \"\"\"\n    An interface for sending objects.\n\n    This interface makes no guarantees that the messages sent will reach the\n    recipient(s) in the same order in which they were sent, or at all.\n    \"\"\"\n\n    @abstractmethod\n    async def send(self, item: T_contra) -> None:\n        \"\"\"\n        Send an item to the peer(s).\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if the send stream has been explicitly\n            closed\n        :raises ~anyio.BrokenResourceError: if this stream has been rendered unusable\n            due to external causes\n        \"\"\"\n\n\nclass UnreliableObjectStream(\n    UnreliableObjectReceiveStream[T_Item], UnreliableObjectSendStream[T_Item]\n):\n    \"\"\"\n    A bidirectional message stream which does not guarantee the order or reliability of\n    message delivery.\n    \"\"\"\n\n\nclass ObjectReceiveStream(UnreliableObjectReceiveStream[T_co]):\n    \"\"\"\n    A receive message stream which guarantees that messages are received in the same\n    order in which they were sent, and that no messages are missed.\n    \"\"\"\n\n\nclass ObjectSendStream(UnreliableObjectSendStream[T_contra]):\n    \"\"\"\n    A send message stream which guarantees that messages are delivered in the same order\n    in which they were sent, without missing any messages in the middle.\n    \"\"\"\n\n\nclass ObjectStream(\n    ObjectReceiveStream[T_Item],\n    ObjectSendStream[T_Item],\n    UnreliableObjectStream[T_Item],\n):\n    \"\"\"\n    A bidirectional message stream which guarantees the order and reliability of message\n    delivery.\n    \"\"\"\n\n    @abstractmethod\n    async def send_eof(self) -> None:\n        \"\"\"\n        Send an end-of-file indication to the peer.\n\n        You should not try to send any further data to this stream after calling this\n        method. This method is idempotent (does nothing on successive calls).\n        \"\"\"\n\n\nclass ByteReceiveStream(AsyncResource, TypedAttributeProvider):\n    \"\"\"\n    An interface for receiving bytes from a single peer.\n\n    Iterating this byte stream will yield a byte string of arbitrary length, but no more\n    than 65536 bytes.\n    \"\"\"\n\n    def __aiter__(self) -> ByteReceiveStream:\n        return self\n\n    async def __anext__(self) -> bytes:\n        try:\n            return await self.receive()\n        except EndOfStream:\n            raise StopAsyncIteration\n\n    @abstractmethod\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        \"\"\"\n        Receive at most ``max_bytes`` bytes from the peer.\n\n        .. note:: Implementors of this interface should not return an empty\n            :class:`bytes` object, and users should ignore them.\n\n        :param max_bytes: maximum number of bytes to receive\n        :return: the received bytes\n        :raises ~anyio.EndOfStream: if this stream has been closed from the other end\n        \"\"\"\n\n\nclass ByteSendStream(AsyncResource, TypedAttributeProvider):\n    \"\"\"An interface for sending bytes to a single peer.\"\"\"\n\n    @abstractmethod\n    async def send(self, item: bytes) -> None:\n        \"\"\"\n        Send the given bytes to the peer.\n\n        :param item: the bytes to send\n        \"\"\"\n\n\nclass ByteStream(ByteReceiveStream, ByteSendStream):\n    \"\"\"A bidirectional byte stream.\"\"\"\n\n    @abstractmethod\n    async def send_eof(self) -> None:\n        \"\"\"\n        Send an end-of-file indication to the peer.\n\n        You should not try to send any further data to this stream after calling this\n        method. This method is idempotent (does nothing on successive calls).\n        \"\"\"\n\n\n#: Type alias for all unreliable bytes-oriented receive streams.\nAnyUnreliableByteReceiveStream = Union[\n    UnreliableObjectReceiveStream[bytes], ByteReceiveStream\n]\n#: Type alias for all unreliable bytes-oriented send streams.\nAnyUnreliableByteSendStream = Union[UnreliableObjectSendStream[bytes], ByteSendStream]\n#: Type alias for all unreliable bytes-oriented streams.\nAnyUnreliableByteStream = Union[UnreliableObjectStream[bytes], ByteStream]\n#: Type alias for all bytes-oriented receive streams.\nAnyByteReceiveStream = Union[ObjectReceiveStream[bytes], ByteReceiveStream]\n#: Type alias for all bytes-oriented send streams.\nAnyByteSendStream = Union[ObjectSendStream[bytes], ByteSendStream]\n#: Type alias for all bytes-oriented streams.\nAnyByteStream = Union[ObjectStream[bytes], ByteStream]\n\n\nclass Listener(Generic[T_co], AsyncResource, TypedAttributeProvider):\n    \"\"\"An interface for objects that let you accept incoming connections.\"\"\"\n\n    @abstractmethod\n    async def serve(\n        self, handler: Callable[[T_co], Any], task_group: TaskGroup | None = None\n    ) -> None:\n        \"\"\"\n        Accept incoming connections as they come in and start tasks to handle them.\n\n        :param handler: a callable that will be used to handle each accepted connection\n        :param task_group: the task group that will be used to start tasks for handling\n            each accepted connection (if omitted, an ad-hoc task group will be created)\n        \"\"\"\n", "src/anyio/abc/_subprocesses.py": "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom signal import Signals\n\nfrom ._resources import AsyncResource\nfrom ._streams import ByteReceiveStream, ByteSendStream\n\n\nclass Process(AsyncResource):\n    \"\"\"An asynchronous version of :class:`subprocess.Popen`.\"\"\"\n\n    @abstractmethod\n    async def wait(self) -> int:\n        \"\"\"\n        Wait until the process exits.\n\n        :return: the exit code of the process\n        \"\"\"\n\n    @abstractmethod\n    def terminate(self) -> None:\n        \"\"\"\n        Terminates the process, gracefully if possible.\n\n        On Windows, this calls ``TerminateProcess()``.\n        On POSIX systems, this sends ``SIGTERM`` to the process.\n\n        .. seealso:: :meth:`subprocess.Popen.terminate`\n        \"\"\"\n\n    @abstractmethod\n    def kill(self) -> None:\n        \"\"\"\n        Kills the process.\n\n        On Windows, this calls ``TerminateProcess()``.\n        On POSIX systems, this sends ``SIGKILL`` to the process.\n\n        .. seealso:: :meth:`subprocess.Popen.kill`\n        \"\"\"\n\n    @abstractmethod\n    def send_signal(self, signal: Signals) -> None:\n        \"\"\"\n        Send a signal to the subprocess.\n\n        .. seealso:: :meth:`subprocess.Popen.send_signal`\n\n        :param signal: the signal number (e.g. :data:`signal.SIGHUP`)\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def pid(self) -> int:\n        \"\"\"The process ID of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def returncode(self) -> int | None:\n        \"\"\"\n        The return code of the process. If the process has not yet terminated, this will\n        be ``None``.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def stdin(self) -> ByteSendStream | None:\n        \"\"\"The stream for the standard input of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def stdout(self) -> ByteReceiveStream | None:\n        \"\"\"The stream for the standard output of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def stderr(self) -> ByteReceiveStream | None:\n        \"\"\"The stream for the standard error output of the process.\"\"\"\n", "src/anyio/abc/__init__.py": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom ._eventloop import AsyncBackend as AsyncBackend\nfrom ._resources import AsyncResource as AsyncResource\nfrom ._sockets import ConnectedUDPSocket as ConnectedUDPSocket\nfrom ._sockets import ConnectedUNIXDatagramSocket as ConnectedUNIXDatagramSocket\nfrom ._sockets import IPAddressType as IPAddressType\nfrom ._sockets import IPSockAddrType as IPSockAddrType\nfrom ._sockets import SocketAttribute as SocketAttribute\nfrom ._sockets import SocketListener as SocketListener\nfrom ._sockets import SocketStream as SocketStream\nfrom ._sockets import UDPPacketType as UDPPacketType\nfrom ._sockets import UDPSocket as UDPSocket\nfrom ._sockets import UNIXDatagramPacketType as UNIXDatagramPacketType\nfrom ._sockets import UNIXDatagramSocket as UNIXDatagramSocket\nfrom ._sockets import UNIXSocketStream as UNIXSocketStream\nfrom ._streams import AnyByteReceiveStream as AnyByteReceiveStream\nfrom ._streams import AnyByteSendStream as AnyByteSendStream\nfrom ._streams import AnyByteStream as AnyByteStream\nfrom ._streams import AnyUnreliableByteReceiveStream as AnyUnreliableByteReceiveStream\nfrom ._streams import AnyUnreliableByteSendStream as AnyUnreliableByteSendStream\nfrom ._streams import AnyUnreliableByteStream as AnyUnreliableByteStream\nfrom ._streams import ByteReceiveStream as ByteReceiveStream\nfrom ._streams import ByteSendStream as ByteSendStream\nfrom ._streams import ByteStream as ByteStream\nfrom ._streams import Listener as Listener\nfrom ._streams import ObjectReceiveStream as ObjectReceiveStream\nfrom ._streams import ObjectSendStream as ObjectSendStream\nfrom ._streams import ObjectStream as ObjectStream\nfrom ._streams import UnreliableObjectReceiveStream as UnreliableObjectReceiveStream\nfrom ._streams import UnreliableObjectSendStream as UnreliableObjectSendStream\nfrom ._streams import UnreliableObjectStream as UnreliableObjectStream\nfrom ._subprocesses import Process as Process\nfrom ._tasks import TaskGroup as TaskGroup\nfrom ._tasks import TaskStatus as TaskStatus\nfrom ._testing import TestRunner as TestRunner\n\n# Re-exported here, for backwards compatibility\n# isort: off\nfrom .._core._synchronization import (\n    CapacityLimiter as CapacityLimiter,\n    Condition as Condition,\n    Event as Event,\n    Lock as Lock,\n    Semaphore as Semaphore,\n)\nfrom .._core._tasks import CancelScope as CancelScope\nfrom ..from_thread import BlockingPortal as BlockingPortal\n\n# Re-export imports so they look like they live directly in this package\nkey: str\nvalue: Any\nfor key, value in list(locals().items()):\n    if getattr(value, \"__module__\", \"\").startswith(\"anyio.abc.\"):\n        value.__module__ = __name__\n", "src/anyio/_core/_tasks.py": "from __future__ import annotations\n\nimport math\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom types import TracebackType\n\nfrom ..abc._tasks import TaskGroup, TaskStatus\nfrom ._eventloop import get_async_backend\n\n\nclass _IgnoredTaskStatus(TaskStatus[object]):\n    def started(self, value: object = None) -> None:\n        pass\n\n\nTASK_STATUS_IGNORED = _IgnoredTaskStatus()\n\n\nclass CancelScope:\n    \"\"\"\n    Wraps a unit of work that can be made separately cancellable.\n\n    :param deadline: The time (clock value) when this scope is cancelled automatically\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    \"\"\"\n\n    def __new__(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return get_async_backend().create_cancel_scope(shield=shield, deadline=deadline)\n\n    def cancel(self) -> None:\n        \"\"\"Cancel this scope immediately.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def deadline(self) -> float:\n        \"\"\"\n        The time (clock value) when this scope is cancelled automatically.\n\n        Will be ``float('inf')`` if no timeout has been set.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        raise NotImplementedError\n\n    @property\n    def cancel_called(self) -> bool:\n        \"\"\"``True`` if :meth:`cancel` has been called.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def cancelled_caught(self) -> bool:\n        \"\"\"\n        ``True`` if this scope suppressed a cancellation exception it itself raised.\n\n        This is typically used to check if any work was interrupted, or to see if the\n        scope was cancelled due to its deadline being reached. The value will, however,\n        only be ``True`` if the cancellation was triggered by the scope itself (and not\n        an outer scope).\n\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def shield(self) -> bool:\n        \"\"\"\n        ``True`` if this scope is shielded from external cancellation.\n\n        While a scope is shielded, it will not receive cancellations from outside.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        raise NotImplementedError\n\n    def __enter__(self) -> CancelScope:\n        raise NotImplementedError\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        raise NotImplementedError\n\n\n@contextmanager\ndef fail_after(\n    delay: float | None, shield: bool = False\n) -> Generator[CancelScope, None, None]:\n    \"\"\"\n    Create a context manager which raises a :class:`TimeoutError` if does not finish in\n    time.\n\n    :param delay: maximum allowed time (in seconds) before raising the exception, or\n        ``None`` to disable the timeout\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a context manager that yields a cancel scope\n    :rtype: :class:`~typing.ContextManager`\\\\[:class:`~anyio.CancelScope`\\\\]\n\n    \"\"\"\n    current_time = get_async_backend().current_time\n    deadline = (current_time() + delay) if delay is not None else math.inf\n    with get_async_backend().create_cancel_scope(\n        deadline=deadline, shield=shield\n    ) as cancel_scope:\n        yield cancel_scope\n\n    if cancel_scope.cancelled_caught and current_time() >= cancel_scope.deadline:\n        raise TimeoutError\n\n\ndef move_on_after(delay: float | None, shield: bool = False) -> CancelScope:\n    \"\"\"\n    Create a cancel scope with a deadline that expires after the given delay.\n\n    :param delay: maximum allowed time (in seconds) before exiting the context block, or\n        ``None`` to disable the timeout\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a cancel scope\n\n    \"\"\"\n    deadline = (\n        (get_async_backend().current_time() + delay) if delay is not None else math.inf\n    )\n    return get_async_backend().create_cancel_scope(deadline=deadline, shield=shield)\n\n\ndef current_effective_deadline() -> float:\n    \"\"\"\n    Return the nearest deadline among all the cancel scopes effective for the current\n    task.\n\n    :return: a clock value from the event loop's internal clock (or ``float('inf')`` if\n        there is no deadline in effect, or ``float('-inf')`` if the current scope has\n        been cancelled)\n    :rtype: float\n\n    \"\"\"\n    return get_async_backend().current_effective_deadline()\n\n\ndef create_task_group() -> TaskGroup:\n    \"\"\"\n    Create a task group.\n\n    :return: a task group\n\n    \"\"\"\n    return get_async_backend().create_task_group()\n", "src/anyio/_core/_testing.py": "from __future__ import annotations\n\nfrom collections.abc import Awaitable, Generator\nfrom typing import Any, cast\n\nfrom ._eventloop import get_async_backend\n\n\nclass TaskInfo:\n    \"\"\"\n    Represents an asynchronous task.\n\n    :ivar int id: the unique identifier of the task\n    :ivar parent_id: the identifier of the parent task, if any\n    :vartype parent_id: Optional[int]\n    :ivar str name: the description of the task (if any)\n    :ivar ~collections.abc.Coroutine coro: the coroutine object of the task\n    \"\"\"\n\n    __slots__ = \"_name\", \"id\", \"parent_id\", \"name\", \"coro\"\n\n    def __init__(\n        self,\n        id: int,\n        parent_id: int | None,\n        name: str | None,\n        coro: Generator[Any, Any, Any] | Awaitable[Any],\n    ):\n        func = get_current_task\n        self._name = f\"{func.__module__}.{func.__qualname__}\"\n        self.id: int = id\n        self.parent_id: int | None = parent_id\n        self.name: str | None = name\n        self.coro: Generator[Any, Any, Any] | Awaitable[Any] = coro\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, TaskInfo):\n            return self.id == other.id\n\n        return NotImplemented\n\n    def __hash__(self) -> int:\n        return hash(self.id)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(id={self.id!r}, name={self.name!r})\"\n\n    def has_pending_cancellation(self) -> bool:\n        \"\"\"\n        Return ``True`` if the task has a cancellation pending, ``False`` otherwise.\n\n        \"\"\"\n        return False\n\n\ndef get_current_task() -> TaskInfo:\n    \"\"\"\n    Return the current task.\n\n    :return: a representation of the current task\n\n    \"\"\"\n    return get_async_backend().get_current_task()\n\n\ndef get_running_tasks() -> list[TaskInfo]:\n    \"\"\"\n    Return a list of running tasks in the current event loop.\n\n    :return: a list of task info objects\n\n    \"\"\"\n    return cast(\"list[TaskInfo]\", get_async_backend().get_running_tasks())\n\n\nasync def wait_all_tasks_blocked() -> None:\n    \"\"\"Wait until all other tasks are waiting for something.\"\"\"\n    await get_async_backend().wait_all_tasks_blocked()\n", "src/anyio/_core/_sockets.py": "from __future__ import annotations\n\nimport errno\nimport os\nimport socket\nimport ssl\nimport stat\nimport sys\nfrom collections.abc import Awaitable\nfrom ipaddress import IPv6Address, ip_address\nfrom os import PathLike, chmod\nfrom socket import AddressFamily, SocketKind\nfrom typing import Any, Literal, cast, overload\n\nfrom .. import to_thread\nfrom ..abc import (\n    ConnectedUDPSocket,\n    ConnectedUNIXDatagramSocket,\n    IPAddressType,\n    IPSockAddrType,\n    SocketListener,\n    SocketStream,\n    UDPSocket,\n    UNIXDatagramSocket,\n    UNIXSocketStream,\n)\nfrom ..streams.stapled import MultiListener\nfrom ..streams.tls import TLSStream\nfrom ._eventloop import get_async_backend\nfrom ._resources import aclose_forcefully\nfrom ._synchronization import Event\nfrom ._tasks import create_task_group, move_on_after\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import ExceptionGroup\n\nIPPROTO_IPV6 = getattr(socket, \"IPPROTO_IPV6\", 41)  # https://bugs.python.org/issue29515\n\nAnyIPAddressFamily = Literal[\n    AddressFamily.AF_UNSPEC, AddressFamily.AF_INET, AddressFamily.AF_INET6\n]\nIPAddressFamily = Literal[AddressFamily.AF_INET, AddressFamily.AF_INET6]\n\n\n# tls_hostname given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# ssl_context given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    ssl_context: ssl.SSLContext,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# tls=True\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    tls: Literal[True],\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# tls=False\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    tls: Literal[False],\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> SocketStream: ...\n\n\n# No TLS arguments\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> SocketStream: ...\n\n\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = None,\n    tls: bool = False,\n    ssl_context: ssl.SSLContext | None = None,\n    tls_standard_compatible: bool = True,\n    tls_hostname: str | None = None,\n    happy_eyeballs_delay: float = 0.25,\n) -> SocketStream | TLSStream:\n    \"\"\"\n    Connect to a host using the TCP protocol.\n\n    This function implements the stateless version of the Happy Eyeballs algorithm (RFC\n    6555). If ``remote_host`` is a host name that resolves to multiple IP addresses,\n    each one is tried until one connection attempt succeeds. If the first attempt does\n    not connected within 250 milliseconds, a second attempt is started using the next\n    address in the list, and so on. On IPv6 enabled systems, an IPv6 address (if\n    available) is tried first.\n\n    When the connection has been established, a TLS handshake will be done if either\n    ``ssl_context`` or ``tls_hostname`` is not ``None``, or if ``tls`` is ``True``.\n\n    :param remote_host: the IP address or host name to connect to\n    :param remote_port: port on the target host to connect to\n    :param local_host: the interface address or name to bind the socket to before\n        connecting\n    :param tls: ``True`` to do a TLS handshake with the connected stream and return a\n        :class:`~anyio.streams.tls.TLSStream` instead\n    :param ssl_context: the SSL context object to use (if omitted, a default context is\n        created)\n    :param tls_standard_compatible: If ``True``, performs the TLS shutdown handshake\n        before closing the stream and requires that the server does this as well.\n        Otherwise, :exc:`~ssl.SSLEOFError` may be raised during reads from the stream.\n        Some protocols, such as HTTP, require this option to be ``False``.\n        See :meth:`~ssl.SSLContext.wrap_socket` for details.\n    :param tls_hostname: host name to check the server certificate against (defaults to\n        the value of ``remote_host``)\n    :param happy_eyeballs_delay: delay (in seconds) before starting the next connection\n        attempt\n    :return: a socket stream object if no TLS handshake was done, otherwise a TLS stream\n    :raises OSError: if the connection attempt fails\n\n    \"\"\"\n    # Placed here due to https://github.com/python/mypy/issues/7057\n    connected_stream: SocketStream | None = None\n\n    async def try_connect(remote_host: str, event: Event) -> None:\n        nonlocal connected_stream\n        try:\n            stream = await asynclib.connect_tcp(remote_host, remote_port, local_address)\n        except OSError as exc:\n            oserrors.append(exc)\n            return\n        else:\n            if connected_stream is None:\n                connected_stream = stream\n                tg.cancel_scope.cancel()\n            else:\n                await stream.aclose()\n        finally:\n            event.set()\n\n    asynclib = get_async_backend()\n    local_address: IPSockAddrType | None = None\n    family = socket.AF_UNSPEC\n    if local_host:\n        gai_res = await getaddrinfo(str(local_host), None)\n        family, *_, local_address = gai_res[0]\n\n    target_host = str(remote_host)\n    try:\n        addr_obj = ip_address(remote_host)\n    except ValueError:\n        # getaddrinfo() will raise an exception if name resolution fails\n        gai_res = await getaddrinfo(\n            target_host, remote_port, family=family, type=socket.SOCK_STREAM\n        )\n\n        # Organize the list so that the first address is an IPv6 address (if available)\n        # and the second one is an IPv4 addresses. The rest can be in whatever order.\n        v6_found = v4_found = False\n        target_addrs: list[tuple[socket.AddressFamily, str]] = []\n        for af, *rest, sa in gai_res:\n            if af == socket.AF_INET6 and not v6_found:\n                v6_found = True\n                target_addrs.insert(0, (af, sa[0]))\n            elif af == socket.AF_INET and not v4_found and v6_found:\n                v4_found = True\n                target_addrs.insert(1, (af, sa[0]))\n            else:\n                target_addrs.append((af, sa[0]))\n    else:\n        if isinstance(addr_obj, IPv6Address):\n            target_addrs = [(socket.AF_INET6, addr_obj.compressed)]\n        else:\n            target_addrs = [(socket.AF_INET, addr_obj.compressed)]\n\n    oserrors: list[OSError] = []\n    async with create_task_group() as tg:\n        for i, (af, addr) in enumerate(target_addrs):\n            event = Event()\n            tg.start_soon(try_connect, addr, event)\n            with move_on_after(happy_eyeballs_delay):\n                await event.wait()\n\n    if connected_stream is None:\n        cause = (\n            oserrors[0]\n            if len(oserrors) == 1\n            else ExceptionGroup(\"multiple connection attempts failed\", oserrors)\n        )\n        raise OSError(\"All connection attempts failed\") from cause\n\n    if tls or tls_hostname or ssl_context:\n        try:\n            return await TLSStream.wrap(\n                connected_stream,\n                server_side=False,\n                hostname=tls_hostname or str(remote_host),\n                ssl_context=ssl_context,\n                standard_compatible=tls_standard_compatible,\n            )\n        except BaseException:\n            await aclose_forcefully(connected_stream)\n            raise\n\n    return connected_stream\n\n\nasync def connect_unix(path: str | bytes | PathLike[Any]) -> UNIXSocketStream:\n    \"\"\"\n    Connect to the given UNIX socket.\n\n    Not available on Windows.\n\n    :param path: path to the socket\n    :return: a socket stream object\n\n    \"\"\"\n    path = os.fspath(path)\n    return await get_async_backend().connect_unix(path)\n\n\nasync def create_tcp_listener(\n    *,\n    local_host: IPAddressType | None = None,\n    local_port: int = 0,\n    family: AnyIPAddressFamily = socket.AddressFamily.AF_UNSPEC,\n    backlog: int = 65536,\n    reuse_port: bool = False,\n) -> MultiListener[SocketStream]:\n    \"\"\"\n    Create a TCP socket listener.\n\n    :param local_port: port number to listen on\n    :param local_host: IP address of the interface to listen on. If omitted, listen on\n        all IPv4 and IPv6 interfaces. To listen on all interfaces on a specific address\n        family, use ``0.0.0.0`` for IPv4 or ``::`` for IPv6.\n    :param family: address family (used if ``local_host`` was omitted)\n    :param backlog: maximum number of queued incoming connections (up to a maximum of\n        2**16, or 65536)\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same\n        address/port (not supported on Windows)\n    :return: a list of listener objects\n\n    \"\"\"\n    asynclib = get_async_backend()\n    backlog = min(backlog, 65536)\n    local_host = str(local_host) if local_host is not None else None\n    gai_res = await getaddrinfo(\n        local_host,\n        local_port,\n        family=family,\n        type=socket.SocketKind.SOCK_STREAM if sys.platform == \"win32\" else 0,\n        flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG,\n    )\n    listeners: list[SocketListener] = []\n    try:\n        # The set() is here to work around a glibc bug:\n        # https://sourceware.org/bugzilla/show_bug.cgi?id=14969\n        sockaddr: tuple[str, int] | tuple[str, int, int, int]\n        for fam, kind, *_, sockaddr in sorted(set(gai_res)):\n            # Workaround for an uvloop bug where we don't get the correct scope ID for\n            # IPv6 link-local addresses when passing type=socket.SOCK_STREAM to\n            # getaddrinfo(): https://github.com/MagicStack/uvloop/issues/539\n            if sys.platform != \"win32\" and kind is not SocketKind.SOCK_STREAM:\n                continue\n\n            raw_socket = socket.socket(fam)\n            raw_socket.setblocking(False)\n\n            # For Windows, enable exclusive address use. For others, enable address\n            # reuse.\n            if sys.platform == \"win32\":\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_EXCLUSIVEADDRUSE, 1)\n            else:\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n            if reuse_port:\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n\n            # If only IPv6 was requested, disable dual stack operation\n            if fam == socket.AF_INET6:\n                raw_socket.setsockopt(IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)\n\n                # Workaround for #554\n                if \"%\" in sockaddr[0]:\n                    addr, scope_id = sockaddr[0].split(\"%\", 1)\n                    sockaddr = (addr, sockaddr[1], 0, int(scope_id))\n\n            raw_socket.bind(sockaddr)\n            raw_socket.listen(backlog)\n            listener = asynclib.create_tcp_listener(raw_socket)\n            listeners.append(listener)\n    except BaseException:\n        for listener in listeners:\n            await listener.aclose()\n\n        raise\n\n    return MultiListener(listeners)\n\n\nasync def create_unix_listener(\n    path: str | bytes | PathLike[Any],\n    *,\n    mode: int | None = None,\n    backlog: int = 65536,\n) -> SocketListener:\n    \"\"\"\n    Create a UNIX socket listener.\n\n    Not available on Windows.\n\n    :param path: path of the socket\n    :param mode: permissions to set on the socket\n    :param backlog: maximum number of queued incoming connections (up to a maximum of\n        2**16, or 65536)\n    :return: a listener object\n\n    .. versionchanged:: 3.0\n        If a socket already exists on the file system in the given path, it will be\n        removed first.\n\n    \"\"\"\n    backlog = min(backlog, 65536)\n    raw_socket = await setup_unix_local_socket(path, mode, socket.SOCK_STREAM)\n    try:\n        raw_socket.listen(backlog)\n        return get_async_backend().create_unix_listener(raw_socket)\n    except BaseException:\n        raw_socket.close()\n        raise\n\n\nasync def create_udp_socket(\n    family: AnyIPAddressFamily = AddressFamily.AF_UNSPEC,\n    *,\n    local_host: IPAddressType | None = None,\n    local_port: int = 0,\n    reuse_port: bool = False,\n) -> UDPSocket:\n    \"\"\"\n    Create a UDP socket.\n\n    If ``port`` has been given, the socket will be bound to this port on the local\n    machine, making this socket suitable for providing UDP based services.\n\n    :param family: address family (``AF_INET`` or ``AF_INET6``) \u2013 automatically\n        determined from ``local_host`` if omitted\n    :param local_host: IP address or host name of the local interface to bind to\n    :param local_port: local port to bind to\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same\n        address/port (not supported on Windows)\n    :return: a UDP socket\n\n    \"\"\"\n    if family is AddressFamily.AF_UNSPEC and not local_host:\n        raise ValueError('Either \"family\" or \"local_host\" must be given')\n\n    if local_host:\n        gai_res = await getaddrinfo(\n            str(local_host),\n            local_port,\n            family=family,\n            type=socket.SOCK_DGRAM,\n            flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG,\n        )\n        family = cast(AnyIPAddressFamily, gai_res[0][0])\n        local_address = gai_res[0][-1]\n    elif family is AddressFamily.AF_INET6:\n        local_address = (\"::\", 0)\n    else:\n        local_address = (\"0.0.0.0\", 0)\n\n    sock = await get_async_backend().create_udp_socket(\n        family, local_address, None, reuse_port\n    )\n    return cast(UDPSocket, sock)\n\n\nasync def create_connected_udp_socket(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    family: AnyIPAddressFamily = AddressFamily.AF_UNSPEC,\n    local_host: IPAddressType | None = None,\n    local_port: int = 0,\n    reuse_port: bool = False,\n) -> ConnectedUDPSocket:\n    \"\"\"\n    Create a connected UDP socket.\n\n    Connected UDP sockets can only communicate with the specified remote host/port, an\n    any packets sent from other sources are dropped.\n\n    :param remote_host: remote host to set as the default target\n    :param remote_port: port on the remote host to set as the default target\n    :param family: address family (``AF_INET`` or ``AF_INET6``) \u2013 automatically\n        determined from ``local_host`` or ``remote_host`` if omitted\n    :param local_host: IP address or host name of the local interface to bind to\n    :param local_port: local port to bind to\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same\n        address/port (not supported on Windows)\n    :return: a connected UDP socket\n\n    \"\"\"\n    local_address = None\n    if local_host:\n        gai_res = await getaddrinfo(\n            str(local_host),\n            local_port,\n            family=family,\n            type=socket.SOCK_DGRAM,\n            flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG,\n        )\n        family = cast(AnyIPAddressFamily, gai_res[0][0])\n        local_address = gai_res[0][-1]\n\n    gai_res = await getaddrinfo(\n        str(remote_host), remote_port, family=family, type=socket.SOCK_DGRAM\n    )\n    family = cast(AnyIPAddressFamily, gai_res[0][0])\n    remote_address = gai_res[0][-1]\n\n    sock = await get_async_backend().create_udp_socket(\n        family, local_address, remote_address, reuse_port\n    )\n    return cast(ConnectedUDPSocket, sock)\n\n\nasync def create_unix_datagram_socket(\n    *,\n    local_path: None | str | bytes | PathLike[Any] = None,\n    local_mode: int | None = None,\n) -> UNIXDatagramSocket:\n    \"\"\"\n    Create a UNIX datagram socket.\n\n    Not available on Windows.\n\n    If ``local_path`` has been given, the socket will be bound to this path, making this\n    socket suitable for receiving datagrams from other processes. Other processes can\n    send datagrams to this socket only if ``local_path`` is set.\n\n    If a socket already exists on the file system in the ``local_path``, it will be\n    removed first.\n\n    :param local_path: the path on which to bind to\n    :param local_mode: permissions to set on the local socket\n    :return: a UNIX datagram socket\n\n    \"\"\"\n    raw_socket = await setup_unix_local_socket(\n        local_path, local_mode, socket.SOCK_DGRAM\n    )\n    return await get_async_backend().create_unix_datagram_socket(raw_socket, None)\n\n\nasync def create_connected_unix_datagram_socket(\n    remote_path: str | bytes | PathLike[Any],\n    *,\n    local_path: None | str | bytes | PathLike[Any] = None,\n    local_mode: int | None = None,\n) -> ConnectedUNIXDatagramSocket:\n    \"\"\"\n    Create a connected UNIX datagram socket.\n\n    Connected datagram sockets can only communicate with the specified remote path.\n\n    If ``local_path`` has been given, the socket will be bound to this path, making\n    this socket suitable for receiving datagrams from other processes. Other processes\n    can send datagrams to this socket only if ``local_path`` is set.\n\n    If a socket already exists on the file system in the ``local_path``, it will be\n    removed first.\n\n    :param remote_path: the path to set as the default target\n    :param local_path: the path on which to bind to\n    :param local_mode: permissions to set on the local socket\n    :return: a connected UNIX datagram socket\n\n    \"\"\"\n    remote_path = os.fspath(remote_path)\n    raw_socket = await setup_unix_local_socket(\n        local_path, local_mode, socket.SOCK_DGRAM\n    )\n    return await get_async_backend().create_unix_datagram_socket(\n        raw_socket, remote_path\n    )\n\n\nasync def getaddrinfo(\n    host: bytes | str | None,\n    port: str | int | None,\n    *,\n    family: int | AddressFamily = 0,\n    type: int | SocketKind = 0,\n    proto: int = 0,\n    flags: int = 0,\n) -> list[tuple[AddressFamily, SocketKind, int, str, tuple[str, int]]]:\n    \"\"\"\n    Look up a numeric IP address given a host name.\n\n    Internationalized domain names are translated according to the (non-transitional)\n    IDNA 2008 standard.\n\n    .. note:: 4-tuple IPv6 socket addresses are automatically converted to 2-tuples of\n        (host, port), unlike what :func:`socket.getaddrinfo` does.\n\n    :param host: host name\n    :param port: port number\n    :param family: socket family (`'AF_INET``, ...)\n    :param type: socket type (``SOCK_STREAM``, ...)\n    :param proto: protocol number\n    :param flags: flags to pass to upstream ``getaddrinfo()``\n    :return: list of tuples containing (family, type, proto, canonname, sockaddr)\n\n    .. seealso:: :func:`socket.getaddrinfo`\n\n    \"\"\"\n    # Handle unicode hostnames\n    if isinstance(host, str):\n        try:\n            encoded_host: bytes | None = host.encode(\"ascii\")\n        except UnicodeEncodeError:\n            import idna\n\n            encoded_host = idna.encode(host, uts46=True)\n    else:\n        encoded_host = host\n\n    gai_res = await get_async_backend().getaddrinfo(\n        encoded_host, port, family=family, type=type, proto=proto, flags=flags\n    )\n    return [\n        (family, type, proto, canonname, convert_ipv6_sockaddr(sockaddr))\n        for family, type, proto, canonname, sockaddr in gai_res\n    ]\n\n\ndef getnameinfo(sockaddr: IPSockAddrType, flags: int = 0) -> Awaitable[tuple[str, str]]:\n    \"\"\"\n    Look up the host name of an IP address.\n\n    :param sockaddr: socket address (e.g. (ipaddress, port) for IPv4)\n    :param flags: flags to pass to upstream ``getnameinfo()``\n    :return: a tuple of (host name, service name)\n\n    .. seealso:: :func:`socket.getnameinfo`\n\n    \"\"\"\n    return get_async_backend().getnameinfo(sockaddr, flags)\n\n\ndef wait_socket_readable(sock: socket.socket) -> Awaitable[None]:\n    \"\"\"\n    Wait until the given socket has data to be read.\n\n    This does **NOT** work on Windows when using the asyncio backend with a proactor\n    event loop (default on py3.8+).\n\n    .. warning:: Only use this on raw sockets that have not been wrapped by any higher\n        level constructs like socket streams!\n\n    :param sock: a socket object\n    :raises ~anyio.ClosedResourceError: if the socket was closed while waiting for the\n        socket to become readable\n    :raises ~anyio.BusyResourceError: if another task is already waiting for the socket\n        to become readable\n\n    \"\"\"\n    return get_async_backend().wait_socket_readable(sock)\n\n\ndef wait_socket_writable(sock: socket.socket) -> Awaitable[None]:\n    \"\"\"\n    Wait until the given socket can be written to.\n\n    This does **NOT** work on Windows when using the asyncio backend with a proactor\n    event loop (default on py3.8+).\n\n    .. warning:: Only use this on raw sockets that have not been wrapped by any higher\n        level constructs like socket streams!\n\n    :param sock: a socket object\n    :raises ~anyio.ClosedResourceError: if the socket was closed while waiting for the\n        socket to become writable\n    :raises ~anyio.BusyResourceError: if another task is already waiting for the socket\n        to become writable\n\n    \"\"\"\n    return get_async_backend().wait_socket_writable(sock)\n\n\n#\n# Private API\n#\n\n\ndef convert_ipv6_sockaddr(\n    sockaddr: tuple[str, int, int, int] | tuple[str, int],\n) -> tuple[str, int]:\n    \"\"\"\n    Convert a 4-tuple IPv6 socket address to a 2-tuple (address, port) format.\n\n    If the scope ID is nonzero, it is added to the address, separated with ``%``.\n    Otherwise the flow id and scope id are simply cut off from the tuple.\n    Any other kinds of socket addresses are returned as-is.\n\n    :param sockaddr: the result of :meth:`~socket.socket.getsockname`\n    :return: the converted socket address\n\n    \"\"\"\n    # This is more complicated than it should be because of MyPy\n    if isinstance(sockaddr, tuple) and len(sockaddr) == 4:\n        host, port, flowinfo, scope_id = sockaddr\n        if scope_id:\n            # PyPy (as of v7.3.11) leaves the interface name in the result, so\n            # we discard it and only get the scope ID from the end\n            # (https://foss.heptapod.net/pypy/pypy/-/issues/3938)\n            host = host.split(\"%\")[0]\n\n            # Add scope_id to the address\n            return f\"{host}%{scope_id}\", port\n        else:\n            return host, port\n    else:\n        return sockaddr\n\n\nasync def setup_unix_local_socket(\n    path: None | str | bytes | PathLike[Any],\n    mode: int | None,\n    socktype: int,\n) -> socket.socket:\n    \"\"\"\n    Create a UNIX local socket object, deleting the socket at the given path if it\n    exists.\n\n    Not available on Windows.\n\n    :param path: path of the socket\n    :param mode: permissions to set on the socket\n    :param socktype: socket.SOCK_STREAM or socket.SOCK_DGRAM\n\n    \"\"\"\n    path_str: str | bytes | None\n    if path is not None:\n        path_str = os.fspath(path)\n\n        # Copied from pathlib...\n        try:\n            stat_result = os.stat(path)\n        except OSError as e:\n            if e.errno not in (errno.ENOENT, errno.ENOTDIR, errno.EBADF, errno.ELOOP):\n                raise\n        else:\n            if stat.S_ISSOCK(stat_result.st_mode):\n                os.unlink(path)\n    else:\n        path_str = None\n\n    raw_socket = socket.socket(socket.AF_UNIX, socktype)\n    raw_socket.setblocking(False)\n\n    if path_str is not None:\n        try:\n            await to_thread.run_sync(raw_socket.bind, path_str, abandon_on_cancel=True)\n            if mode is not None:\n                await to_thread.run_sync(chmod, path_str, mode, abandon_on_cancel=True)\n        except BaseException:\n            raw_socket.close()\n            raise\n\n    return raw_socket\n", "src/anyio/_core/_exceptions.py": "from __future__ import annotations\n\n\nclass BrokenResourceError(Exception):\n    \"\"\"\n    Raised when trying to use a resource that has been rendered unusable due to external\n    causes (e.g. a send stream whose peer has disconnected).\n    \"\"\"\n\n\nclass BrokenWorkerProcess(Exception):\n    \"\"\"\n    Raised by :func:`run_sync_in_process` if the worker process terminates abruptly or\n    otherwise misbehaves.\n    \"\"\"\n\n\nclass BusyResourceError(Exception):\n    \"\"\"\n    Raised when two tasks are trying to read from or write to the same resource\n    concurrently.\n    \"\"\"\n\n    def __init__(self, action: str):\n        super().__init__(f\"Another task is already {action} this resource\")\n\n\nclass ClosedResourceError(Exception):\n    \"\"\"Raised when trying to use a resource that has been closed.\"\"\"\n\n\nclass DelimiterNotFound(Exception):\n    \"\"\"\n    Raised during\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the\n    maximum number of bytes has been read without the delimiter being found.\n    \"\"\"\n\n    def __init__(self, max_bytes: int) -> None:\n        super().__init__(\n            f\"The delimiter was not found among the first {max_bytes} bytes\"\n        )\n\n\nclass EndOfStream(Exception):\n    \"\"\"\n    Raised when trying to read from a stream that has been closed from the other end.\n    \"\"\"\n\n\nclass IncompleteRead(Exception):\n    \"\"\"\n    Raised during\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_exactly` or\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the\n    connection is closed before the requested amount of bytes has been read.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(\n            \"The stream was closed before the read operation could be completed\"\n        )\n\n\nclass TypedAttributeLookupError(LookupError):\n    \"\"\"\n    Raised by :meth:`~anyio.TypedAttributeProvider.extra` when the given typed attribute\n    is not found and no default value has been given.\n    \"\"\"\n\n\nclass WouldBlock(Exception):\n    \"\"\"Raised by ``X_nowait`` functions if ``X()`` would block.\"\"\"\n", "src/anyio/_core/_signals.py": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterator\nfrom signal import Signals\nfrom typing import ContextManager\n\nfrom ._eventloop import get_async_backend\n\n\ndef open_signal_receiver(*signals: Signals) -> ContextManager[AsyncIterator[Signals]]:\n    \"\"\"\n    Start receiving operating system signals.\n\n    :param signals: signals to receive (e.g. ``signal.SIGINT``)\n    :return: an asynchronous context manager for an asynchronous iterator which yields\n        signal numbers\n\n    .. warning:: Windows does not support signals natively so it is best to avoid\n        relying on this in cross-platform applications.\n\n    .. warning:: On asyncio, this permanently replaces any previous signal handler for\n        the given signals, as set via :meth:`~asyncio.loop.add_signal_handler`.\n\n    \"\"\"\n    return get_async_backend().open_signal_receiver(*signals)\n", "src/anyio/_core/_resources.py": "from __future__ import annotations\n\nfrom ..abc import AsyncResource\nfrom ._tasks import CancelScope\n\n\nasync def aclose_forcefully(resource: AsyncResource) -> None:\n    \"\"\"\n    Close an asynchronous resource in a cancelled scope.\n\n    Doing this closes the resource without waiting on anything.\n\n    :param resource: the resource to close\n\n    \"\"\"\n    with CancelScope() as scope:\n        scope.cancel()\n        await resource.aclose()\n", "src/anyio/_core/_eventloop.py": "from __future__ import annotations\n\nimport math\nimport sys\nimport threading\nfrom collections.abc import Awaitable, Callable, Generator\nfrom contextlib import contextmanager\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nimport sniffio\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nif TYPE_CHECKING:\n    from ..abc import AsyncBackend\n\n# This must be updated when new backends are introduced\nBACKENDS = \"asyncio\", \"trio\"\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\nthreadlocals = threading.local()\nloaded_backends: dict[str, type[AsyncBackend]] = {}\n\n\ndef run(\n    func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n    *args: Unpack[PosArgsT],\n    backend: str = \"asyncio\",\n    backend_options: dict[str, Any] | None = None,\n) -> T_Retval:\n    \"\"\"\n    Run the given coroutine function in an asynchronous event loop.\n\n    The current thread must not be already running an event loop.\n\n    :param func: a coroutine function\n    :param args: positional arguments to ``func``\n    :param backend: name of the asynchronous event loop implementation \u2013 currently\n        either ``asyncio`` or ``trio``\n    :param backend_options: keyword arguments to call the backend ``run()``\n        implementation with (documented :ref:`here <backend options>`)\n    :return: the return value of the coroutine function\n    :raises RuntimeError: if an asynchronous event loop is already running in this\n        thread\n    :raises LookupError: if the named backend is not found\n\n    \"\"\"\n    try:\n        asynclib_name = sniffio.current_async_library()\n    except sniffio.AsyncLibraryNotFoundError:\n        pass\n    else:\n        raise RuntimeError(f\"Already running {asynclib_name} in this thread\")\n\n    try:\n        async_backend = get_async_backend(backend)\n    except ImportError as exc:\n        raise LookupError(f\"No such backend: {backend}\") from exc\n\n    token = None\n    if sniffio.current_async_library_cvar.get(None) is None:\n        # Since we're in control of the event loop, we can cache the name of the async\n        # library\n        token = sniffio.current_async_library_cvar.set(backend)\n\n    try:\n        backend_options = backend_options or {}\n        return async_backend.run(func, args, {}, backend_options)\n    finally:\n        if token:\n            sniffio.current_async_library_cvar.reset(token)\n\n\nasync def sleep(delay: float) -> None:\n    \"\"\"\n    Pause the current task for the specified duration.\n\n    :param delay: the duration, in seconds\n\n    \"\"\"\n    return await get_async_backend().sleep(delay)\n\n\nasync def sleep_forever() -> None:\n    \"\"\"\n    Pause the current task until it's cancelled.\n\n    This is a shortcut for ``sleep(math.inf)``.\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    await sleep(math.inf)\n\n\nasync def sleep_until(deadline: float) -> None:\n    \"\"\"\n    Pause the current task until the given time.\n\n    :param deadline: the absolute time to wake up at (according to the internal\n        monotonic clock of the event loop)\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    now = current_time()\n    await sleep(max(deadline - now, 0))\n\n\ndef current_time() -> float:\n    \"\"\"\n    Return the current value of the event loop's internal clock.\n\n    :return: the clock value (seconds)\n\n    \"\"\"\n    return get_async_backend().current_time()\n\n\ndef get_all_backends() -> tuple[str, ...]:\n    \"\"\"Return a tuple of the names of all built-in backends.\"\"\"\n    return BACKENDS\n\n\ndef get_cancelled_exc_class() -> type[BaseException]:\n    \"\"\"Return the current async library's cancellation exception class.\"\"\"\n    return get_async_backend().cancelled_exception_class()\n\n\n#\n# Private API\n#\n\n\n@contextmanager\ndef claim_worker_thread(\n    backend_class: type[AsyncBackend], token: object\n) -> Generator[Any, None, None]:\n    threadlocals.current_async_backend = backend_class\n    threadlocals.current_token = token\n    try:\n        yield\n    finally:\n        del threadlocals.current_async_backend\n        del threadlocals.current_token\n\n\ndef get_async_backend(asynclib_name: str | None = None) -> type[AsyncBackend]:\n    if asynclib_name is None:\n        asynclib_name = sniffio.current_async_library()\n\n    # We use our own dict instead of sys.modules to get the already imported back-end\n    # class because the appropriate modules in sys.modules could potentially be only\n    # partially initialized\n    try:\n        return loaded_backends[asynclib_name]\n    except KeyError:\n        module = import_module(f\"anyio._backends._{asynclib_name}\")\n        loaded_backends[asynclib_name] = module.backend_class\n        return module.backend_class\n", "src/anyio/_core/_typedattr.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nfrom typing import Any, TypeVar, final, overload\n\nfrom ._exceptions import TypedAttributeLookupError\n\nT_Attr = TypeVar(\"T_Attr\")\nT_Default = TypeVar(\"T_Default\")\nundefined = object()\n\n\ndef typed_attribute() -> Any:\n    \"\"\"Return a unique object, used to mark typed attributes.\"\"\"\n    return object()\n\n\nclass TypedAttributeSet:\n    \"\"\"\n    Superclass for typed attribute collections.\n\n    Checks that every public attribute of every subclass has a type annotation.\n    \"\"\"\n\n    def __init_subclass__(cls) -> None:\n        annotations: dict[str, Any] = getattr(cls, \"__annotations__\", {})\n        for attrname in dir(cls):\n            if not attrname.startswith(\"_\") and attrname not in annotations:\n                raise TypeError(\n                    f\"Attribute {attrname!r} is missing its type annotation\"\n                )\n\n        super().__init_subclass__()\n\n\nclass TypedAttributeProvider:\n    \"\"\"Base class for classes that wish to provide typed extra attributes.\"\"\"\n\n    @property\n    def extra_attributes(self) -> Mapping[T_Attr, Callable[[], T_Attr]]:\n        \"\"\"\n        A mapping of the extra attributes to callables that return the corresponding\n        values.\n\n        If the provider wraps another provider, the attributes from that wrapper should\n        also be included in the returned mapping (but the wrapper may override the\n        callables from the wrapped instance).\n\n        \"\"\"\n        return {}\n\n    @overload\n    def extra(self, attribute: T_Attr) -> T_Attr: ...\n\n    @overload\n    def extra(self, attribute: T_Attr, default: T_Default) -> T_Attr | T_Default: ...\n\n    @final\n    def extra(self, attribute: Any, default: object = undefined) -> object:\n        \"\"\"\n        extra(attribute, default=undefined)\n\n        Return the value of the given typed extra attribute.\n\n        :param attribute: the attribute (member of a :class:`~TypedAttributeSet`) to\n            look for\n        :param default: the value that should be returned if no value is found for the\n            attribute\n        :raises ~anyio.TypedAttributeLookupError: if the search failed and no default\n            value was given\n\n        \"\"\"\n        try:\n            getter = self.extra_attributes[attribute]\n        except KeyError:\n            if default is undefined:\n                raise TypedAttributeLookupError(\"Attribute not found\") from None\n            else:\n                return default\n\n        return getter()\n", "src/anyio/_core/_streams.py": "from __future__ import annotations\n\nimport math\nfrom typing import Tuple, TypeVar\nfrom warnings import warn\n\nfrom ..streams.memory import (\n    MemoryObjectReceiveStream,\n    MemoryObjectSendStream,\n    MemoryObjectStreamState,\n)\n\nT_Item = TypeVar(\"T_Item\")\n\n\nclass create_memory_object_stream(\n    Tuple[MemoryObjectSendStream[T_Item], MemoryObjectReceiveStream[T_Item]],\n):\n    \"\"\"\n    Create a memory object stream.\n\n    The stream's item type can be annotated like\n    :func:`create_memory_object_stream[T_Item]`.\n\n    :param max_buffer_size: number of items held in the buffer until ``send()`` starts\n        blocking\n    :param item_type: old way of marking the streams with the right generic type for\n        static typing (does nothing on AnyIO 4)\n\n        .. deprecated:: 4.0\n          Use ``create_memory_object_stream[YourItemType](...)`` instead.\n    :return: a tuple of (send stream, receive stream)\n\n    \"\"\"\n\n    def __new__(  # type: ignore[misc]\n        cls, max_buffer_size: float = 0, item_type: object = None\n    ) -> tuple[MemoryObjectSendStream[T_Item], MemoryObjectReceiveStream[T_Item]]:\n        if max_buffer_size != math.inf and not isinstance(max_buffer_size, int):\n            raise ValueError(\"max_buffer_size must be either an integer or math.inf\")\n        if max_buffer_size < 0:\n            raise ValueError(\"max_buffer_size cannot be negative\")\n        if item_type is not None:\n            warn(\n                \"The item_type argument has been deprecated in AnyIO 4.0. \"\n                \"Use create_memory_object_stream[YourItemType](...) instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        state = MemoryObjectStreamState[T_Item](max_buffer_size)\n        return (MemoryObjectSendStream(state), MemoryObjectReceiveStream(state))\n", "src/anyio/_core/_subprocesses.py": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterable, Mapping, Sequence\nfrom io import BytesIO\nfrom os import PathLike\nfrom subprocess import DEVNULL, PIPE, CalledProcessError, CompletedProcess\nfrom typing import IO, Any, cast\n\nfrom ..abc import Process\nfrom ._eventloop import get_async_backend\nfrom ._tasks import create_task_group\n\n\nasync def run_process(\n    command: str | bytes | Sequence[str | bytes],\n    *,\n    input: bytes | None = None,\n    stdout: int | IO[Any] | None = PIPE,\n    stderr: int | IO[Any] | None = PIPE,\n    check: bool = True,\n    cwd: str | bytes | PathLike[str] | None = None,\n    env: Mapping[str, str] | None = None,\n    start_new_session: bool = False,\n) -> CompletedProcess[bytes]:\n    \"\"\"\n    Run an external command in a subprocess and wait until it completes.\n\n    .. seealso:: :func:`subprocess.run`\n\n    :param command: either a string to pass to the shell, or an iterable of strings\n        containing the executable name or path and its arguments\n    :param input: bytes passed to the standard input of the subprocess\n    :param stdout: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        a file-like object, or `None`\n    :param stderr: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        :data:`subprocess.STDOUT`, a file-like object, or `None`\n    :param check: if ``True``, raise :exc:`~subprocess.CalledProcessError` if the\n        process terminates with a return code other than 0\n    :param cwd: If not ``None``, change the working directory to this before running the\n        command\n    :param env: if not ``None``, this mapping replaces the inherited environment\n        variables from the parent process\n    :param start_new_session: if ``true`` the setsid() system call will be made in the\n        child process prior to the execution of the subprocess. (POSIX only)\n    :return: an object representing the completed process\n    :raises ~subprocess.CalledProcessError: if ``check`` is ``True`` and the process\n        exits with a nonzero return code\n\n    \"\"\"\n\n    async def drain_stream(stream: AsyncIterable[bytes], index: int) -> None:\n        buffer = BytesIO()\n        async for chunk in stream:\n            buffer.write(chunk)\n\n        stream_contents[index] = buffer.getvalue()\n\n    async with await open_process(\n        command,\n        stdin=PIPE if input else DEVNULL,\n        stdout=stdout,\n        stderr=stderr,\n        cwd=cwd,\n        env=env,\n        start_new_session=start_new_session,\n    ) as process:\n        stream_contents: list[bytes | None] = [None, None]\n        async with create_task_group() as tg:\n            if process.stdout:\n                tg.start_soon(drain_stream, process.stdout, 0)\n\n            if process.stderr:\n                tg.start_soon(drain_stream, process.stderr, 1)\n\n            if process.stdin and input:\n                await process.stdin.send(input)\n                await process.stdin.aclose()\n\n            await process.wait()\n\n    output, errors = stream_contents\n    if check and process.returncode != 0:\n        raise CalledProcessError(cast(int, process.returncode), command, output, errors)\n\n    return CompletedProcess(command, cast(int, process.returncode), output, errors)\n\n\nasync def open_process(\n    command: str | bytes | Sequence[str | bytes],\n    *,\n    stdin: int | IO[Any] | None = PIPE,\n    stdout: int | IO[Any] | None = PIPE,\n    stderr: int | IO[Any] | None = PIPE,\n    cwd: str | bytes | PathLike[str] | None = None,\n    env: Mapping[str, str] | None = None,\n    start_new_session: bool = False,\n) -> Process:\n    \"\"\"\n    Start an external command in a subprocess.\n\n    .. seealso:: :class:`subprocess.Popen`\n\n    :param command: either a string to pass to the shell, or an iterable of strings\n        containing the executable name or path and its arguments\n    :param stdin: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`, a\n        file-like object, or ``None``\n    :param stdout: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        a file-like object, or ``None``\n    :param stderr: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        :data:`subprocess.STDOUT`, a file-like object, or ``None``\n    :param cwd: If not ``None``, the working directory is changed before executing\n    :param env: If env is not ``None``, it must be a mapping that defines the\n        environment variables for the new process\n    :param start_new_session: if ``true`` the setsid() system call will be made in the\n        child process prior to the execution of the subprocess. (POSIX only)\n    :return: an asynchronous process object\n\n    \"\"\"\n    if isinstance(command, (str, bytes)):\n        return await get_async_backend().open_process(\n            command,\n            shell=True,\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n    else:\n        return await get_async_backend().open_process(\n            command,\n            shell=False,\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n", "src/anyio/_core/_synchronization.py": "from __future__ import annotations\n\nimport math\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom types import TracebackType\n\nfrom sniffio import AsyncLibraryNotFoundError\n\nfrom ..lowlevel import cancel_shielded_checkpoint, checkpoint, checkpoint_if_cancelled\nfrom ._eventloop import get_async_backend\nfrom ._exceptions import BusyResourceError, WouldBlock\nfrom ._tasks import CancelScope\nfrom ._testing import TaskInfo, get_current_task\n\n\n@dataclass(frozen=True)\nclass EventStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Event.wait`\n    \"\"\"\n\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass CapacityLimiterStatistics:\n    \"\"\"\n    :ivar int borrowed_tokens: number of tokens currently borrowed by tasks\n    :ivar float total_tokens: total number of available tokens\n    :ivar tuple borrowers: tasks or other objects currently holding tokens borrowed from\n        this limiter\n    :ivar int tasks_waiting: number of tasks waiting on\n        :meth:`~.CapacityLimiter.acquire` or\n        :meth:`~.CapacityLimiter.acquire_on_behalf_of`\n    \"\"\"\n\n    borrowed_tokens: int\n    total_tokens: float\n    borrowers: tuple[object, ...]\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass LockStatistics:\n    \"\"\"\n    :ivar bool locked: flag indicating if this lock is locked or not\n    :ivar ~anyio.TaskInfo owner: task currently holding the lock (or ``None`` if the\n        lock is not held by any task)\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Lock.acquire`\n    \"\"\"\n\n    locked: bool\n    owner: TaskInfo | None\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass ConditionStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks blocked on :meth:`~.Condition.wait`\n    :ivar ~anyio.LockStatistics lock_statistics: statistics of the underlying\n        :class:`~.Lock`\n    \"\"\"\n\n    tasks_waiting: int\n    lock_statistics: LockStatistics\n\n\n@dataclass(frozen=True)\nclass SemaphoreStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Semaphore.acquire`\n\n    \"\"\"\n\n    tasks_waiting: int\n\n\nclass Event:\n    def __new__(cls) -> Event:\n        try:\n            return get_async_backend().create_event()\n        except AsyncLibraryNotFoundError:\n            return EventAdapter()\n\n    def set(self) -> None:\n        \"\"\"Set the flag, notifying all listeners.\"\"\"\n        raise NotImplementedError\n\n    def is_set(self) -> bool:\n        \"\"\"Return ``True`` if the flag is set, ``False`` if not.\"\"\"\n        raise NotImplementedError\n\n    async def wait(self) -> None:\n        \"\"\"\n        Wait until the flag has been set.\n\n        If the flag has already been set when this method is called, it returns\n        immediately.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def statistics(self) -> EventStatistics:\n        \"\"\"Return statistics about the current state of this event.\"\"\"\n        raise NotImplementedError\n\n\nclass EventAdapter(Event):\n    _internal_event: Event | None = None\n\n    def __new__(cls) -> EventAdapter:\n        return object.__new__(cls)\n\n    @property\n    def _event(self) -> Event:\n        if self._internal_event is None:\n            self._internal_event = get_async_backend().create_event()\n\n        return self._internal_event\n\n    def set(self) -> None:\n        self._event.set()\n\n    def is_set(self) -> bool:\n        return self._internal_event is not None and self._internal_event.is_set()\n\n    async def wait(self) -> None:\n        await self._event.wait()\n\n    def statistics(self) -> EventStatistics:\n        if self._internal_event is None:\n            return EventStatistics(tasks_waiting=0)\n\n        return self._internal_event.statistics()\n\n\nclass Lock:\n    _owner_task: TaskInfo | None = None\n\n    def __init__(self) -> None:\n        self._waiters: deque[tuple[TaskInfo, Event]] = deque()\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire the lock.\"\"\"\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_nowait()\n        except WouldBlock:\n            task = get_current_task()\n            event = Event()\n            token = task, event\n            self._waiters.append(token)\n            try:\n                await event.wait()\n            except BaseException:\n                if not event.is_set():\n                    self._waiters.remove(token)\n                elif self._owner_task == task:\n                    self.release()\n\n                raise\n\n            assert self._owner_task == task\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the lock, without blocking.\n\n        :raises ~anyio.WouldBlock: if the operation would block\n\n        \"\"\"\n        task = get_current_task()\n        if self._owner_task == task:\n            raise RuntimeError(\"Attempted to acquire an already held Lock\")\n\n        if self._owner_task is not None:\n            raise WouldBlock\n\n        self._owner_task = task\n\n    def release(self) -> None:\n        \"\"\"Release the lock.\"\"\"\n        if self._owner_task != get_current_task():\n            raise RuntimeError(\"The current task is not holding this lock\")\n\n        if self._waiters:\n            self._owner_task, event = self._waiters.popleft()\n            event.set()\n        else:\n            del self._owner_task\n\n    def locked(self) -> bool:\n        \"\"\"Return True if the lock is currently held.\"\"\"\n        return self._owner_task is not None\n\n    def statistics(self) -> LockStatistics:\n        \"\"\"\n        Return statistics about the current state of this lock.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return LockStatistics(self.locked(), self._owner_task, len(self._waiters))\n\n\nclass Condition:\n    _owner_task: TaskInfo | None = None\n\n    def __init__(self, lock: Lock | None = None):\n        self._lock = lock or Lock()\n        self._waiters: deque[Event] = deque()\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    def _check_acquired(self) -> None:\n        if self._owner_task != get_current_task():\n            raise RuntimeError(\"The current task is not holding the underlying lock\")\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire the underlying lock.\"\"\"\n        await self._lock.acquire()\n        self._owner_task = get_current_task()\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the underlying lock, without blocking.\n\n        :raises ~anyio.WouldBlock: if the operation would block\n\n        \"\"\"\n        self._lock.acquire_nowait()\n        self._owner_task = get_current_task()\n\n    def release(self) -> None:\n        \"\"\"Release the underlying lock.\"\"\"\n        self._lock.release()\n\n    def locked(self) -> bool:\n        \"\"\"Return True if the lock is set.\"\"\"\n        return self._lock.locked()\n\n    def notify(self, n: int = 1) -> None:\n        \"\"\"Notify exactly n listeners.\"\"\"\n        self._check_acquired()\n        for _ in range(n):\n            try:\n                event = self._waiters.popleft()\n            except IndexError:\n                break\n\n            event.set()\n\n    def notify_all(self) -> None:\n        \"\"\"Notify all the listeners.\"\"\"\n        self._check_acquired()\n        for event in self._waiters:\n            event.set()\n\n        self._waiters.clear()\n\n    async def wait(self) -> None:\n        \"\"\"Wait for a notification.\"\"\"\n        await checkpoint()\n        event = Event()\n        self._waiters.append(event)\n        self.release()\n        try:\n            await event.wait()\n        except BaseException:\n            if not event.is_set():\n                self._waiters.remove(event)\n\n            raise\n        finally:\n            with CancelScope(shield=True):\n                await self.acquire()\n\n    def statistics(self) -> ConditionStatistics:\n        \"\"\"\n        Return statistics about the current state of this condition.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return ConditionStatistics(len(self._waiters), self._lock.statistics())\n\n\nclass Semaphore:\n    def __init__(self, initial_value: int, *, max_value: int | None = None):\n        if not isinstance(initial_value, int):\n            raise TypeError(\"initial_value must be an integer\")\n        if initial_value < 0:\n            raise ValueError(\"initial_value must be >= 0\")\n        if max_value is not None:\n            if not isinstance(max_value, int):\n                raise TypeError(\"max_value must be an integer or None\")\n            if max_value < initial_value:\n                raise ValueError(\n                    \"max_value must be equal to or higher than initial_value\"\n                )\n\n        self._value = initial_value\n        self._max_value = max_value\n        self._waiters: deque[Event] = deque()\n\n    async def __aenter__(self) -> Semaphore:\n        await self.acquire()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    async def acquire(self) -> None:\n        \"\"\"Decrement the semaphore value, blocking if necessary.\"\"\"\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_nowait()\n        except WouldBlock:\n            event = Event()\n            self._waiters.append(event)\n            try:\n                await event.wait()\n            except BaseException:\n                if not event.is_set():\n                    self._waiters.remove(event)\n                else:\n                    self.release()\n\n                raise\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the underlying lock, without blocking.\n\n        :raises ~anyio.WouldBlock: if the operation would block\n\n        \"\"\"\n        if self._value == 0:\n            raise WouldBlock\n\n        self._value -= 1\n\n    def release(self) -> None:\n        \"\"\"Increment the semaphore value.\"\"\"\n        if self._max_value is not None and self._value == self._max_value:\n            raise ValueError(\"semaphore released too many times\")\n\n        if self._waiters:\n            self._waiters.popleft().set()\n        else:\n            self._value += 1\n\n    @property\n    def value(self) -> int:\n        \"\"\"The current value of the semaphore.\"\"\"\n        return self._value\n\n    @property\n    def max_value(self) -> int | None:\n        \"\"\"The maximum value of the semaphore.\"\"\"\n        return self._max_value\n\n    def statistics(self) -> SemaphoreStatistics:\n        \"\"\"\n        Return statistics about the current state of this semaphore.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return SemaphoreStatistics(len(self._waiters))\n\n\nclass CapacityLimiter:\n    def __new__(cls, total_tokens: float) -> CapacityLimiter:\n        try:\n            return get_async_backend().create_capacity_limiter(total_tokens)\n        except AsyncLibraryNotFoundError:\n            return CapacityLimiterAdapter(total_tokens)\n\n    async def __aenter__(self) -> None:\n        raise NotImplementedError\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        raise NotImplementedError\n\n    @property\n    def total_tokens(self) -> float:\n        \"\"\"\n        The total number of tokens available for borrowing.\n\n        This is a read-write property. If the total number of tokens is increased, the\n        proportionate number of tasks waiting on this limiter will be granted their\n        tokens.\n\n        .. versionchanged:: 3.0\n            The property is now writable.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        raise NotImplementedError\n\n    @property\n    def borrowed_tokens(self) -> int:\n        \"\"\"The number of tokens that have currently been borrowed.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def available_tokens(self) -> float:\n        \"\"\"The number of tokens currently available to be borrowed\"\"\"\n        raise NotImplementedError\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire a token for the current task without waiting for one to become\n        available.\n\n        :raises ~anyio.WouldBlock: if there are no tokens available for borrowing\n\n        \"\"\"\n        raise NotImplementedError\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        \"\"\"\n        Acquire a token without waiting for one to become available.\n\n        :param borrower: the entity borrowing a token\n        :raises ~anyio.WouldBlock: if there are no tokens available for borrowing\n\n        \"\"\"\n        raise NotImplementedError\n\n    async def acquire(self) -> None:\n        \"\"\"\n        Acquire a token for the current task, waiting if necessary for one to become\n        available.\n\n        \"\"\"\n        raise NotImplementedError\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        \"\"\"\n        Acquire a token, waiting if necessary for one to become available.\n\n        :param borrower: the entity borrowing a token\n\n        \"\"\"\n        raise NotImplementedError\n\n    def release(self) -> None:\n        \"\"\"\n        Release the token held by the current task.\n\n        :raises RuntimeError: if the current task has not borrowed a token from this\n            limiter.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        \"\"\"\n        Release the token held by the given borrower.\n\n        :raises RuntimeError: if the borrower has not borrowed a token from this\n            limiter.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        \"\"\"\n        Return statistics about the current state of this limiter.\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass CapacityLimiterAdapter(CapacityLimiter):\n    _internal_limiter: CapacityLimiter | None = None\n\n    def __new__(cls, total_tokens: float) -> CapacityLimiterAdapter:\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float) -> None:\n        self.total_tokens = total_tokens\n\n    @property\n    def _limiter(self) -> CapacityLimiter:\n        if self._internal_limiter is None:\n            self._internal_limiter = get_async_backend().create_capacity_limiter(\n                self._total_tokens\n            )\n\n        return self._internal_limiter\n\n    async def __aenter__(self) -> None:\n        await self._limiter.__aenter__()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        return await self._limiter.__aexit__(exc_type, exc_val, exc_tb)\n\n    @property\n    def total_tokens(self) -> float:\n        if self._internal_limiter is None:\n            return self._total_tokens\n\n        return self._internal_limiter.total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and value is not math.inf:\n            raise TypeError(\"total_tokens must be an int or math.inf\")\n        elif value < 1:\n            raise ValueError(\"total_tokens must be >= 1\")\n\n        if self._internal_limiter is None:\n            self._total_tokens = value\n            return\n\n        self._limiter.total_tokens = value\n\n    @property\n    def borrowed_tokens(self) -> int:\n        if self._internal_limiter is None:\n            return 0\n\n        return self._internal_limiter.borrowed_tokens\n\n    @property\n    def available_tokens(self) -> float:\n        if self._internal_limiter is None:\n            return self._total_tokens\n\n        return self._internal_limiter.available_tokens\n\n    def acquire_nowait(self) -> None:\n        self._limiter.acquire_nowait()\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        self._limiter.acquire_on_behalf_of_nowait(borrower)\n\n    async def acquire(self) -> None:\n        await self._limiter.acquire()\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await self._limiter.acquire_on_behalf_of(borrower)\n\n    def release(self) -> None:\n        self._limiter.release()\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        self._limiter.release_on_behalf_of(borrower)\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        if self._internal_limiter is None:\n            return CapacityLimiterStatistics(\n                borrowed_tokens=0,\n                total_tokens=self.total_tokens,\n                borrowers=(),\n                tasks_waiting=0,\n            )\n\n        return self._internal_limiter.statistics()\n\n\nclass ResourceGuard:\n    \"\"\"\n    A context manager for ensuring that a resource is only used by a single task at a\n    time.\n\n    Entering this context manager while the previous has not exited it yet will trigger\n    :exc:`BusyResourceError`.\n\n    :param action: the action to guard against (visible in the :exc:`BusyResourceError`\n        when triggered, e.g. \"Another task is already {action} this resource\")\n\n    .. versionadded:: 4.1\n    \"\"\"\n\n    __slots__ = \"action\", \"_guarded\"\n\n    def __init__(self, action: str = \"using\"):\n        self.action: str = action\n        self._guarded = False\n\n    def __enter__(self) -> None:\n        if self._guarded:\n            raise BusyResourceError(self.action)\n\n        self._guarded = True\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        self._guarded = False\n        return None\n", "src/anyio/_core/__init__.py": "", "src/anyio/_core/_fileio.py": "from __future__ import annotations\n\nimport os\nimport pathlib\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom os import PathLike\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    AsyncIterator,\n    Final,\n    Generic,\n    overload,\n)\n\nfrom .. import to_thread\nfrom ..abc import AsyncResource\n\nif TYPE_CHECKING:\n    from _typeshed import OpenBinaryMode, OpenTextMode, ReadableBuffer, WriteableBuffer\nelse:\n    ReadableBuffer = OpenBinaryMode = OpenTextMode = WriteableBuffer = object\n\n\nclass AsyncFile(AsyncResource, Generic[AnyStr]):\n    \"\"\"\n    An asynchronous file object.\n\n    This class wraps a standard file object and provides async friendly versions of the\n    following blocking methods (where available on the original file object):\n\n    * read\n    * read1\n    * readline\n    * readlines\n    * readinto\n    * readinto1\n    * write\n    * writelines\n    * truncate\n    * seek\n    * tell\n    * flush\n\n    All other methods are directly passed through.\n\n    This class supports the asynchronous context manager protocol which closes the\n    underlying file at the end of the context block.\n\n    This class also supports asynchronous iteration::\n\n        async with await open_file(...) as f:\n            async for line in f:\n                print(line)\n    \"\"\"\n\n    def __init__(self, fp: IO[AnyStr]) -> None:\n        self._fp: Any = fp\n\n    def __getattr__(self, name: str) -> object:\n        return getattr(self._fp, name)\n\n    @property\n    def wrapped(self) -> IO[AnyStr]:\n        \"\"\"The wrapped file object.\"\"\"\n        return self._fp\n\n    async def __aiter__(self) -> AsyncIterator[AnyStr]:\n        while True:\n            line = await self.readline()\n            if line:\n                yield line\n            else:\n                break\n\n    async def aclose(self) -> None:\n        return await to_thread.run_sync(self._fp.close)\n\n    async def read(self, size: int = -1) -> AnyStr:\n        return await to_thread.run_sync(self._fp.read, size)\n\n    async def read1(self: AsyncFile[bytes], size: int = -1) -> bytes:\n        return await to_thread.run_sync(self._fp.read1, size)\n\n    async def readline(self) -> AnyStr:\n        return await to_thread.run_sync(self._fp.readline)\n\n    async def readlines(self) -> list[AnyStr]:\n        return await to_thread.run_sync(self._fp.readlines)\n\n    async def readinto(self: AsyncFile[bytes], b: WriteableBuffer) -> bytes:\n        return await to_thread.run_sync(self._fp.readinto, b)\n\n    async def readinto1(self: AsyncFile[bytes], b: WriteableBuffer) -> bytes:\n        return await to_thread.run_sync(self._fp.readinto1, b)\n\n    @overload\n    async def write(self: AsyncFile[bytes], b: ReadableBuffer) -> int: ...\n\n    @overload\n    async def write(self: AsyncFile[str], b: str) -> int: ...\n\n    async def write(self, b: ReadableBuffer | str) -> int:\n        return await to_thread.run_sync(self._fp.write, b)\n\n    @overload\n    async def writelines(\n        self: AsyncFile[bytes], lines: Iterable[ReadableBuffer]\n    ) -> None: ...\n\n    @overload\n    async def writelines(self: AsyncFile[str], lines: Iterable[str]) -> None: ...\n\n    async def writelines(self, lines: Iterable[ReadableBuffer] | Iterable[str]) -> None:\n        return await to_thread.run_sync(self._fp.writelines, lines)\n\n    async def truncate(self, size: int | None = None) -> int:\n        return await to_thread.run_sync(self._fp.truncate, size)\n\n    async def seek(self, offset: int, whence: int | None = os.SEEK_SET) -> int:\n        return await to_thread.run_sync(self._fp.seek, offset, whence)\n\n    async def tell(self) -> int:\n        return await to_thread.run_sync(self._fp.tell)\n\n    async def flush(self) -> None:\n        return await to_thread.run_sync(self._fp.flush)\n\n\n@overload\nasync def open_file(\n    file: str | PathLike[str] | int,\n    mode: OpenBinaryMode,\n    buffering: int = ...,\n    encoding: str | None = ...,\n    errors: str | None = ...,\n    newline: str | None = ...,\n    closefd: bool = ...,\n    opener: Callable[[str, int], int] | None = ...,\n) -> AsyncFile[bytes]: ...\n\n\n@overload\nasync def open_file(\n    file: str | PathLike[str] | int,\n    mode: OpenTextMode = ...,\n    buffering: int = ...,\n    encoding: str | None = ...,\n    errors: str | None = ...,\n    newline: str | None = ...,\n    closefd: bool = ...,\n    opener: Callable[[str, int], int] | None = ...,\n) -> AsyncFile[str]: ...\n\n\nasync def open_file(\n    file: str | PathLike[str] | int,\n    mode: str = \"r\",\n    buffering: int = -1,\n    encoding: str | None = None,\n    errors: str | None = None,\n    newline: str | None = None,\n    closefd: bool = True,\n    opener: Callable[[str, int], int] | None = None,\n) -> AsyncFile[Any]:\n    \"\"\"\n    Open a file asynchronously.\n\n    The arguments are exactly the same as for the builtin :func:`open`.\n\n    :return: an asynchronous file object\n\n    \"\"\"\n    fp = await to_thread.run_sync(\n        open, file, mode, buffering, encoding, errors, newline, closefd, opener\n    )\n    return AsyncFile(fp)\n\n\ndef wrap_file(file: IO[AnyStr]) -> AsyncFile[AnyStr]:\n    \"\"\"\n    Wrap an existing file as an asynchronous file.\n\n    :param file: an existing file-like object\n    :return: an asynchronous file object\n\n    \"\"\"\n    return AsyncFile(file)\n\n\n@dataclass(eq=False)\nclass _PathIterator(AsyncIterator[\"Path\"]):\n    iterator: Iterator[PathLike[str]]\n\n    async def __anext__(self) -> Path:\n        nextval = await to_thread.run_sync(\n            next, self.iterator, None, abandon_on_cancel=True\n        )\n        if nextval is None:\n            raise StopAsyncIteration from None\n\n        return Path(nextval)\n\n\nclass Path:\n    \"\"\"\n    An asynchronous version of :class:`pathlib.Path`.\n\n    This class cannot be substituted for :class:`pathlib.Path` or\n    :class:`pathlib.PurePath`, but it is compatible with the :class:`os.PathLike`\n    interface.\n\n    It implements the Python 3.10 version of :class:`pathlib.Path` interface, except for\n    the deprecated :meth:`~pathlib.Path.link_to` method.\n\n    Any methods that do disk I/O need to be awaited on. These methods are:\n\n    * :meth:`~pathlib.Path.absolute`\n    * :meth:`~pathlib.Path.chmod`\n    * :meth:`~pathlib.Path.cwd`\n    * :meth:`~pathlib.Path.exists`\n    * :meth:`~pathlib.Path.expanduser`\n    * :meth:`~pathlib.Path.group`\n    * :meth:`~pathlib.Path.hardlink_to`\n    * :meth:`~pathlib.Path.home`\n    * :meth:`~pathlib.Path.is_block_device`\n    * :meth:`~pathlib.Path.is_char_device`\n    * :meth:`~pathlib.Path.is_dir`\n    * :meth:`~pathlib.Path.is_fifo`\n    * :meth:`~pathlib.Path.is_file`\n    * :meth:`~pathlib.Path.is_mount`\n    * :meth:`~pathlib.Path.lchmod`\n    * :meth:`~pathlib.Path.lstat`\n    * :meth:`~pathlib.Path.mkdir`\n    * :meth:`~pathlib.Path.open`\n    * :meth:`~pathlib.Path.owner`\n    * :meth:`~pathlib.Path.read_bytes`\n    * :meth:`~pathlib.Path.read_text`\n    * :meth:`~pathlib.Path.readlink`\n    * :meth:`~pathlib.Path.rename`\n    * :meth:`~pathlib.Path.replace`\n    * :meth:`~pathlib.Path.rmdir`\n    * :meth:`~pathlib.Path.samefile`\n    * :meth:`~pathlib.Path.stat`\n    * :meth:`~pathlib.Path.touch`\n    * :meth:`~pathlib.Path.unlink`\n    * :meth:`~pathlib.Path.write_bytes`\n    * :meth:`~pathlib.Path.write_text`\n\n    Additionally, the following methods return an async iterator yielding\n    :class:`~.Path` objects:\n\n    * :meth:`~pathlib.Path.glob`\n    * :meth:`~pathlib.Path.iterdir`\n    * :meth:`~pathlib.Path.rglob`\n    \"\"\"\n\n    __slots__ = \"_path\", \"__weakref__\"\n\n    __weakref__: Any\n\n    def __init__(self, *args: str | PathLike[str]) -> None:\n        self._path: Final[pathlib.Path] = pathlib.Path(*args)\n\n    def __fspath__(self) -> str:\n        return self._path.__fspath__()\n\n    def __str__(self) -> str:\n        return self._path.__str__()\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.as_posix()!r})\"\n\n    def __bytes__(self) -> bytes:\n        return self._path.__bytes__()\n\n    def __hash__(self) -> int:\n        return self._path.__hash__()\n\n    def __eq__(self, other: object) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__eq__(target)\n\n    def __lt__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__lt__(target)\n\n    def __le__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__le__(target)\n\n    def __gt__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__gt__(target)\n\n    def __ge__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__ge__(target)\n\n    def __truediv__(self, other: str | PathLike[str]) -> Path:\n        return Path(self._path / other)\n\n    def __rtruediv__(self, other: str | PathLike[str]) -> Path:\n        return Path(other) / self\n\n    @property\n    def parts(self) -> tuple[str, ...]:\n        return self._path.parts\n\n    @property\n    def drive(self) -> str:\n        return self._path.drive\n\n    @property\n    def root(self) -> str:\n        return self._path.root\n\n    @property\n    def anchor(self) -> str:\n        return self._path.anchor\n\n    @property\n    def parents(self) -> Sequence[Path]:\n        return tuple(Path(p) for p in self._path.parents)\n\n    @property\n    def parent(self) -> Path:\n        return Path(self._path.parent)\n\n    @property\n    def name(self) -> str:\n        return self._path.name\n\n    @property\n    def suffix(self) -> str:\n        return self._path.suffix\n\n    @property\n    def suffixes(self) -> list[str]:\n        return self._path.suffixes\n\n    @property\n    def stem(self) -> str:\n        return self._path.stem\n\n    async def absolute(self) -> Path:\n        path = await to_thread.run_sync(self._path.absolute)\n        return Path(path)\n\n    def as_posix(self) -> str:\n        return self._path.as_posix()\n\n    def as_uri(self) -> str:\n        return self._path.as_uri()\n\n    if sys.version_info >= (3, 13):\n        parser = pathlib.Path.parser  # type: ignore[attr-defined]\n\n        @classmethod\n        def from_uri(cls, uri: str) -> Path:\n            return Path(pathlib.Path.from_uri(uri))  # type: ignore[attr-defined]\n\n        def full_match(\n            self, path_pattern: str, *, case_sensitive: bool | None = None\n        ) -> bool:\n            return self._path.full_match(  # type: ignore[attr-defined]\n                path_pattern, case_sensitive=case_sensitive\n            )\n\n        def match(\n            self, path_pattern: str, *, case_sensitive: bool | None = None\n        ) -> bool:\n            return self._path.match(path_pattern, case_sensitive=case_sensitive)\n    else:\n\n        def match(self, path_pattern: str) -> bool:\n            return self._path.match(path_pattern)\n\n    def is_relative_to(self, other: str | PathLike[str]) -> bool:\n        try:\n            self.relative_to(other)\n            return True\n        except ValueError:\n            return False\n\n    async def is_junction(self) -> bool:\n        return await to_thread.run_sync(self._path.is_junction)\n\n    async def chmod(self, mode: int, *, follow_symlinks: bool = True) -> None:\n        func = partial(os.chmod, follow_symlinks=follow_symlinks)\n        return await to_thread.run_sync(func, self._path, mode)\n\n    @classmethod\n    async def cwd(cls) -> Path:\n        path = await to_thread.run_sync(pathlib.Path.cwd)\n        return cls(path)\n\n    async def exists(self) -> bool:\n        return await to_thread.run_sync(self._path.exists, abandon_on_cancel=True)\n\n    async def expanduser(self) -> Path:\n        return Path(\n            await to_thread.run_sync(self._path.expanduser, abandon_on_cancel=True)\n        )\n\n    def glob(self, pattern: str) -> AsyncIterator[Path]:\n        gen = self._path.glob(pattern)\n        return _PathIterator(gen)\n\n    async def group(self) -> str:\n        return await to_thread.run_sync(self._path.group, abandon_on_cancel=True)\n\n    async def hardlink_to(\n        self, target: str | bytes | PathLike[str] | PathLike[bytes]\n    ) -> None:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(os.link, target, self)\n\n    @classmethod\n    async def home(cls) -> Path:\n        home_path = await to_thread.run_sync(pathlib.Path.home)\n        return cls(home_path)\n\n    def is_absolute(self) -> bool:\n        return self._path.is_absolute()\n\n    async def is_block_device(self) -> bool:\n        return await to_thread.run_sync(\n            self._path.is_block_device, abandon_on_cancel=True\n        )\n\n    async def is_char_device(self) -> bool:\n        return await to_thread.run_sync(\n            self._path.is_char_device, abandon_on_cancel=True\n        )\n\n    async def is_dir(self) -> bool:\n        return await to_thread.run_sync(self._path.is_dir, abandon_on_cancel=True)\n\n    async def is_fifo(self) -> bool:\n        return await to_thread.run_sync(self._path.is_fifo, abandon_on_cancel=True)\n\n    async def is_file(self) -> bool:\n        return await to_thread.run_sync(self._path.is_file, abandon_on_cancel=True)\n\n    async def is_mount(self) -> bool:\n        return await to_thread.run_sync(\n            os.path.ismount, self._path, abandon_on_cancel=True\n        )\n\n    def is_reserved(self) -> bool:\n        return self._path.is_reserved()\n\n    async def is_socket(self) -> bool:\n        return await to_thread.run_sync(self._path.is_socket, abandon_on_cancel=True)\n\n    async def is_symlink(self) -> bool:\n        return await to_thread.run_sync(self._path.is_symlink, abandon_on_cancel=True)\n\n    def iterdir(self) -> AsyncIterator[Path]:\n        gen = self._path.iterdir()\n        return _PathIterator(gen)\n\n    def joinpath(self, *args: str | PathLike[str]) -> Path:\n        return Path(self._path.joinpath(*args))\n\n    async def lchmod(self, mode: int) -> None:\n        await to_thread.run_sync(self._path.lchmod, mode)\n\n    async def lstat(self) -> os.stat_result:\n        return await to_thread.run_sync(self._path.lstat, abandon_on_cancel=True)\n\n    async def mkdir(\n        self, mode: int = 0o777, parents: bool = False, exist_ok: bool = False\n    ) -> None:\n        await to_thread.run_sync(self._path.mkdir, mode, parents, exist_ok)\n\n    @overload\n    async def open(\n        self,\n        mode: OpenBinaryMode,\n        buffering: int = ...,\n        encoding: str | None = ...,\n        errors: str | None = ...,\n        newline: str | None = ...,\n    ) -> AsyncFile[bytes]: ...\n\n    @overload\n    async def open(\n        self,\n        mode: OpenTextMode = ...,\n        buffering: int = ...,\n        encoding: str | None = ...,\n        errors: str | None = ...,\n        newline: str | None = ...,\n    ) -> AsyncFile[str]: ...\n\n    async def open(\n        self,\n        mode: str = \"r\",\n        buffering: int = -1,\n        encoding: str | None = None,\n        errors: str | None = None,\n        newline: str | None = None,\n    ) -> AsyncFile[Any]:\n        fp = await to_thread.run_sync(\n            self._path.open, mode, buffering, encoding, errors, newline\n        )\n        return AsyncFile(fp)\n\n    async def owner(self) -> str:\n        return await to_thread.run_sync(self._path.owner, abandon_on_cancel=True)\n\n    async def read_bytes(self) -> bytes:\n        return await to_thread.run_sync(self._path.read_bytes)\n\n    async def read_text(\n        self, encoding: str | None = None, errors: str | None = None\n    ) -> str:\n        return await to_thread.run_sync(self._path.read_text, encoding, errors)\n\n    if sys.version_info >= (3, 12):\n\n        def relative_to(\n            self, *other: str | PathLike[str], walk_up: bool = False\n        ) -> Path:\n            return Path(self._path.relative_to(*other, walk_up=walk_up))\n\n    else:\n\n        def relative_to(self, *other: str | PathLike[str]) -> Path:\n            return Path(self._path.relative_to(*other))\n\n    async def readlink(self) -> Path:\n        target = await to_thread.run_sync(os.readlink, self._path)\n        return Path(target)\n\n    async def rename(self, target: str | pathlib.PurePath | Path) -> Path:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.rename, target)\n        return Path(target)\n\n    async def replace(self, target: str | pathlib.PurePath | Path) -> Path:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.replace, target)\n        return Path(target)\n\n    async def resolve(self, strict: bool = False) -> Path:\n        func = partial(self._path.resolve, strict=strict)\n        return Path(await to_thread.run_sync(func, abandon_on_cancel=True))\n\n    def rglob(self, pattern: str) -> AsyncIterator[Path]:\n        gen = self._path.rglob(pattern)\n        return _PathIterator(gen)\n\n    async def rmdir(self) -> None:\n        await to_thread.run_sync(self._path.rmdir)\n\n    async def samefile(self, other_path: str | PathLike[str]) -> bool:\n        if isinstance(other_path, Path):\n            other_path = other_path._path\n\n        return await to_thread.run_sync(\n            self._path.samefile, other_path, abandon_on_cancel=True\n        )\n\n    async def stat(self, *, follow_symlinks: bool = True) -> os.stat_result:\n        func = partial(os.stat, follow_symlinks=follow_symlinks)\n        return await to_thread.run_sync(func, self._path, abandon_on_cancel=True)\n\n    async def symlink_to(\n        self,\n        target: str | bytes | PathLike[str] | PathLike[bytes],\n        target_is_directory: bool = False,\n    ) -> None:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.symlink_to, target, target_is_directory)\n\n    async def touch(self, mode: int = 0o666, exist_ok: bool = True) -> None:\n        await to_thread.run_sync(self._path.touch, mode, exist_ok)\n\n    async def unlink(self, missing_ok: bool = False) -> None:\n        try:\n            await to_thread.run_sync(self._path.unlink)\n        except FileNotFoundError:\n            if not missing_ok:\n                raise\n\n    if sys.version_info >= (3, 12):\n\n        async def walk(\n            self,\n            top_down: bool = True,\n            on_error: Callable[[OSError], object] | None = None,\n            follow_symlinks: bool = False,\n        ) -> AsyncIterator[tuple[Path, list[str], list[str]]]:\n            def get_next_value() -> tuple[pathlib.Path, list[str], list[str]] | None:\n                try:\n                    return next(gen)\n                except StopIteration:\n                    return None\n\n            gen = self._path.walk(top_down, on_error, follow_symlinks)\n            while True:\n                value = await to_thread.run_sync(get_next_value)\n                if value is None:\n                    return\n\n                root, dirs, paths = value\n                yield Path(root), dirs, paths\n\n    def with_name(self, name: str) -> Path:\n        return Path(self._path.with_name(name))\n\n    def with_stem(self, stem: str) -> Path:\n        return Path(self._path.with_name(stem + self._path.suffix))\n\n    def with_suffix(self, suffix: str) -> Path:\n        return Path(self._path.with_suffix(suffix))\n\n    def with_segments(self, *pathsegments: str | PathLike[str]) -> Path:\n        return Path(*pathsegments)\n\n    async def write_bytes(self, data: bytes) -> int:\n        return await to_thread.run_sync(self._path.write_bytes, data)\n\n    async def write_text(\n        self,\n        data: str,\n        encoding: str | None = None,\n        errors: str | None = None,\n        newline: str | None = None,\n    ) -> int:\n        # Path.write_text() does not support the \"newline\" parameter before Python 3.10\n        def sync_write_text() -> int:\n            with self._path.open(\n                \"w\", encoding=encoding, errors=errors, newline=newline\n            ) as fp:\n                return fp.write(data)\n\n        return await to_thread.run_sync(sync_write_text)\n\n\nPathLike.register(Path)\n", "src/anyio/streams/file.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nfrom io import SEEK_SET, UnsupportedOperation\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Any, BinaryIO, cast\n\nfrom .. import (\n    BrokenResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    TypedAttributeSet,\n    to_thread,\n    typed_attribute,\n)\nfrom ..abc import ByteReceiveStream, ByteSendStream\n\n\nclass FileStreamAttribute(TypedAttributeSet):\n    #: the open file descriptor\n    file: BinaryIO = typed_attribute()\n    #: the path of the file on the file system, if available (file must be a real file)\n    path: Path = typed_attribute()\n    #: the file number, if available (file must be a real file or a TTY)\n    fileno: int = typed_attribute()\n\n\nclass _BaseFileStream:\n    def __init__(self, file: BinaryIO):\n        self._file = file\n\n    async def aclose(self) -> None:\n        await to_thread.run_sync(self._file.close)\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        attributes: dict[Any, Callable[[], Any]] = {\n            FileStreamAttribute.file: lambda: self._file,\n        }\n\n        if hasattr(self._file, \"name\"):\n            attributes[FileStreamAttribute.path] = lambda: Path(self._file.name)\n\n        try:\n            self._file.fileno()\n        except UnsupportedOperation:\n            pass\n        else:\n            attributes[FileStreamAttribute.fileno] = lambda: self._file.fileno()\n\n        return attributes\n\n\nclass FileReadStream(_BaseFileStream, ByteReceiveStream):\n    \"\"\"\n    A byte stream that reads from a file in the file system.\n\n    :param file: a file that has been opened for reading in binary mode\n\n    .. versionadded:: 3.0\n    \"\"\"\n\n    @classmethod\n    async def from_path(cls, path: str | PathLike[str]) -> FileReadStream:\n        \"\"\"\n        Create a file read stream by opening the given file.\n\n        :param path: path of the file to read from\n\n        \"\"\"\n        file = await to_thread.run_sync(Path(path).open, \"rb\")\n        return cls(cast(BinaryIO, file))\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        try:\n            data = await to_thread.run_sync(self._file.read, max_bytes)\n        except ValueError:\n            raise ClosedResourceError from None\n        except OSError as exc:\n            raise BrokenResourceError from exc\n\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def seek(self, position: int, whence: int = SEEK_SET) -> int:\n        \"\"\"\n        Seek the file to the given position.\n\n        .. seealso:: :meth:`io.IOBase.seek`\n\n        .. note:: Not all file descriptors are seekable.\n\n        :param position: position to seek the file to\n        :param whence: controls how ``position`` is interpreted\n        :return: the new absolute position\n        :raises OSError: if the file is not seekable\n\n        \"\"\"\n        return await to_thread.run_sync(self._file.seek, position, whence)\n\n    async def tell(self) -> int:\n        \"\"\"\n        Return the current stream position.\n\n        .. note:: Not all file descriptors are seekable.\n\n        :return: the current absolute position\n        :raises OSError: if the file is not seekable\n\n        \"\"\"\n        return await to_thread.run_sync(self._file.tell)\n\n\nclass FileWriteStream(_BaseFileStream, ByteSendStream):\n    \"\"\"\n    A byte stream that writes to a file in the file system.\n\n    :param file: a file that has been opened for writing in binary mode\n\n    .. versionadded:: 3.0\n    \"\"\"\n\n    @classmethod\n    async def from_path(\n        cls, path: str | PathLike[str], append: bool = False\n    ) -> FileWriteStream:\n        \"\"\"\n        Create a file write stream by opening the given file for writing.\n\n        :param path: path of the file to write to\n        :param append: if ``True``, open the file for appending; if ``False``, any\n            existing file at the given path will be truncated\n\n        \"\"\"\n        mode = \"ab\" if append else \"wb\"\n        file = await to_thread.run_sync(Path(path).open, mode)\n        return cls(cast(BinaryIO, file))\n\n    async def send(self, item: bytes) -> None:\n        try:\n            await to_thread.run_sync(self._file.write, item)\n        except ValueError:\n            raise ClosedResourceError from None\n        except OSError as exc:\n            raise BrokenResourceError from exc\n", "src/anyio/streams/memory.py": "from __future__ import annotations\n\nimport warnings\nfrom collections import OrderedDict, deque\nfrom dataclasses import dataclass, field\nfrom types import TracebackType\nfrom typing import Generic, NamedTuple, TypeVar\n\nfrom .. import (\n    BrokenResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    WouldBlock,\n)\nfrom .._core._testing import TaskInfo, get_current_task\nfrom ..abc import Event, ObjectReceiveStream, ObjectSendStream\nfrom ..lowlevel import checkpoint\n\nT_Item = TypeVar(\"T_Item\")\nT_co = TypeVar(\"T_co\", covariant=True)\nT_contra = TypeVar(\"T_contra\", contravariant=True)\n\n\nclass MemoryObjectStreamStatistics(NamedTuple):\n    current_buffer_used: int  #: number of items stored in the buffer\n    #: maximum number of items that can be stored on this stream (or :data:`math.inf`)\n    max_buffer_size: float\n    open_send_streams: int  #: number of unclosed clones of the send stream\n    open_receive_streams: int  #: number of unclosed clones of the receive stream\n    #: number of tasks blocked on :meth:`MemoryObjectSendStream.send`\n    tasks_waiting_send: int\n    #: number of tasks blocked on :meth:`MemoryObjectReceiveStream.receive`\n    tasks_waiting_receive: int\n\n\n@dataclass(eq=False)\nclass MemoryObjectItemReceiver(Generic[T_Item]):\n    task_info: TaskInfo = field(init=False, default_factory=get_current_task)\n    item: T_Item = field(init=False)\n\n\n@dataclass(eq=False)\nclass MemoryObjectStreamState(Generic[T_Item]):\n    max_buffer_size: float = field()\n    buffer: deque[T_Item] = field(init=False, default_factory=deque)\n    open_send_channels: int = field(init=False, default=0)\n    open_receive_channels: int = field(init=False, default=0)\n    waiting_receivers: OrderedDict[Event, MemoryObjectItemReceiver[T_Item]] = field(\n        init=False, default_factory=OrderedDict\n    )\n    waiting_senders: OrderedDict[Event, T_Item] = field(\n        init=False, default_factory=OrderedDict\n    )\n\n    def statistics(self) -> MemoryObjectStreamStatistics:\n        return MemoryObjectStreamStatistics(\n            len(self.buffer),\n            self.max_buffer_size,\n            self.open_send_channels,\n            self.open_receive_channels,\n            len(self.waiting_senders),\n            len(self.waiting_receivers),\n        )\n\n\n@dataclass(eq=False)\nclass MemoryObjectReceiveStream(Generic[T_co], ObjectReceiveStream[T_co]):\n    _state: MemoryObjectStreamState[T_co]\n    _closed: bool = field(init=False, default=False)\n\n    def __post_init__(self) -> None:\n        self._state.open_receive_channels += 1\n\n    def receive_nowait(self) -> T_co:\n        \"\"\"\n        Receive the next item if it can be done without waiting.\n\n        :return: the received item\n        :raises ~anyio.ClosedResourceError: if this send stream has been closed\n        :raises ~anyio.EndOfStream: if the buffer is empty and this stream has been\n            closed from the sending end\n        :raises ~anyio.WouldBlock: if there are no items in the buffer and no tasks\n            waiting to send\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n\n        if self._state.waiting_senders:\n            # Get the item from the next sender\n            send_event, item = self._state.waiting_senders.popitem(last=False)\n            self._state.buffer.append(item)\n            send_event.set()\n\n        if self._state.buffer:\n            return self._state.buffer.popleft()\n        elif not self._state.open_send_channels:\n            raise EndOfStream\n\n        raise WouldBlock\n\n    async def receive(self) -> T_co:\n        await checkpoint()\n        try:\n            return self.receive_nowait()\n        except WouldBlock:\n            # Add ourselves in the queue\n            receive_event = Event()\n            receiver = MemoryObjectItemReceiver[T_co]()\n            self._state.waiting_receivers[receive_event] = receiver\n\n            try:\n                await receive_event.wait()\n            finally:\n                self._state.waiting_receivers.pop(receive_event, None)\n\n            try:\n                return receiver.item\n            except AttributeError:\n                raise EndOfStream\n\n    def clone(self) -> MemoryObjectReceiveStream[T_co]:\n        \"\"\"\n        Create a clone of this receive stream.\n\n        Each clone can be closed separately. Only when all clones have been closed will\n        the receiving end of the memory stream be considered closed by the sending ends.\n\n        :return: the cloned stream\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n\n        return MemoryObjectReceiveStream(_state=self._state)\n\n    def close(self) -> None:\n        \"\"\"\n        Close the stream.\n\n        This works the exact same way as :meth:`aclose`, but is provided as a special\n        case for the benefit of synchronous callbacks.\n\n        \"\"\"\n        if not self._closed:\n            self._closed = True\n            self._state.open_receive_channels -= 1\n            if self._state.open_receive_channels == 0:\n                send_events = list(self._state.waiting_senders.keys())\n                for event in send_events:\n                    event.set()\n\n    async def aclose(self) -> None:\n        self.close()\n\n    def statistics(self) -> MemoryObjectStreamStatistics:\n        \"\"\"\n        Return statistics about the current state of this stream.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return self._state.statistics()\n\n    def __enter__(self) -> MemoryObjectReceiveStream[T_co]:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.close()\n\n    def __del__(self) -> None:\n        if not self._closed:\n            warnings.warn(\n                f\"Unclosed <{self.__class__.__name__} at {id(self):x}>\",\n                ResourceWarning,\n                source=self,\n            )\n\n\n@dataclass(eq=False)\nclass MemoryObjectSendStream(Generic[T_contra], ObjectSendStream[T_contra]):\n    _state: MemoryObjectStreamState[T_contra]\n    _closed: bool = field(init=False, default=False)\n\n    def __post_init__(self) -> None:\n        self._state.open_send_channels += 1\n\n    def send_nowait(self, item: T_contra) -> None:\n        \"\"\"\n        Send an item immediately if it can be done without waiting.\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if this send stream has been closed\n        :raises ~anyio.BrokenResourceError: if the stream has been closed from the\n            receiving end\n        :raises ~anyio.WouldBlock: if the buffer is full and there are no tasks waiting\n            to receive\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n        if not self._state.open_receive_channels:\n            raise BrokenResourceError\n\n        while self._state.waiting_receivers:\n            receive_event, receiver = self._state.waiting_receivers.popitem(last=False)\n            if not receiver.task_info.has_pending_cancellation():\n                receiver.item = item\n                receive_event.set()\n                return\n\n        if len(self._state.buffer) < self._state.max_buffer_size:\n            self._state.buffer.append(item)\n        else:\n            raise WouldBlock\n\n    async def send(self, item: T_contra) -> None:\n        \"\"\"\n        Send an item to the stream.\n\n        If the buffer is full, this method blocks until there is again room in the\n        buffer or the item can be sent directly to a receiver.\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if this send stream has been closed\n        :raises ~anyio.BrokenResourceError: if the stream has been closed from the\n            receiving end\n\n        \"\"\"\n        await checkpoint()\n        try:\n            self.send_nowait(item)\n        except WouldBlock:\n            # Wait until there's someone on the receiving end\n            send_event = Event()\n            self._state.waiting_senders[send_event] = item\n            try:\n                await send_event.wait()\n            except BaseException:\n                self._state.waiting_senders.pop(send_event, None)\n                raise\n\n            if send_event in self._state.waiting_senders:\n                del self._state.waiting_senders[send_event]\n                raise BrokenResourceError from None\n\n    def clone(self) -> MemoryObjectSendStream[T_contra]:\n        \"\"\"\n        Create a clone of this send stream.\n\n        Each clone can be closed separately. Only when all clones have been closed will\n        the sending end of the memory stream be considered closed by the receiving ends.\n\n        :return: the cloned stream\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n\n        return MemoryObjectSendStream(_state=self._state)\n\n    def close(self) -> None:\n        \"\"\"\n        Close the stream.\n\n        This works the exact same way as :meth:`aclose`, but is provided as a special\n        case for the benefit of synchronous callbacks.\n\n        \"\"\"\n        if not self._closed:\n            self._closed = True\n            self._state.open_send_channels -= 1\n            if self._state.open_send_channels == 0:\n                receive_events = list(self._state.waiting_receivers.keys())\n                self._state.waiting_receivers.clear()\n                for event in receive_events:\n                    event.set()\n\n    async def aclose(self) -> None:\n        self.close()\n\n    def statistics(self) -> MemoryObjectStreamStatistics:\n        \"\"\"\n        Return statistics about the current state of this stream.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return self._state.statistics()\n\n    def __enter__(self) -> MemoryObjectSendStream[T_contra]:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.close()\n\n    def __del__(self) -> None:\n        if not self._closed:\n            warnings.warn(\n                f\"Unclosed <{self.__class__.__name__} at {id(self):x}>\",\n                ResourceWarning,\n                source=self,\n            )\n", "src/anyio/streams/tls.py": "from __future__ import annotations\n\nimport logging\nimport re\nimport ssl\nimport sys\nfrom collections.abc import Callable, Mapping\nfrom dataclasses import dataclass\nfrom functools import wraps\nfrom typing import Any, Tuple, TypeVar\n\nfrom .. import (\n    BrokenResourceError,\n    EndOfStream,\n    aclose_forcefully,\n    get_cancelled_exc_class,\n)\nfrom .._core._typedattr import TypedAttributeSet, typed_attribute\nfrom ..abc import AnyByteStream, ByteStream, Listener, TaskGroup\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n_PCTRTT = Tuple[Tuple[str, str], ...]\n_PCTRTTT = Tuple[_PCTRTT, ...]\n\n\nclass TLSAttribute(TypedAttributeSet):\n    \"\"\"Contains Transport Layer Security related attributes.\"\"\"\n\n    #: the selected ALPN protocol\n    alpn_protocol: str | None = typed_attribute()\n    #: the channel binding for type ``tls-unique``\n    channel_binding_tls_unique: bytes = typed_attribute()\n    #: the selected cipher\n    cipher: tuple[str, str, int] = typed_attribute()\n    #: the peer certificate in dictionary form (see :meth:`ssl.SSLSocket.getpeercert`\n    # for more information)\n    peer_certificate: None | (dict[str, str | _PCTRTTT | _PCTRTT]) = typed_attribute()\n    #: the peer certificate in binary form\n    peer_certificate_binary: bytes | None = typed_attribute()\n    #: ``True`` if this is the server side of the connection\n    server_side: bool = typed_attribute()\n    #: ciphers shared by the client during the TLS handshake (``None`` if this is the\n    #: client side)\n    shared_ciphers: list[tuple[str, str, int]] | None = typed_attribute()\n    #: the :class:`~ssl.SSLObject` used for encryption\n    ssl_object: ssl.SSLObject = typed_attribute()\n    #: ``True`` if this stream does (and expects) a closing TLS handshake when the\n    #: stream is being closed\n    standard_compatible: bool = typed_attribute()\n    #: the TLS protocol version (e.g. ``TLSv1.2``)\n    tls_version: str = typed_attribute()\n\n\n@dataclass(eq=False)\nclass TLSStream(ByteStream):\n    \"\"\"\n    A stream wrapper that encrypts all sent data and decrypts received data.\n\n    This class has no public initializer; use :meth:`wrap` instead.\n    All extra attributes from :class:`~TLSAttribute` are supported.\n\n    :var AnyByteStream transport_stream: the wrapped stream\n\n    \"\"\"\n\n    transport_stream: AnyByteStream\n    standard_compatible: bool\n    _ssl_object: ssl.SSLObject\n    _read_bio: ssl.MemoryBIO\n    _write_bio: ssl.MemoryBIO\n\n    @classmethod\n    async def wrap(\n        cls,\n        transport_stream: AnyByteStream,\n        *,\n        server_side: bool | None = None,\n        hostname: str | None = None,\n        ssl_context: ssl.SSLContext | None = None,\n        standard_compatible: bool = True,\n    ) -> TLSStream:\n        \"\"\"\n        Wrap an existing stream with Transport Layer Security.\n\n        This performs a TLS handshake with the peer.\n\n        :param transport_stream: a bytes-transporting stream to wrap\n        :param server_side: ``True`` if this is the server side of the connection,\n            ``False`` if this is the client side (if omitted, will be set to ``False``\n            if ``hostname`` has been provided, ``False`` otherwise). Used only to create\n            a default context when an explicit context has not been provided.\n        :param hostname: host name of the peer (if host name checking is desired)\n        :param ssl_context: the SSLContext object to use (if not provided, a secure\n            default will be created)\n        :param standard_compatible: if ``False``, skip the closing handshake when\n            closing the connection, and don't raise an exception if the peer does the\n            same\n        :raises ~ssl.SSLError: if the TLS handshake fails\n\n        \"\"\"\n        if server_side is None:\n            server_side = not hostname\n\n        if not ssl_context:\n            purpose = (\n                ssl.Purpose.CLIENT_AUTH if server_side else ssl.Purpose.SERVER_AUTH\n            )\n            ssl_context = ssl.create_default_context(purpose)\n\n            # Re-enable detection of unexpected EOFs if it was disabled by Python\n            if hasattr(ssl, \"OP_IGNORE_UNEXPECTED_EOF\"):\n                ssl_context.options &= ~ssl.OP_IGNORE_UNEXPECTED_EOF\n\n        bio_in = ssl.MemoryBIO()\n        bio_out = ssl.MemoryBIO()\n        ssl_object = ssl_context.wrap_bio(\n            bio_in, bio_out, server_side=server_side, server_hostname=hostname\n        )\n        wrapper = cls(\n            transport_stream=transport_stream,\n            standard_compatible=standard_compatible,\n            _ssl_object=ssl_object,\n            _read_bio=bio_in,\n            _write_bio=bio_out,\n        )\n        await wrapper._call_sslobject_method(ssl_object.do_handshake)\n        return wrapper\n\n    async def _call_sslobject_method(\n        self, func: Callable[[Unpack[PosArgsT]], T_Retval], *args: Unpack[PosArgsT]\n    ) -> T_Retval:\n        while True:\n            try:\n                result = func(*args)\n            except ssl.SSLWantReadError:\n                try:\n                    # Flush any pending writes first\n                    if self._write_bio.pending:\n                        await self.transport_stream.send(self._write_bio.read())\n\n                    data = await self.transport_stream.receive()\n                except EndOfStream:\n                    self._read_bio.write_eof()\n                except OSError as exc:\n                    self._read_bio.write_eof()\n                    self._write_bio.write_eof()\n                    raise BrokenResourceError from exc\n                else:\n                    self._read_bio.write(data)\n            except ssl.SSLWantWriteError:\n                await self.transport_stream.send(self._write_bio.read())\n            except ssl.SSLSyscallError as exc:\n                self._read_bio.write_eof()\n                self._write_bio.write_eof()\n                raise BrokenResourceError from exc\n            except ssl.SSLError as exc:\n                self._read_bio.write_eof()\n                self._write_bio.write_eof()\n                if (\n                    isinstance(exc, ssl.SSLEOFError)\n                    or \"UNEXPECTED_EOF_WHILE_READING\" in exc.strerror\n                ):\n                    if self.standard_compatible:\n                        raise BrokenResourceError from exc\n                    else:\n                        raise EndOfStream from None\n\n                raise\n            else:\n                # Flush any pending writes first\n                if self._write_bio.pending:\n                    await self.transport_stream.send(self._write_bio.read())\n\n                return result\n\n    async def unwrap(self) -> tuple[AnyByteStream, bytes]:\n        \"\"\"\n        Does the TLS closing handshake.\n\n        :return: a tuple of (wrapped byte stream, bytes left in the read buffer)\n\n        \"\"\"\n        await self._call_sslobject_method(self._ssl_object.unwrap)\n        self._read_bio.write_eof()\n        self._write_bio.write_eof()\n        return self.transport_stream, self._read_bio.read()\n\n    async def aclose(self) -> None:\n        if self.standard_compatible:\n            try:\n                await self.unwrap()\n            except BaseException:\n                await aclose_forcefully(self.transport_stream)\n                raise\n\n        await self.transport_stream.aclose()\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n        if not data:\n            raise EndOfStream\n\n        return data\n\n    async def send(self, item: bytes) -> None:\n        await self._call_sslobject_method(self._ssl_object.write, item)\n\n    async def send_eof(self) -> None:\n        tls_version = self.extra(TLSAttribute.tls_version)\n        match = re.match(r\"TLSv(\\d+)(?:\\.(\\d+))?\", tls_version)\n        if match:\n            major, minor = int(match.group(1)), int(match.group(2) or 0)\n            if (major, minor) < (1, 3):\n                raise NotImplementedError(\n                    f\"send_eof() requires at least TLSv1.3; current \"\n                    f\"session uses {tls_version}\"\n                )\n\n        raise NotImplementedError(\n            \"send_eof() has not yet been implemented for TLS streams\"\n        )\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self.transport_stream.extra_attributes,\n            TLSAttribute.alpn_protocol: self._ssl_object.selected_alpn_protocol,\n            TLSAttribute.channel_binding_tls_unique: (\n                self._ssl_object.get_channel_binding\n            ),\n            TLSAttribute.cipher: self._ssl_object.cipher,\n            TLSAttribute.peer_certificate: lambda: self._ssl_object.getpeercert(False),\n            TLSAttribute.peer_certificate_binary: lambda: self._ssl_object.getpeercert(\n                True\n            ),\n            TLSAttribute.server_side: lambda: self._ssl_object.server_side,\n            TLSAttribute.shared_ciphers: lambda: self._ssl_object.shared_ciphers()\n            if self._ssl_object.server_side\n            else None,\n            TLSAttribute.standard_compatible: lambda: self.standard_compatible,\n            TLSAttribute.ssl_object: lambda: self._ssl_object,\n            TLSAttribute.tls_version: self._ssl_object.version,\n        }\n\n\n@dataclass(eq=False)\nclass TLSListener(Listener[TLSStream]):\n    \"\"\"\n    A convenience listener that wraps another listener and auto-negotiates a TLS session\n    on every accepted connection.\n\n    If the TLS handshake times out or raises an exception,\n    :meth:`handle_handshake_error` is called to do whatever post-mortem processing is\n    deemed necessary.\n\n    Supports only the :attr:`~TLSAttribute.standard_compatible` extra attribute.\n\n    :param Listener listener: the listener to wrap\n    :param ssl_context: the SSL context object\n    :param standard_compatible: a flag passed through to :meth:`TLSStream.wrap`\n    :param handshake_timeout: time limit for the TLS handshake\n        (passed to :func:`~anyio.fail_after`)\n    \"\"\"\n\n    listener: Listener[Any]\n    ssl_context: ssl.SSLContext\n    standard_compatible: bool = True\n    handshake_timeout: float = 30\n\n    @staticmethod\n    async def handle_handshake_error(exc: BaseException, stream: AnyByteStream) -> None:\n        \"\"\"\n        Handle an exception raised during the TLS handshake.\n\n        This method does 3 things:\n\n        #. Forcefully closes the original stream\n        #. Logs the exception (unless it was a cancellation exception) using the\n           ``anyio.streams.tls`` logger\n        #. Reraises the exception if it was a base exception or a cancellation exception\n\n        :param exc: the exception\n        :param stream: the original stream\n\n        \"\"\"\n        await aclose_forcefully(stream)\n\n        # Log all except cancellation exceptions\n        if not isinstance(exc, get_cancelled_exc_class()):\n            # CPython (as of 3.11.5) returns incorrect `sys.exc_info()` here when using\n            # any asyncio implementation, so we explicitly pass the exception to log\n            # (https://github.com/python/cpython/issues/108668). Trio does not have this\n            # issue because it works around the CPython bug.\n            logging.getLogger(__name__).exception(\n                \"Error during TLS handshake\", exc_info=exc\n            )\n\n        # Only reraise base exceptions and cancellation exceptions\n        if not isinstance(exc, Exception) or isinstance(exc, get_cancelled_exc_class()):\n            raise\n\n    async def serve(\n        self,\n        handler: Callable[[TLSStream], Any],\n        task_group: TaskGroup | None = None,\n    ) -> None:\n        @wraps(handler)\n        async def handler_wrapper(stream: AnyByteStream) -> None:\n            from .. import fail_after\n\n            try:\n                with fail_after(self.handshake_timeout):\n                    wrapped_stream = await TLSStream.wrap(\n                        stream,\n                        ssl_context=self.ssl_context,\n                        standard_compatible=self.standard_compatible,\n                    )\n            except BaseException as exc:\n                await self.handle_handshake_error(exc, stream)\n            else:\n                await handler(wrapped_stream)\n\n        await self.listener.serve(handler_wrapper, task_group)\n\n    async def aclose(self) -> None:\n        await self.listener.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            TLSAttribute.standard_compatible: lambda: self.standard_compatible,\n        }\n", "src/anyio/streams/__init__.py": "", "src/anyio/streams/stapled.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, TypeVar\n\nfrom ..abc import (\n    ByteReceiveStream,\n    ByteSendStream,\n    ByteStream,\n    Listener,\n    ObjectReceiveStream,\n    ObjectSendStream,\n    ObjectStream,\n    TaskGroup,\n)\n\nT_Item = TypeVar(\"T_Item\")\nT_Stream = TypeVar(\"T_Stream\")\n\n\n@dataclass(eq=False)\nclass StapledByteStream(ByteStream):\n    \"\"\"\n    Combines two byte streams into a single, bidirectional byte stream.\n\n    Extra attributes will be provided from both streams, with the receive stream\n    providing the values in case of a conflict.\n\n    :param ByteSendStream send_stream: the sending byte stream\n    :param ByteReceiveStream receive_stream: the receiving byte stream\n    \"\"\"\n\n    send_stream: ByteSendStream\n    receive_stream: ByteReceiveStream\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        return await self.receive_stream.receive(max_bytes)\n\n    async def send(self, item: bytes) -> None:\n        await self.send_stream.send(item)\n\n    async def send_eof(self) -> None:\n        await self.send_stream.aclose()\n\n    async def aclose(self) -> None:\n        await self.send_stream.aclose()\n        await self.receive_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self.send_stream.extra_attributes,\n            **self.receive_stream.extra_attributes,\n        }\n\n\n@dataclass(eq=False)\nclass StapledObjectStream(Generic[T_Item], ObjectStream[T_Item]):\n    \"\"\"\n    Combines two object streams into a single, bidirectional object stream.\n\n    Extra attributes will be provided from both streams, with the receive stream\n    providing the values in case of a conflict.\n\n    :param ObjectSendStream send_stream: the sending object stream\n    :param ObjectReceiveStream receive_stream: the receiving object stream\n    \"\"\"\n\n    send_stream: ObjectSendStream[T_Item]\n    receive_stream: ObjectReceiveStream[T_Item]\n\n    async def receive(self) -> T_Item:\n        return await self.receive_stream.receive()\n\n    async def send(self, item: T_Item) -> None:\n        await self.send_stream.send(item)\n\n    async def send_eof(self) -> None:\n        await self.send_stream.aclose()\n\n    async def aclose(self) -> None:\n        await self.send_stream.aclose()\n        await self.receive_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self.send_stream.extra_attributes,\n            **self.receive_stream.extra_attributes,\n        }\n\n\n@dataclass(eq=False)\nclass MultiListener(Generic[T_Stream], Listener[T_Stream]):\n    \"\"\"\n    Combines multiple listeners into one, serving connections from all of them at once.\n\n    Any MultiListeners in the given collection of listeners will have their listeners\n    moved into this one.\n\n    Extra attributes are provided from each listener, with each successive listener\n    overriding any conflicting attributes from the previous one.\n\n    :param listeners: listeners to serve\n    :type listeners: Sequence[Listener[T_Stream]]\n    \"\"\"\n\n    listeners: Sequence[Listener[T_Stream]]\n\n    def __post_init__(self) -> None:\n        listeners: list[Listener[T_Stream]] = []\n        for listener in self.listeners:\n            if isinstance(listener, MultiListener):\n                listeners.extend(listener.listeners)\n                del listener.listeners[:]  # type: ignore[attr-defined]\n            else:\n                listeners.append(listener)\n\n        self.listeners = listeners\n\n    async def serve(\n        self, handler: Callable[[T_Stream], Any], task_group: TaskGroup | None = None\n    ) -> None:\n        from .. import create_task_group\n\n        async with create_task_group() as tg:\n            for listener in self.listeners:\n                tg.start_soon(listener.serve, handler, task_group)\n\n    async def aclose(self) -> None:\n        for listener in self.listeners:\n            await listener.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        attributes: dict = {}\n        for listener in self.listeners:\n            attributes.update(listener.extra_attributes)\n\n        return attributes\n", "src/anyio/streams/text.py": "from __future__ import annotations\n\nimport codecs\nfrom collections.abc import Callable, Mapping\nfrom dataclasses import InitVar, dataclass, field\nfrom typing import Any\n\nfrom ..abc import (\n    AnyByteReceiveStream,\n    AnyByteSendStream,\n    AnyByteStream,\n    ObjectReceiveStream,\n    ObjectSendStream,\n    ObjectStream,\n)\n\n\n@dataclass(eq=False)\nclass TextReceiveStream(ObjectReceiveStream[str]):\n    \"\"\"\n    Stream wrapper that decodes bytes to strings using the given encoding.\n\n    Decoding is done using :class:`~codecs.IncrementalDecoder` which returns any\n    completely received unicode characters as soon as they come in.\n\n    :param transport_stream: any bytes-based receive stream\n    :param encoding: character encoding to use for decoding bytes to strings (defaults\n        to ``utf-8``)\n    :param errors: handling scheme for decoding errors (defaults to ``strict``; see the\n        `codecs module documentation`_ for a comprehensive list of options)\n\n    .. _codecs module documentation:\n        https://docs.python.org/3/library/codecs.html#codec-objects\n    \"\"\"\n\n    transport_stream: AnyByteReceiveStream\n    encoding: InitVar[str] = \"utf-8\"\n    errors: InitVar[str] = \"strict\"\n    _decoder: codecs.IncrementalDecoder = field(init=False)\n\n    def __post_init__(self, encoding: str, errors: str) -> None:\n        decoder_class = codecs.getincrementaldecoder(encoding)\n        self._decoder = decoder_class(errors=errors)\n\n    async def receive(self) -> str:\n        while True:\n            chunk = await self.transport_stream.receive()\n            decoded = self._decoder.decode(chunk)\n            if decoded:\n                return decoded\n\n    async def aclose(self) -> None:\n        await self.transport_stream.aclose()\n        self._decoder.reset()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return self.transport_stream.extra_attributes\n\n\n@dataclass(eq=False)\nclass TextSendStream(ObjectSendStream[str]):\n    \"\"\"\n    Sends strings to the wrapped stream as bytes using the given encoding.\n\n    :param AnyByteSendStream transport_stream: any bytes-based send stream\n    :param str encoding: character encoding to use for encoding strings to bytes\n        (defaults to ``utf-8``)\n    :param str errors: handling scheme for encoding errors (defaults to ``strict``; see\n        the `codecs module documentation`_ for a comprehensive list of options)\n\n    .. _codecs module documentation:\n        https://docs.python.org/3/library/codecs.html#codec-objects\n    \"\"\"\n\n    transport_stream: AnyByteSendStream\n    encoding: InitVar[str] = \"utf-8\"\n    errors: str = \"strict\"\n    _encoder: Callable[..., tuple[bytes, int]] = field(init=False)\n\n    def __post_init__(self, encoding: str) -> None:\n        self._encoder = codecs.getencoder(encoding)\n\n    async def send(self, item: str) -> None:\n        encoded = self._encoder(item, self.errors)[0]\n        await self.transport_stream.send(encoded)\n\n    async def aclose(self) -> None:\n        await self.transport_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return self.transport_stream.extra_attributes\n\n\n@dataclass(eq=False)\nclass TextStream(ObjectStream[str]):\n    \"\"\"\n    A bidirectional stream that decodes bytes to strings on receive and encodes strings\n    to bytes on send.\n\n    Extra attributes will be provided from both streams, with the receive stream\n    providing the values in case of a conflict.\n\n    :param AnyByteStream transport_stream: any bytes-based stream\n    :param str encoding: character encoding to use for encoding/decoding strings to/from\n        bytes (defaults to ``utf-8``)\n    :param str errors: handling scheme for encoding errors (defaults to ``strict``; see\n        the `codecs module documentation`_ for a comprehensive list of options)\n\n    .. _codecs module documentation:\n        https://docs.python.org/3/library/codecs.html#codec-objects\n    \"\"\"\n\n    transport_stream: AnyByteStream\n    encoding: InitVar[str] = \"utf-8\"\n    errors: InitVar[str] = \"strict\"\n    _receive_stream: TextReceiveStream = field(init=False)\n    _send_stream: TextSendStream = field(init=False)\n\n    def __post_init__(self, encoding: str, errors: str) -> None:\n        self._receive_stream = TextReceiveStream(\n            self.transport_stream, encoding=encoding, errors=errors\n        )\n        self._send_stream = TextSendStream(\n            self.transport_stream, encoding=encoding, errors=errors\n        )\n\n    async def receive(self) -> str:\n        return await self._receive_stream.receive()\n\n    async def send(self, item: str) -> None:\n        await self._send_stream.send(item)\n\n    async def send_eof(self) -> None:\n        await self.transport_stream.send_eof()\n\n    async def aclose(self) -> None:\n        await self._send_stream.aclose()\n        await self._receive_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self._send_stream.extra_attributes,\n            **self._receive_stream.extra_attributes,\n        }\n", "src/anyio/streams/buffered.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nfrom dataclasses import dataclass, field\nfrom typing import Any\n\nfrom .. import ClosedResourceError, DelimiterNotFound, EndOfStream, IncompleteRead\nfrom ..abc import AnyByteReceiveStream, ByteReceiveStream\n\n\n@dataclass(eq=False)\nclass BufferedByteReceiveStream(ByteReceiveStream):\n    \"\"\"\n    Wraps any bytes-based receive stream and uses a buffer to provide sophisticated\n    receiving capabilities in the form of a byte stream.\n    \"\"\"\n\n    receive_stream: AnyByteReceiveStream\n    _buffer: bytearray = field(init=False, default_factory=bytearray)\n    _closed: bool = field(init=False, default=False)\n\n    async def aclose(self) -> None:\n        await self.receive_stream.aclose()\n        self._closed = True\n\n    @property\n    def buffer(self) -> bytes:\n        \"\"\"The bytes currently in the buffer.\"\"\"\n        return bytes(self._buffer)\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return self.receive_stream.extra_attributes\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        if self._closed:\n            raise ClosedResourceError\n\n        if self._buffer:\n            chunk = bytes(self._buffer[:max_bytes])\n            del self._buffer[:max_bytes]\n            return chunk\n        elif isinstance(self.receive_stream, ByteReceiveStream):\n            return await self.receive_stream.receive(max_bytes)\n        else:\n            # With a bytes-oriented object stream, we need to handle any surplus bytes\n            # we get from the receive() call\n            chunk = await self.receive_stream.receive()\n            if len(chunk) > max_bytes:\n                # Save the surplus bytes in the buffer\n                self._buffer.extend(chunk[max_bytes:])\n                return chunk[:max_bytes]\n            else:\n                return chunk\n\n    async def receive_exactly(self, nbytes: int) -> bytes:\n        \"\"\"\n        Read exactly the given amount of bytes from the stream.\n\n        :param nbytes: the number of bytes to read\n        :return: the bytes read\n        :raises ~anyio.IncompleteRead: if the stream was closed before the requested\n            amount of bytes could be read from the stream\n\n        \"\"\"\n        while True:\n            remaining = nbytes - len(self._buffer)\n            if remaining <= 0:\n                retval = self._buffer[:nbytes]\n                del self._buffer[:nbytes]\n                return bytes(retval)\n\n            try:\n                if isinstance(self.receive_stream, ByteReceiveStream):\n                    chunk = await self.receive_stream.receive(remaining)\n                else:\n                    chunk = await self.receive_stream.receive()\n            except EndOfStream as exc:\n                raise IncompleteRead from exc\n\n            self._buffer.extend(chunk)\n\n    async def receive_until(self, delimiter: bytes, max_bytes: int) -> bytes:\n        \"\"\"\n        Read from the stream until the delimiter is found or max_bytes have been read.\n\n        :param delimiter: the marker to look for in the stream\n        :param max_bytes: maximum number of bytes that will be read before raising\n            :exc:`~anyio.DelimiterNotFound`\n        :return: the bytes read (not including the delimiter)\n        :raises ~anyio.IncompleteRead: if the stream was closed before the delimiter\n            was found\n        :raises ~anyio.DelimiterNotFound: if the delimiter is not found within the\n            bytes read up to the maximum allowed\n\n        \"\"\"\n        delimiter_size = len(delimiter)\n        offset = 0\n        while True:\n            # Check if the delimiter can be found in the current buffer\n            index = self._buffer.find(delimiter, offset)\n            if index >= 0:\n                found = self._buffer[:index]\n                del self._buffer[: index + len(delimiter) :]\n                return bytes(found)\n\n            # Check if the buffer is already at or over the limit\n            if len(self._buffer) >= max_bytes:\n                raise DelimiterNotFound(max_bytes)\n\n            # Read more data into the buffer from the socket\n            try:\n                data = await self.receive_stream.receive()\n            except EndOfStream as exc:\n                raise IncompleteRead from exc\n\n            # Move the offset forward and add the new data to the buffer\n            offset = max(len(self._buffer) - delimiter_size + 1, 0)\n            self._buffer.extend(data)\n"}