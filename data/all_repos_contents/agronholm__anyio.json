{"src/anyio/to_process.py": "from __future__ import annotations\n\nimport os\nimport pickle\nimport subprocess\nimport sys\nfrom collections import deque\nfrom collections.abc import Callable\nfrom importlib.util import module_from_spec, spec_from_file_location\nfrom typing import TypeVar, cast\n\nfrom ._core._eventloop import current_time, get_async_backend, get_cancelled_exc_class\nfrom ._core._exceptions import BrokenWorkerProcess\nfrom ._core._subprocesses import open_process\nfrom ._core._synchronization import CapacityLimiter\nfrom ._core._tasks import CancelScope, fail_after\nfrom .abc import ByteReceiveStream, ByteSendStream, Process\nfrom .lowlevel import RunVar, checkpoint_if_cancelled\nfrom .streams.buffered import BufferedByteReceiveStream\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nWORKER_MAX_IDLE_TIME = 300  # 5 minutes\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n_process_pool_workers: RunVar[set[Process]] = RunVar(\"_process_pool_workers\")\n_process_pool_idle_workers: RunVar[deque[tuple[Process, float]]] = RunVar(\n    \"_process_pool_idle_workers\"\n)\n_default_process_limiter: RunVar[CapacityLimiter] = RunVar(\"_default_process_limiter\")\n\n\nasync def run_sync(\n    func: Callable[[Unpack[PosArgsT]], T_Retval],\n    *args: Unpack[PosArgsT],\n    cancellable: bool = False,\n    limiter: CapacityLimiter | None = None,\n) -> T_Retval:\n    \"\"\"\n    Call the given function with the given arguments in a worker process.\n\n    If the ``cancellable`` option is enabled and the task waiting for its completion is\n    cancelled, the worker process running it will be abruptly terminated using SIGKILL\n    (or ``terminateProcess()`` on Windows).\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :param cancellable: ``True`` to allow cancellation of the operation while it's\n        running\n    :param limiter: capacity limiter to use to limit the total amount of processes\n        running (if omitted, the default limiter is used)\n    :return: an awaitable that yields the return value of the function.\n\n    \"\"\"\n\n    async def send_raw_command(pickled_cmd: bytes) -> object:\n        try:\n            await stdin.send(pickled_cmd)\n            response = await buffered.receive_until(b\"\\n\", 50)\n            status, length = response.split(b\" \")\n            if status not in (b\"RETURN\", b\"EXCEPTION\"):\n                raise RuntimeError(\n                    f\"Worker process returned unexpected response: {response!r}\"\n                )\n\n            pickled_response = await buffered.receive_exactly(int(length))\n        except BaseException as exc:\n            workers.discard(process)\n            try:\n                process.kill()\n                with CancelScope(shield=True):\n                    await process.aclose()\n            except ProcessLookupError:\n                pass\n\n            if isinstance(exc, get_cancelled_exc_class()):\n                raise\n            else:\n                raise BrokenWorkerProcess from exc\n\n        retval = pickle.loads(pickled_response)\n        if status == b\"EXCEPTION\":\n            assert isinstance(retval, BaseException)\n            raise retval\n        else:\n            return retval\n\n    # First pickle the request before trying to reserve a worker process\n    await checkpoint_if_cancelled()\n    request = pickle.dumps((\"run\", func, args), protocol=pickle.HIGHEST_PROTOCOL)\n\n    # If this is the first run in this event loop thread, set up the necessary variables\n    try:\n        workers = _process_pool_workers.get()\n        idle_workers = _process_pool_idle_workers.get()\n    except LookupError:\n        workers = set()\n        idle_workers = deque()\n        _process_pool_workers.set(workers)\n        _process_pool_idle_workers.set(idle_workers)\n        get_async_backend().setup_process_pool_exit_at_shutdown(workers)\n\n    async with limiter or current_default_process_limiter():\n        # Pop processes from the pool (starting from the most recently used) until we\n        # find one that hasn't exited yet\n        process: Process\n        while idle_workers:\n            process, idle_since = idle_workers.pop()\n            if process.returncode is None:\n                stdin = cast(ByteSendStream, process.stdin)\n                buffered = BufferedByteReceiveStream(\n                    cast(ByteReceiveStream, process.stdout)\n                )\n\n                # Prune any other workers that have been idle for WORKER_MAX_IDLE_TIME\n                # seconds or longer\n                now = current_time()\n                killed_processes: list[Process] = []\n                while idle_workers:\n                    if now - idle_workers[0][1] < WORKER_MAX_IDLE_TIME:\n                        break\n\n                    process_to_kill, idle_since = idle_workers.popleft()\n                    process_to_kill.kill()\n                    workers.remove(process_to_kill)\n                    killed_processes.append(process_to_kill)\n\n                with CancelScope(shield=True):\n                    for killed_process in killed_processes:\n                        await killed_process.aclose()\n\n                break\n\n            workers.remove(process)\n        else:\n            command = [sys.executable, \"-u\", \"-m\", __name__]\n            process = await open_process(\n                command, stdin=subprocess.PIPE, stdout=subprocess.PIPE\n            )\n            try:\n                stdin = cast(ByteSendStream, process.stdin)\n                buffered = BufferedByteReceiveStream(\n                    cast(ByteReceiveStream, process.stdout)\n                )\n                with fail_after(20):\n                    message = await buffered.receive(6)\n\n                if message != b\"READY\\n\":\n                    raise BrokenWorkerProcess(\n                        f\"Worker process returned unexpected response: {message!r}\"\n                    )\n\n                main_module_path = getattr(sys.modules[\"__main__\"], \"__file__\", None)\n                pickled = pickle.dumps(\n                    (\"init\", sys.path, main_module_path),\n                    protocol=pickle.HIGHEST_PROTOCOL,\n                )\n                await send_raw_command(pickled)\n            except (BrokenWorkerProcess, get_cancelled_exc_class()):\n                raise\n            except BaseException as exc:\n                process.kill()\n                raise BrokenWorkerProcess(\n                    \"Error during worker process initialization\"\n                ) from exc\n\n            workers.add(process)\n\n        with CancelScope(shield=not cancellable):\n            try:\n                return cast(T_Retval, await send_raw_command(request))\n            finally:\n                if process in workers:\n                    idle_workers.append((process, current_time()))\n\n\ndef current_default_process_limiter() -> CapacityLimiter:\n    \"\"\"\n    Return the capacity limiter that is used by default to limit the number of worker\n    processes.\n\n    :return: a capacity limiter object\n\n    \"\"\"\n    try:\n        return _default_process_limiter.get()\n    except LookupError:\n        limiter = CapacityLimiter(os.cpu_count() or 2)\n        _default_process_limiter.set(limiter)\n        return limiter\n\n\ndef process_worker() -> None:\n    # Redirect standard streams to os.devnull so that user code won't interfere with the\n    # parent-worker communication\n    stdin = sys.stdin\n    stdout = sys.stdout\n    sys.stdin = open(os.devnull)\n    sys.stdout = open(os.devnull, \"w\")\n\n    stdout.buffer.write(b\"READY\\n\")\n    while True:\n        retval = exception = None\n        try:\n            command, *args = pickle.load(stdin.buffer)\n        except EOFError:\n            return\n        except BaseException as exc:\n            exception = exc\n        else:\n            if command == \"run\":\n                func, args = args\n                try:\n                    retval = func(*args)\n                except BaseException as exc:\n                    exception = exc\n            elif command == \"init\":\n                main_module_path: str | None\n                sys.path, main_module_path = args\n                del sys.modules[\"__main__\"]\n                if main_module_path and os.path.isfile(main_module_path):\n                    # Load the parent's main module but as __mp_main__ instead of\n                    # __main__ (like multiprocessing does) to avoid infinite recursion\n                    try:\n                        spec = spec_from_file_location(\"__mp_main__\", main_module_path)\n                        if spec and spec.loader:\n                            main = module_from_spec(spec)\n                            spec.loader.exec_module(main)\n                            sys.modules[\"__main__\"] = main\n                    except BaseException as exc:\n                        exception = exc\n        try:\n            if exception is not None:\n                status = b\"EXCEPTION\"\n                pickled = pickle.dumps(exception, pickle.HIGHEST_PROTOCOL)\n            else:\n                status = b\"RETURN\"\n                pickled = pickle.dumps(retval, pickle.HIGHEST_PROTOCOL)\n        except BaseException as exc:\n            exception = exc\n            status = b\"EXCEPTION\"\n            pickled = pickle.dumps(exc, pickle.HIGHEST_PROTOCOL)\n\n        stdout.buffer.write(b\"%s %d\\n\" % (status, len(pickled)))\n        stdout.buffer.write(pickled)\n\n        # Respect SIGTERM\n        if isinstance(exception, SystemExit):\n            raise exception\n\n\nif __name__ == \"__main__\":\n    process_worker()\n", "src/anyio/from_thread.py": "from __future__ import annotations\n\nimport sys\nimport threading\nfrom collections.abc import Awaitable, Callable, Generator\nfrom concurrent.futures import FIRST_COMPLETED, Future, ThreadPoolExecutor, wait\nfrom contextlib import AbstractContextManager, contextmanager\nfrom dataclasses import dataclass, field\nfrom inspect import isawaitable\nfrom types import TracebackType\nfrom typing import (\n    Any,\n    AsyncContextManager,\n    ContextManager,\n    Generic,\n    Iterable,\n    TypeVar,\n    cast,\n    overload,\n)\n\nfrom ._core import _eventloop\nfrom ._core._eventloop import get_async_backend, get_cancelled_exc_class, threadlocals\nfrom ._core._synchronization import Event\nfrom ._core._tasks import CancelScope, create_task_group\nfrom .abc import AsyncBackend\nfrom .abc._tasks import TaskStatus\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nT_Retval = TypeVar(\"T_Retval\")\nT_co = TypeVar(\"T_co\", covariant=True)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\ndef run(\n    func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]], *args: Unpack[PosArgsT]\n) -> T_Retval:\n    \"\"\"\n    Call a coroutine function from a worker thread.\n\n    :param func: a coroutine function\n    :param args: positional arguments for the callable\n    :return: the return value of the coroutine function\n\n    \"\"\"\n    try:\n        async_backend = threadlocals.current_async_backend\n        token = threadlocals.current_token\n    except AttributeError:\n        raise RuntimeError(\n            \"This function can only be run from an AnyIO worker thread\"\n        ) from None\n\n    return async_backend.run_async_from_thread(func, args, token=token)\n\n\ndef run_sync(\n    func: Callable[[Unpack[PosArgsT]], T_Retval], *args: Unpack[PosArgsT]\n) -> T_Retval:\n    \"\"\"\n    Call a function in the event loop thread from a worker thread.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :return: the return value of the callable\n\n    \"\"\"\n    try:\n        async_backend = threadlocals.current_async_backend\n        token = threadlocals.current_token\n    except AttributeError:\n        raise RuntimeError(\n            \"This function can only be run from an AnyIO worker thread\"\n        ) from None\n\n    return async_backend.run_sync_from_thread(func, args, token=token)\n\n\nclass _BlockingAsyncContextManager(Generic[T_co], AbstractContextManager):\n    _enter_future: Future[T_co]\n    _exit_future: Future[bool | None]\n    _exit_event: Event\n    _exit_exc_info: tuple[\n        type[BaseException] | None, BaseException | None, TracebackType | None\n    ] = (None, None, None)\n\n    def __init__(self, async_cm: AsyncContextManager[T_co], portal: BlockingPortal):\n        self._async_cm = async_cm\n        self._portal = portal\n\n    async def run_async_cm(self) -> bool | None:\n        try:\n            self._exit_event = Event()\n            value = await self._async_cm.__aenter__()\n        except BaseException as exc:\n            self._enter_future.set_exception(exc)\n            raise\n        else:\n            self._enter_future.set_result(value)\n\n        try:\n            # Wait for the sync context manager to exit.\n            # This next statement can raise `get_cancelled_exc_class()` if\n            # something went wrong in a task group in this async context\n            # manager.\n            await self._exit_event.wait()\n        finally:\n            # In case of cancellation, it could be that we end up here before\n            # `_BlockingAsyncContextManager.__exit__` is called, and an\n            # `_exit_exc_info` has been set.\n            result = await self._async_cm.__aexit__(*self._exit_exc_info)\n            return result\n\n    def __enter__(self) -> T_co:\n        self._enter_future = Future()\n        self._exit_future = self._portal.start_task_soon(self.run_async_cm)\n        return self._enter_future.result()\n\n    def __exit__(\n        self,\n        __exc_type: type[BaseException] | None,\n        __exc_value: BaseException | None,\n        __traceback: TracebackType | None,\n    ) -> bool | None:\n        self._exit_exc_info = __exc_type, __exc_value, __traceback\n        self._portal.call(self._exit_event.set)\n        return self._exit_future.result()\n\n\nclass _BlockingPortalTaskStatus(TaskStatus):\n    def __init__(self, future: Future):\n        self._future = future\n\n    def started(self, value: object = None) -> None:\n        self._future.set_result(value)\n\n\nclass BlockingPortal:\n    \"\"\"An object that lets external threads run code in an asynchronous event loop.\"\"\"\n\n    def __new__(cls) -> BlockingPortal:\n        return get_async_backend().create_blocking_portal()\n\n    def __init__(self) -> None:\n        self._event_loop_thread_id: int | None = threading.get_ident()\n        self._stop_event = Event()\n        self._task_group = create_task_group()\n        self._cancelled_exc_class = get_cancelled_exc_class()\n\n    async def __aenter__(self) -> BlockingPortal:\n        await self._task_group.__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        await self.stop()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    def _check_running(self) -> None:\n        if self._event_loop_thread_id is None:\n            raise RuntimeError(\"This portal is not running\")\n        if self._event_loop_thread_id == threading.get_ident():\n            raise RuntimeError(\n                \"This method cannot be called from the event loop thread\"\n            )\n\n    async def sleep_until_stopped(self) -> None:\n        \"\"\"Sleep until :meth:`stop` is called.\"\"\"\n        await self._stop_event.wait()\n\n    async def stop(self, cancel_remaining: bool = False) -> None:\n        \"\"\"\n        Signal the portal to shut down.\n\n        This marks the portal as no longer accepting new calls and exits from\n        :meth:`sleep_until_stopped`.\n\n        :param cancel_remaining: ``True`` to cancel all the remaining tasks, ``False``\n            to let them finish before returning\n\n        \"\"\"\n        self._event_loop_thread_id = None\n        self._stop_event.set()\n        if cancel_remaining:\n            self._task_group.cancel_scope.cancel()\n\n    async def _call_func(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        future: Future[T_Retval],\n    ) -> None:\n        def callback(f: Future[T_Retval]) -> None:\n            if f.cancelled() and self._event_loop_thread_id not in (\n                None,\n                threading.get_ident(),\n            ):\n                self.call(scope.cancel)\n\n        try:\n            retval_or_awaitable = func(*args, **kwargs)\n            if isawaitable(retval_or_awaitable):\n                with CancelScope() as scope:\n                    if future.cancelled():\n                        scope.cancel()\n                    else:\n                        future.add_done_callback(callback)\n\n                    retval = await retval_or_awaitable\n            else:\n                retval = retval_or_awaitable\n        except self._cancelled_exc_class:\n            future.cancel()\n            future.set_running_or_notify_cancel()\n        except BaseException as exc:\n            if not future.cancelled():\n                future.set_exception(exc)\n\n            # Let base exceptions fall through\n            if not isinstance(exc, Exception):\n                raise\n        else:\n            if not future.cancelled():\n                future.set_result(retval)\n        finally:\n            scope = None  # type: ignore[assignment]\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future[T_Retval],\n    ) -> None:\n        \"\"\"\n        Spawn a new task using the given callable.\n\n        Implementors must ensure that the future is resolved when the task finishes.\n\n        :param func: a callable\n        :param args: positional arguments to be passed to the callable\n        :param kwargs: keyword arguments to be passed to the callable\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :param future: a future that will resolve to the return value of the callable,\n            or the exception raised during its execution\n\n        \"\"\"\n        raise NotImplementedError\n\n    @overload\n    def call(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        *args: Unpack[PosArgsT],\n    ) -> T_Retval: ...\n\n    @overload\n    def call(\n        self, func: Callable[[Unpack[PosArgsT]], T_Retval], *args: Unpack[PosArgsT]\n    ) -> T_Retval: ...\n\n    def call(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        *args: Unpack[PosArgsT],\n    ) -> T_Retval:\n        \"\"\"\n        Call the given function in the event loop thread.\n\n        If the callable returns a coroutine object, it is awaited on.\n\n        :param func: any callable\n        :raises RuntimeError: if the portal is not running or if this method is called\n            from within the event loop thread\n\n        \"\"\"\n        return cast(T_Retval, self.start_task_soon(func, *args).result())\n\n    @overload\n    def start_task_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> Future[T_Retval]: ...\n\n    @overload\n    def start_task_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> Future[T_Retval]: ...\n\n    def start_task_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> Future[T_Retval]:\n        \"\"\"\n        Start a task in the portal's task group.\n\n        The task will be run inside a cancel scope which can be cancelled by cancelling\n        the returned future.\n\n        :param func: the target function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a future that resolves with the return value of the callable if the\n            task completes successfully, or with the exception raised in the task\n        :raises RuntimeError: if the portal is not running or if this method is called\n            from within the event loop thread\n        :rtype: concurrent.futures.Future[T_Retval]\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        self._check_running()\n        f: Future[T_Retval] = Future()\n        self._spawn_task_from_thread(func, args, {}, name, f)\n        return f\n\n    def start_task(\n        self,\n        func: Callable[..., Awaitable[T_Retval]],\n        *args: object,\n        name: object = None,\n    ) -> tuple[Future[T_Retval], Any]:\n        \"\"\"\n        Start a task in the portal's task group and wait until it signals for readiness.\n\n        This method works the same way as :meth:`.abc.TaskGroup.start`.\n\n        :param func: the target function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a tuple of (future, task_status_value) where the ``task_status_value``\n            is the value passed to ``task_status.started()`` from within the target\n            function\n        :rtype: tuple[concurrent.futures.Future[T_Retval], Any]\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n\n        def task_done(future: Future[T_Retval]) -> None:\n            if not task_status_future.done():\n                if future.cancelled():\n                    task_status_future.cancel()\n                elif future.exception():\n                    task_status_future.set_exception(future.exception())\n                else:\n                    exc = RuntimeError(\n                        \"Task exited without calling task_status.started()\"\n                    )\n                    task_status_future.set_exception(exc)\n\n        self._check_running()\n        task_status_future: Future = Future()\n        task_status = _BlockingPortalTaskStatus(task_status_future)\n        f: Future = Future()\n        f.add_done_callback(task_done)\n        self._spawn_task_from_thread(func, args, {\"task_status\": task_status}, name, f)\n        return f, task_status_future.result()\n\n    def wrap_async_context_manager(\n        self, cm: AsyncContextManager[T_co]\n    ) -> ContextManager[T_co]:\n        \"\"\"\n        Wrap an async context manager as a synchronous context manager via this portal.\n\n        Spawns a task that will call both ``__aenter__()`` and ``__aexit__()``, stopping\n        in the middle until the synchronous context manager exits.\n\n        :param cm: an asynchronous context manager\n        :return: a synchronous context manager\n\n        .. versionadded:: 2.1\n\n        \"\"\"\n        return _BlockingAsyncContextManager(cm, self)\n\n\n@dataclass\nclass BlockingPortalProvider:\n    \"\"\"\n    A manager for a blocking portal. Used as a context manager. The first thread to\n    enter this context manager causes a blocking portal to be started with the specific\n    parameters, and the last thread to exit causes the portal to be shut down. Thus,\n    there will be exactly one blocking portal running in this context as long as at\n    least one thread has entered this context manager.\n\n    The parameters are the same as for :func:`~anyio.run`.\n\n    :param backend: name of the backend\n    :param backend_options: backend options\n\n    .. versionadded:: 4.4\n    \"\"\"\n\n    backend: str = \"asyncio\"\n    backend_options: dict[str, Any] | None = None\n    _lock: threading.Lock = field(init=False, default_factory=threading.Lock)\n    _leases: int = field(init=False, default=0)\n    _portal: BlockingPortal = field(init=False)\n    _portal_cm: AbstractContextManager[BlockingPortal] | None = field(\n        init=False, default=None\n    )\n\n    def __enter__(self) -> BlockingPortal:\n        with self._lock:\n            if self._portal_cm is None:\n                self._portal_cm = start_blocking_portal(\n                    self.backend, self.backend_options\n                )\n                self._portal = self._portal_cm.__enter__()\n\n            self._leases += 1\n            return self._portal\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        portal_cm: AbstractContextManager[BlockingPortal] | None = None\n        with self._lock:\n            assert self._portal_cm\n            assert self._leases > 0\n            self._leases -= 1\n            if not self._leases:\n                portal_cm = self._portal_cm\n                self._portal_cm = None\n                del self._portal\n\n        if portal_cm:\n            portal_cm.__exit__(None, None, None)\n\n\n@contextmanager\ndef start_blocking_portal(\n    backend: str = \"asyncio\", backend_options: dict[str, Any] | None = None\n) -> Generator[BlockingPortal, Any, None]:\n    \"\"\"\n    Start a new event loop in a new thread and run a blocking portal in its main task.\n\n    The parameters are the same as for :func:`~anyio.run`.\n\n    :param backend: name of the backend\n    :param backend_options: backend options\n    :return: a context manager that yields a blocking portal\n\n    .. versionchanged:: 3.0\n        Usage as a context manager is now required.\n\n    \"\"\"\n\n    async def run_portal() -> None:\n        async with BlockingPortal() as portal_:\n            if future.set_running_or_notify_cancel():\n                future.set_result(portal_)\n                await portal_.sleep_until_stopped()\n\n    future: Future[BlockingPortal] = Future()\n    with ThreadPoolExecutor(1) as executor:\n        run_future = executor.submit(\n            _eventloop.run,  # type: ignore[arg-type]\n            run_portal,\n            backend=backend,\n            backend_options=backend_options,\n        )\n        try:\n            wait(\n                cast(Iterable[Future], [run_future, future]),\n                return_when=FIRST_COMPLETED,\n            )\n        except BaseException:\n            future.cancel()\n            run_future.cancel()\n            raise\n\n        if future.done():\n            portal = future.result()\n            cancel_remaining_tasks = False\n            try:\n                yield portal\n            except BaseException:\n                cancel_remaining_tasks = True\n                raise\n            finally:\n                try:\n                    portal.call(portal.stop, cancel_remaining_tasks)\n                except RuntimeError:\n                    pass\n\n        run_future.result()\n\n\ndef check_cancelled() -> None:\n    \"\"\"\n    Check if the cancel scope of the host task's running the current worker thread has\n    been cancelled.\n\n    If the host task's current cancel scope has indeed been cancelled, the\n    backend-specific cancellation exception will be raised.\n\n    :raises RuntimeError: if the current thread was not spawned by\n        :func:`.to_thread.run_sync`\n\n    \"\"\"\n    try:\n        async_backend: AsyncBackend = threadlocals.current_async_backend\n    except AttributeError:\n        raise RuntimeError(\n            \"This function can only be run from an AnyIO worker thread\"\n        ) from None\n\n    async_backend.check_cancelled()\n", "src/anyio/to_thread.py": "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Callable\nfrom typing import TypeVar\nfrom warnings import warn\n\nfrom ._core._eventloop import get_async_backend\nfrom .abc import CapacityLimiter\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\nasync def run_sync(\n    func: Callable[[Unpack[PosArgsT]], T_Retval],\n    *args: Unpack[PosArgsT],\n    abandon_on_cancel: bool = False,\n    cancellable: bool | None = None,\n    limiter: CapacityLimiter | None = None,\n) -> T_Retval:\n    \"\"\"\n    Call the given function with the given arguments in a worker thread.\n\n    If the ``cancellable`` option is enabled and the task waiting for its completion is\n    cancelled, the thread will still run its course but its return value (or any raised\n    exception) will be ignored.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :param abandon_on_cancel: ``True`` to abandon the thread (leaving it to run\n        unchecked on own) if the host task is cancelled, ``False`` to ignore\n        cancellations in the host task until the operation has completed in the worker\n        thread\n    :param cancellable: deprecated alias of ``abandon_on_cancel``; will override\n        ``abandon_on_cancel`` if both parameters are passed\n    :param limiter: capacity limiter to use to limit the total amount of threads running\n        (if omitted, the default limiter is used)\n    :return: an awaitable that yields the return value of the function.\n\n    \"\"\"\n    if cancellable is not None:\n        abandon_on_cancel = cancellable\n        warn(\n            \"The `cancellable=` keyword argument to `anyio.to_thread.run_sync` is \"\n            \"deprecated since AnyIO 4.1.0; use `abandon_on_cancel=` instead\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    return await get_async_backend().run_sync_in_worker_thread(\n        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n    )\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    \"\"\"\n    Return the capacity limiter that is used by default to limit the number of\n    concurrent threads.\n\n    :return: a capacity limiter object\n\n    \"\"\"\n    return get_async_backend().current_default_thread_limiter()\n", "src/anyio/__init__.py": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom ._core._eventloop import current_time as current_time\nfrom ._core._eventloop import get_all_backends as get_all_backends\nfrom ._core._eventloop import get_cancelled_exc_class as get_cancelled_exc_class\nfrom ._core._eventloop import run as run\nfrom ._core._eventloop import sleep as sleep\nfrom ._core._eventloop import sleep_forever as sleep_forever\nfrom ._core._eventloop import sleep_until as sleep_until\nfrom ._core._exceptions import BrokenResourceError as BrokenResourceError\nfrom ._core._exceptions import BrokenWorkerProcess as BrokenWorkerProcess\nfrom ._core._exceptions import BusyResourceError as BusyResourceError\nfrom ._core._exceptions import ClosedResourceError as ClosedResourceError\nfrom ._core._exceptions import DelimiterNotFound as DelimiterNotFound\nfrom ._core._exceptions import EndOfStream as EndOfStream\nfrom ._core._exceptions import IncompleteRead as IncompleteRead\nfrom ._core._exceptions import TypedAttributeLookupError as TypedAttributeLookupError\nfrom ._core._exceptions import WouldBlock as WouldBlock\nfrom ._core._fileio import AsyncFile as AsyncFile\nfrom ._core._fileio import Path as Path\nfrom ._core._fileio import open_file as open_file\nfrom ._core._fileio import wrap_file as wrap_file\nfrom ._core._resources import aclose_forcefully as aclose_forcefully\nfrom ._core._signals import open_signal_receiver as open_signal_receiver\nfrom ._core._sockets import connect_tcp as connect_tcp\nfrom ._core._sockets import connect_unix as connect_unix\nfrom ._core._sockets import create_connected_udp_socket as create_connected_udp_socket\nfrom ._core._sockets import (\n    create_connected_unix_datagram_socket as create_connected_unix_datagram_socket,\n)\nfrom ._core._sockets import create_tcp_listener as create_tcp_listener\nfrom ._core._sockets import create_udp_socket as create_udp_socket\nfrom ._core._sockets import create_unix_datagram_socket as create_unix_datagram_socket\nfrom ._core._sockets import create_unix_listener as create_unix_listener\nfrom ._core._sockets import getaddrinfo as getaddrinfo\nfrom ._core._sockets import getnameinfo as getnameinfo\nfrom ._core._sockets import wait_socket_readable as wait_socket_readable\nfrom ._core._sockets import wait_socket_writable as wait_socket_writable\nfrom ._core._streams import create_memory_object_stream as create_memory_object_stream\nfrom ._core._subprocesses import open_process as open_process\nfrom ._core._subprocesses import run_process as run_process\nfrom ._core._synchronization import CapacityLimiter as CapacityLimiter\nfrom ._core._synchronization import (\n    CapacityLimiterStatistics as CapacityLimiterStatistics,\n)\nfrom ._core._synchronization import Condition as Condition\nfrom ._core._synchronization import ConditionStatistics as ConditionStatistics\nfrom ._core._synchronization import Event as Event\nfrom ._core._synchronization import EventStatistics as EventStatistics\nfrom ._core._synchronization import Lock as Lock\nfrom ._core._synchronization import LockStatistics as LockStatistics\nfrom ._core._synchronization import ResourceGuard as ResourceGuard\nfrom ._core._synchronization import Semaphore as Semaphore\nfrom ._core._synchronization import SemaphoreStatistics as SemaphoreStatistics\nfrom ._core._tasks import TASK_STATUS_IGNORED as TASK_STATUS_IGNORED\nfrom ._core._tasks import CancelScope as CancelScope\nfrom ._core._tasks import create_task_group as create_task_group\nfrom ._core._tasks import current_effective_deadline as current_effective_deadline\nfrom ._core._tasks import fail_after as fail_after\nfrom ._core._tasks import move_on_after as move_on_after\nfrom ._core._testing import TaskInfo as TaskInfo\nfrom ._core._testing import get_current_task as get_current_task\nfrom ._core._testing import get_running_tasks as get_running_tasks\nfrom ._core._testing import wait_all_tasks_blocked as wait_all_tasks_blocked\nfrom ._core._typedattr import TypedAttributeProvider as TypedAttributeProvider\nfrom ._core._typedattr import TypedAttributeSet as TypedAttributeSet\nfrom ._core._typedattr import typed_attribute as typed_attribute\n\n# Re-export imports so they look like they live directly in this package\nkey: str\nvalue: Any\nfor key, value in list(locals().items()):\n    if getattr(value, \"__module__\", \"\").startswith(\"anyio.\"):\n        value.__module__ = __name__\n", "src/anyio/lowlevel.py": "from __future__ import annotations\n\nimport enum\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Literal, TypeVar, overload\nfrom weakref import WeakKeyDictionary\n\nfrom ._core._eventloop import get_async_backend\n\nT = TypeVar(\"T\")\nD = TypeVar(\"D\")\n\n\nasync def checkpoint() -> None:\n    \"\"\"\n    Check for cancellation and allow the scheduler to switch to another task.\n\n    Equivalent to (but more efficient than)::\n\n        await checkpoint_if_cancelled()\n        await cancel_shielded_checkpoint()\n\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().checkpoint()\n\n\nasync def checkpoint_if_cancelled() -> None:\n    \"\"\"\n    Enter a checkpoint if the enclosing cancel scope has been cancelled.\n\n    This does not allow the scheduler to switch to a different task.\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().checkpoint_if_cancelled()\n\n\nasync def cancel_shielded_checkpoint() -> None:\n    \"\"\"\n    Allow the scheduler to switch to another task but without checking for cancellation.\n\n    Equivalent to (but potentially more efficient than)::\n\n        with CancelScope(shield=True):\n            await checkpoint()\n\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().cancel_shielded_checkpoint()\n\n\ndef current_token() -> object:\n    \"\"\"\n    Return a backend specific token object that can be used to get back to the event\n    loop.\n\n    \"\"\"\n    return get_async_backend().current_token()\n\n\n_run_vars: WeakKeyDictionary[Any, dict[str, Any]] = WeakKeyDictionary()\n_token_wrappers: dict[Any, _TokenWrapper] = {}\n\n\n@dataclass(frozen=True)\nclass _TokenWrapper:\n    __slots__ = \"_token\", \"__weakref__\"\n    _token: object\n\n\nclass _NoValueSet(enum.Enum):\n    NO_VALUE_SET = enum.auto()\n\n\nclass RunvarToken(Generic[T]):\n    __slots__ = \"_var\", \"_value\", \"_redeemed\"\n\n    def __init__(self, var: RunVar[T], value: T | Literal[_NoValueSet.NO_VALUE_SET]):\n        self._var = var\n        self._value: T | Literal[_NoValueSet.NO_VALUE_SET] = value\n        self._redeemed = False\n\n\nclass RunVar(Generic[T]):\n    \"\"\"\n    Like a :class:`~contextvars.ContextVar`, except scoped to the running event loop.\n    \"\"\"\n\n    __slots__ = \"_name\", \"_default\"\n\n    NO_VALUE_SET: Literal[_NoValueSet.NO_VALUE_SET] = _NoValueSet.NO_VALUE_SET\n\n    _token_wrappers: set[_TokenWrapper] = set()\n\n    def __init__(\n        self, name: str, default: T | Literal[_NoValueSet.NO_VALUE_SET] = NO_VALUE_SET\n    ):\n        self._name = name\n        self._default = default\n\n    @property\n    def _current_vars(self) -> dict[str, T]:\n        token = current_token()\n        try:\n            return _run_vars[token]\n        except KeyError:\n            run_vars = _run_vars[token] = {}\n            return run_vars\n\n    @overload\n    def get(self, default: D) -> T | D: ...\n\n    @overload\n    def get(self) -> T: ...\n\n    def get(\n        self, default: D | Literal[_NoValueSet.NO_VALUE_SET] = NO_VALUE_SET\n    ) -> T | D:\n        try:\n            return self._current_vars[self._name]\n        except KeyError:\n            if default is not RunVar.NO_VALUE_SET:\n                return default\n            elif self._default is not RunVar.NO_VALUE_SET:\n                return self._default\n\n        raise LookupError(\n            f'Run variable \"{self._name}\" has no value and no default set'\n        )\n\n    def set(self, value: T) -> RunvarToken[T]:\n        current_vars = self._current_vars\n        token = RunvarToken(self, current_vars.get(self._name, RunVar.NO_VALUE_SET))\n        current_vars[self._name] = value\n        return token\n\n    def reset(self, token: RunvarToken[T]) -> None:\n        if token._var is not self:\n            raise ValueError(\"This token does not belong to this RunVar\")\n\n        if token._redeemed:\n            raise ValueError(\"This token has already been used\")\n\n        if token._value is _NoValueSet.NO_VALUE_SET:\n            try:\n                del self._current_vars[self._name]\n            except KeyError:\n                pass\n        else:\n            self._current_vars[self._name] = token._value\n\n        token._redeemed = True\n\n    def __repr__(self) -> str:\n        return f\"<RunVar name={self._name!r}>\"\n", "src/anyio/_backends/_asyncio.py": "from __future__ import annotations\n\nimport array\nimport asyncio\nimport concurrent.futures\nimport math\nimport socket\nimport sys\nimport threading\nimport weakref\nfrom asyncio import (\n    AbstractEventLoop,\n    CancelledError,\n    all_tasks,\n    create_task,\n    current_task,\n    get_running_loop,\n    sleep,\n)\nfrom asyncio.base_events import _run_until_complete_cb  # type: ignore[attr-defined]\nfrom collections import OrderedDict, deque\nfrom collections.abc import AsyncIterator, Generator, Iterable\nfrom concurrent.futures import Future\nfrom contextlib import suppress\nfrom contextvars import Context, copy_context\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom inspect import (\n    CORO_RUNNING,\n    CORO_SUSPENDED,\n    getcoroutinestate,\n    iscoroutine,\n)\nfrom io import IOBase\nfrom os import PathLike\nfrom queue import Queue\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Collection,\n    ContextManager,\n    Coroutine,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    cast,\n)\nfrom weakref import WeakKeyDictionary\n\nimport sniffio\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._eventloop import claim_worker_thread, threadlocals\nfrom .._core._exceptions import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    WouldBlock,\n)\nfrom .._core._sockets import convert_ipv6_sockaddr\nfrom .._core._streams import create_memory_object_stream\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import (\n    AsyncBackend,\n    IPSockAddrType,\n    SocketListener,\n    UDPPacketType,\n    UNIXDatagramPacketType,\n)\nfrom ..lowlevel import RunVar\nfrom ..streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\nif sys.version_info >= (3, 11):\n    from asyncio import Runner\n    from typing import TypeVarTuple, Unpack\nelse:\n    import contextvars\n    import enum\n    import signal\n    from asyncio import coroutines, events, exceptions, tasks\n\n    from exceptiongroup import BaseExceptionGroup\n    from typing_extensions import TypeVarTuple, Unpack\n\n    class _State(enum.Enum):\n        CREATED = \"created\"\n        INITIALIZED = \"initialized\"\n        CLOSED = \"closed\"\n\n    class Runner:\n        # Copied from CPython 3.11\n        def __init__(\n            self,\n            *,\n            debug: bool | None = None,\n            loop_factory: Callable[[], AbstractEventLoop] | None = None,\n        ):\n            self._state = _State.CREATED\n            self._debug = debug\n            self._loop_factory = loop_factory\n            self._loop: AbstractEventLoop | None = None\n            self._context = None\n            self._interrupt_count = 0\n            self._set_event_loop = False\n\n        def __enter__(self) -> Runner:\n            self._lazy_init()\n            return self\n\n        def __exit__(\n            self,\n            exc_type: type[BaseException],\n            exc_val: BaseException,\n            exc_tb: TracebackType,\n        ) -> None:\n            self.close()\n\n        def close(self) -> None:\n            \"\"\"Shutdown and close event loop.\"\"\"\n            if self._state is not _State.INITIALIZED:\n                return\n            try:\n                loop = self._loop\n                _cancel_all_tasks(loop)\n                loop.run_until_complete(loop.shutdown_asyncgens())\n                if hasattr(loop, \"shutdown_default_executor\"):\n                    loop.run_until_complete(loop.shutdown_default_executor())\n                else:\n                    loop.run_until_complete(_shutdown_default_executor(loop))\n            finally:\n                if self._set_event_loop:\n                    events.set_event_loop(None)\n                loop.close()\n                self._loop = None\n                self._state = _State.CLOSED\n\n        def get_loop(self) -> AbstractEventLoop:\n            \"\"\"Return embedded event loop.\"\"\"\n            self._lazy_init()\n            return self._loop\n\n        def run(self, coro: Coroutine[T_Retval], *, context=None) -> T_Retval:\n            \"\"\"Run a coroutine inside the embedded event loop.\"\"\"\n            if not coroutines.iscoroutine(coro):\n                raise ValueError(f\"a coroutine was expected, got {coro!r}\")\n\n            if events._get_running_loop() is not None:\n                # fail fast with short traceback\n                raise RuntimeError(\n                    \"Runner.run() cannot be called from a running event loop\"\n                )\n\n            self._lazy_init()\n\n            if context is None:\n                context = self._context\n            task = context.run(self._loop.create_task, coro)\n\n            if (\n                threading.current_thread() is threading.main_thread()\n                and signal.getsignal(signal.SIGINT) is signal.default_int_handler\n            ):\n                sigint_handler = partial(self._on_sigint, main_task=task)\n                try:\n                    signal.signal(signal.SIGINT, sigint_handler)\n                except ValueError:\n                    # `signal.signal` may throw if `threading.main_thread` does\n                    # not support signals (e.g. embedded interpreter with signals\n                    # not registered - see gh-91880)\n                    sigint_handler = None\n            else:\n                sigint_handler = None\n\n            self._interrupt_count = 0\n            try:\n                return self._loop.run_until_complete(task)\n            except exceptions.CancelledError:\n                if self._interrupt_count > 0:\n                    uncancel = getattr(task, \"uncancel\", None)\n                    if uncancel is not None and uncancel() == 0:\n                        raise KeyboardInterrupt()\n                raise  # CancelledError\n            finally:\n                if (\n                    sigint_handler is not None\n                    and signal.getsignal(signal.SIGINT) is sigint_handler\n                ):\n                    signal.signal(signal.SIGINT, signal.default_int_handler)\n\n        def _lazy_init(self) -> None:\n            if self._state is _State.CLOSED:\n                raise RuntimeError(\"Runner is closed\")\n            if self._state is _State.INITIALIZED:\n                return\n            if self._loop_factory is None:\n                self._loop = events.new_event_loop()\n                if not self._set_event_loop:\n                    # Call set_event_loop only once to avoid calling\n                    # attach_loop multiple times on child watchers\n                    events.set_event_loop(self._loop)\n                    self._set_event_loop = True\n            else:\n                self._loop = self._loop_factory()\n            if self._debug is not None:\n                self._loop.set_debug(self._debug)\n            self._context = contextvars.copy_context()\n            self._state = _State.INITIALIZED\n\n        def _on_sigint(self, signum, frame, main_task: asyncio.Task) -> None:\n            self._interrupt_count += 1\n            if self._interrupt_count == 1 and not main_task.done():\n                main_task.cancel()\n                # wakeup loop if it is blocked by select() with long timeout\n                self._loop.call_soon_threadsafe(lambda: None)\n                return\n            raise KeyboardInterrupt()\n\n    def _cancel_all_tasks(loop: AbstractEventLoop) -> None:\n        to_cancel = tasks.all_tasks(loop)\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler(\n                    {\n                        \"message\": \"unhandled exception during asyncio.run() shutdown\",\n                        \"exception\": task.exception(),\n                        \"task\": task,\n                    }\n                )\n\n    async def _shutdown_default_executor(loop: AbstractEventLoop) -> None:\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n\n        def _do_shutdown(future: asyncio.futures.Future) -> None:\n            try:\n                loop._default_executor.shutdown(wait=True)  # type: ignore[attr-defined]\n                loop.call_soon_threadsafe(future.set_result, None)\n            except Exception as ex:\n                loop.call_soon_threadsafe(future.set_exception, ex)\n\n        loop._executor_shutdown_called = True\n        if loop._default_executor is None:\n            return\n        future = loop.create_future()\n        thread = threading.Thread(target=_do_shutdown, args=(future,))\n        thread.start()\n        try:\n            await future\n        finally:\n            thread.join()\n\n\nT_Retval = TypeVar(\"T_Retval\")\nT_contra = TypeVar(\"T_contra\", contravariant=True)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\nP = ParamSpec(\"P\")\n\n_root_task: RunVar[asyncio.Task | None] = RunVar(\"_root_task\")\n\n\ndef find_root_task() -> asyncio.Task:\n    root_task = _root_task.get(None)\n    if root_task is not None and not root_task.done():\n        return root_task\n\n    # Look for a task that has been started via run_until_complete()\n    for task in all_tasks():\n        if task._callbacks and not task.done():\n            callbacks = [cb for cb, context in task._callbacks]\n            for cb in callbacks:\n                if (\n                    cb is _run_until_complete_cb\n                    or getattr(cb, \"__module__\", None) == \"uvloop.loop\"\n                ):\n                    _root_task.set(task)\n                    return task\n\n    # Look up the topmost task in the AnyIO task tree, if possible\n    task = cast(asyncio.Task, current_task())\n    state = _task_states.get(task)\n    if state:\n        cancel_scope = state.cancel_scope\n        while cancel_scope and cancel_scope._parent_scope is not None:\n            cancel_scope = cancel_scope._parent_scope\n\n        if cancel_scope is not None:\n            return cast(asyncio.Task, cancel_scope._host_task)\n\n    return task\n\n\ndef get_callable_name(func: Callable) -> str:\n    module = getattr(func, \"__module__\", None)\n    qualname = getattr(func, \"__qualname__\", None)\n    return \".\".join([x for x in (module, qualname) if x])\n\n\n#\n# Event loop\n#\n\n_run_vars: WeakKeyDictionary[asyncio.AbstractEventLoop, Any] = WeakKeyDictionary()\n\n\ndef _task_started(task: asyncio.Task) -> bool:\n    \"\"\"Return ``True`` if the task has been started and has not finished.\"\"\"\n    try:\n        return getcoroutinestate(task.get_coro()) in (CORO_RUNNING, CORO_SUSPENDED)\n    except AttributeError:\n        # task coro is async_genenerator_asend https://bugs.python.org/issue37771\n        raise Exception(f\"Cannot determine if task {task} has started or not\") from None\n\n\n#\n# Timeouts and cancellation\n#\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return object.__new__(cls)\n\n    def __init__(self, deadline: float = math.inf, shield: bool = False):\n        self._deadline = deadline\n        self._shield = shield\n        self._parent_scope: CancelScope | None = None\n        self._child_scopes: set[CancelScope] = set()\n        self._cancel_called = False\n        self._cancelled_caught = False\n        self._active = False\n        self._timeout_handle: asyncio.TimerHandle | None = None\n        self._cancel_handle: asyncio.Handle | None = None\n        self._tasks: set[asyncio.Task] = set()\n        self._host_task: asyncio.Task | None = None\n        self._cancel_calls: int = 0\n        self._cancelling: int | None = None\n\n    def __enter__(self) -> CancelScope:\n        if self._active:\n            raise RuntimeError(\n                \"Each CancelScope may only be used for a single 'with' block\"\n            )\n\n        self._host_task = host_task = cast(asyncio.Task, current_task())\n        self._tasks.add(host_task)\n        try:\n            task_state = _task_states[host_task]\n        except KeyError:\n            task_state = TaskState(None, self)\n            _task_states[host_task] = task_state\n        else:\n            self._parent_scope = task_state.cancel_scope\n            task_state.cancel_scope = self\n            if self._parent_scope is not None:\n                self._parent_scope._child_scopes.add(self)\n                self._parent_scope._tasks.remove(host_task)\n\n        self._timeout()\n        self._active = True\n        if sys.version_info >= (3, 11):\n            self._cancelling = self._host_task.cancelling()\n\n        # Start cancelling the host task if the scope was cancelled before entering\n        if self._cancel_called:\n            self._deliver_cancellation(self)\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        if not self._active:\n            raise RuntimeError(\"This cancel scope is not active\")\n        if current_task() is not self._host_task:\n            raise RuntimeError(\n                \"Attempted to exit cancel scope in a different task than it was \"\n                \"entered in\"\n            )\n\n        assert self._host_task is not None\n        host_task_state = _task_states.get(self._host_task)\n        if host_task_state is None or host_task_state.cancel_scope is not self:\n            raise RuntimeError(\n                \"Attempted to exit a cancel scope that isn't the current tasks's \"\n                \"current cancel scope\"\n            )\n\n        self._active = False\n        if self._timeout_handle:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        self._tasks.remove(self._host_task)\n        if self._parent_scope is not None:\n            self._parent_scope._child_scopes.remove(self)\n            self._parent_scope._tasks.add(self._host_task)\n\n        host_task_state.cancel_scope = self._parent_scope\n\n        # Restart the cancellation effort in the closest directly cancelled parent\n        # scope if this one was shielded\n        self._restart_cancellation_in_parent()\n\n        if self._cancel_called and exc_val is not None:\n            for exc in iterate_exceptions(exc_val):\n                if isinstance(exc, CancelledError):\n                    self._cancelled_caught = self._uncancel(exc)\n                    if self._cancelled_caught:\n                        break\n\n            return self._cancelled_caught\n\n        return None\n\n    def _uncancel(self, cancelled_exc: CancelledError) -> bool:\n        if sys.version_info < (3, 9) or self._host_task is None:\n            self._cancel_calls = 0\n            return True\n\n        # Undo all cancellations done by this scope\n        if self._cancelling is not None:\n            while self._cancel_calls:\n                self._cancel_calls -= 1\n                if self._host_task.uncancel() <= self._cancelling:\n                    return True\n\n        self._cancel_calls = 0\n        return f\"Cancelled by cancel scope {id(self):x}\" in cancelled_exc.args\n\n    def _timeout(self) -> None:\n        if self._deadline != math.inf:\n            loop = get_running_loop()\n            if loop.time() >= self._deadline:\n                self.cancel()\n            else:\n                self._timeout_handle = loop.call_at(self._deadline, self._timeout)\n\n    def _deliver_cancellation(self, origin: CancelScope) -> bool:\n        \"\"\"\n        Deliver cancellation to directly contained tasks and nested cancel scopes.\n\n        Schedule another run at the end if we still have tasks eligible for\n        cancellation.\n\n        :param origin: the cancel scope that originated the cancellation\n        :return: ``True`` if the delivery needs to be retried on the next cycle\n\n        \"\"\"\n        should_retry = False\n        current = current_task()\n        for task in self._tasks:\n            if task._must_cancel:  # type: ignore[attr-defined]\n                continue\n\n            # The task is eligible for cancellation if it has started\n            should_retry = True\n            if task is not current and (task is self._host_task or _task_started(task)):\n                waiter = task._fut_waiter  # type: ignore[attr-defined]\n                if not isinstance(waiter, asyncio.Future) or not waiter.done():\n                    origin._cancel_calls += 1\n                    if sys.version_info >= (3, 9):\n                        task.cancel(f\"Cancelled by cancel scope {id(origin):x}\")\n                    else:\n                        task.cancel()\n\n        # Deliver cancellation to child scopes that aren't shielded or running their own\n        # cancellation callbacks\n        for scope in self._child_scopes:\n            if not scope._shield and not scope.cancel_called:\n                should_retry = scope._deliver_cancellation(origin) or should_retry\n\n        # Schedule another callback if there are still tasks left\n        if origin is self:\n            if should_retry:\n                self._cancel_handle = get_running_loop().call_soon(\n                    self._deliver_cancellation, origin\n                )\n            else:\n                self._cancel_handle = None\n\n        return should_retry\n\n    def _restart_cancellation_in_parent(self) -> None:\n        \"\"\"\n        Restart the cancellation effort in the closest directly cancelled parent scope.\n\n        \"\"\"\n        scope = self._parent_scope\n        while scope is not None:\n            if scope._cancel_called:\n                if scope._cancel_handle is None:\n                    scope._deliver_cancellation(scope)\n\n                break\n\n            # No point in looking beyond any shielded scope\n            if scope._shield:\n                break\n\n            scope = scope._parent_scope\n\n    def _parent_cancelled(self) -> bool:\n        # Check whether any parent has been cancelled\n        cancel_scope = self._parent_scope\n        while cancel_scope is not None and not cancel_scope._shield:\n            if cancel_scope._cancel_called:\n                return True\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return False\n\n    def cancel(self) -> None:\n        if not self._cancel_called:\n            if self._timeout_handle:\n                self._timeout_handle.cancel()\n                self._timeout_handle = None\n\n            self._cancel_called = True\n            if self._host_task is not None:\n                self._deliver_cancellation(self)\n\n    @property\n    def deadline(self) -> float:\n        return self._deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self._deadline = float(value)\n        if self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        if self._active and not self._cancel_called:\n            self._timeout()\n\n    @property\n    def cancel_called(self) -> bool:\n        return self._cancel_called\n\n    @property\n    def cancelled_caught(self) -> bool:\n        return self._cancelled_caught\n\n    @property\n    def shield(self) -> bool:\n        return self._shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        if self._shield != value:\n            self._shield = value\n            if not value:\n                self._restart_cancellation_in_parent()\n\n\n#\n# Task states\n#\n\n\nclass TaskState:\n    \"\"\"\n    Encapsulates auxiliary task information that cannot be added to the Task instance\n    itself because there are no guarantees about its implementation.\n    \"\"\"\n\n    __slots__ = \"parent_id\", \"cancel_scope\", \"__weakref__\"\n\n    def __init__(self, parent_id: int | None, cancel_scope: CancelScope | None):\n        self.parent_id = parent_id\n        self.cancel_scope = cancel_scope\n\n\n_task_states: WeakKeyDictionary[asyncio.Task, TaskState] = WeakKeyDictionary()\n\n\n#\n# Task groups\n#\n\n\nclass _AsyncioTaskStatus(abc.TaskStatus):\n    def __init__(self, future: asyncio.Future, parent_id: int):\n        self._future = future\n        self._parent_id = parent_id\n\n    def started(self, value: T_contra | None = None) -> None:\n        try:\n            self._future.set_result(value)\n        except asyncio.InvalidStateError:\n            if not self._future.cancelled():\n                raise RuntimeError(\n                    \"called 'started' twice on the same task status\"\n                ) from None\n\n        task = cast(asyncio.Task, current_task())\n        _task_states[task].parent_id = self._parent_id\n\n\ndef iterate_exceptions(\n    exception: BaseException,\n) -> Generator[BaseException, None, None]:\n    if isinstance(exception, BaseExceptionGroup):\n        for exc in exception.exceptions:\n            yield from iterate_exceptions(exc)\n    else:\n        yield exception\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self.cancel_scope: CancelScope = CancelScope()\n        self._active = False\n        self._exceptions: list[BaseException] = []\n        self._tasks: set[asyncio.Task] = set()\n\n    async def __aenter__(self) -> TaskGroup:\n        self.cancel_scope.__enter__()\n        self._active = True\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        ignore_exception = self.cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n        if exc_val is not None:\n            self.cancel_scope.cancel()\n            if not isinstance(exc_val, CancelledError):\n                self._exceptions.append(exc_val)\n\n        cancelled_exc_while_waiting_tasks: CancelledError | None = None\n        while self._tasks:\n            try:\n                await asyncio.wait(self._tasks)\n            except CancelledError as exc:\n                # This task was cancelled natively; reraise the CancelledError later\n                # unless this task was already interrupted by another exception\n                self.cancel_scope.cancel()\n                if cancelled_exc_while_waiting_tasks is None:\n                    cancelled_exc_while_waiting_tasks = exc\n\n        self._active = False\n        if self._exceptions:\n            raise BaseExceptionGroup(\n                \"unhandled errors in a TaskGroup\", self._exceptions\n            )\n\n        # Raise the CancelledError received while waiting for child tasks to exit,\n        # unless the context manager itself was previously exited with another\n        # exception, or if any of the  child tasks raised an exception other than\n        # CancelledError\n        if cancelled_exc_while_waiting_tasks:\n            if exc_val is None or ignore_exception:\n                raise cancelled_exc_while_waiting_tasks\n\n        return ignore_exception\n\n    def _spawn(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        args: tuple[Unpack[PosArgsT]],\n        name: object,\n        task_status_future: asyncio.Future | None = None,\n    ) -> asyncio.Task:\n        def task_done(_task: asyncio.Task) -> None:\n            task_state = _task_states[_task]\n            assert task_state.cancel_scope is not None\n            assert _task in task_state.cancel_scope._tasks\n            task_state.cancel_scope._tasks.remove(_task)\n            self._tasks.remove(task)\n            del _task_states[_task]\n\n            try:\n                exc = _task.exception()\n            except CancelledError as e:\n                while isinstance(e.__context__, CancelledError):\n                    e = e.__context__\n\n                exc = e\n\n            if exc is not None:\n                # The future can only be in the cancelled state if the host task was\n                # cancelled, so return immediately instead of adding one more\n                # CancelledError to the exceptions list\n                if task_status_future is not None and task_status_future.cancelled():\n                    return\n\n                if task_status_future is None or task_status_future.done():\n                    if not isinstance(exc, CancelledError):\n                        self._exceptions.append(exc)\n\n                    if not self.cancel_scope._parent_cancelled():\n                        self.cancel_scope.cancel()\n                else:\n                    task_status_future.set_exception(exc)\n            elif task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError(\"Child exited without calling task_status.started()\")\n                )\n\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        kwargs = {}\n        if task_status_future:\n            parent_id = id(current_task())\n            kwargs[\"task_status\"] = _AsyncioTaskStatus(\n                task_status_future, id(self.cancel_scope._host_task)\n            )\n        else:\n            parent_id = id(self.cancel_scope._host_task)\n\n        coro = func(*args, **kwargs)\n        if not iscoroutine(coro):\n            prefix = f\"{func.__module__}.\" if hasattr(func, \"__module__\") else \"\"\n            raise TypeError(\n                f\"Expected {prefix}{func.__qualname__}() to return a coroutine, but \"\n                f\"the return value ({coro!r}) is not a coroutine object\"\n            )\n\n        name = get_callable_name(func) if name is None else str(name)\n        task = create_task(coro, name=name)\n        task.add_done_callback(task_done)\n\n        # Make the spawned task inherit the task group's cancel scope\n        _task_states[task] = TaskState(\n            parent_id=parent_id, cancel_scope=self.cancel_scope\n        )\n        self.cancel_scope._tasks.add(task)\n        self._tasks.add(task)\n        return task\n\n    def start_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> None:\n        self._spawn(func, args, name)\n\n    async def start(\n        self, func: Callable[..., Awaitable[Any]], *args: object, name: object = None\n    ) -> Any:\n        future: asyncio.Future = asyncio.Future()\n        task = self._spawn(func, args, name, future)\n\n        # If the task raises an exception after sending a start value without a switch\n        # point between, the task group is cancelled and this method never proceeds to\n        # process the completed future. That's why we have to have a shielded cancel\n        # scope here.\n        try:\n            return await future\n        except CancelledError:\n            # Cancel the task and wait for it to exit before returning\n            task.cancel()\n            with CancelScope(shield=True), suppress(CancelledError):\n                await task\n\n            raise\n\n\n#\n# Threads\n#\n\n_Retval_Queue_Type = Tuple[Optional[T_Retval], Optional[BaseException]]\n\n\nclass WorkerThread(Thread):\n    MAX_IDLE_TIME = 10  # seconds\n\n    def __init__(\n        self,\n        root_task: asyncio.Task,\n        workers: set[WorkerThread],\n        idle_workers: deque[WorkerThread],\n    ):\n        super().__init__(name=\"AnyIO worker thread\")\n        self.root_task = root_task\n        self.workers = workers\n        self.idle_workers = idle_workers\n        self.loop = root_task._loop\n        self.queue: Queue[\n            tuple[Context, Callable, tuple, asyncio.Future, CancelScope] | None\n        ] = Queue(2)\n        self.idle_since = AsyncIOBackend.current_time()\n        self.stopping = False\n\n    def _report_result(\n        self, future: asyncio.Future, result: Any, exc: BaseException | None\n    ) -> None:\n        self.idle_since = AsyncIOBackend.current_time()\n        if not self.stopping:\n            self.idle_workers.append(self)\n\n        if not future.cancelled():\n            if exc is not None:\n                if isinstance(exc, StopIteration):\n                    new_exc = RuntimeError(\"coroutine raised StopIteration\")\n                    new_exc.__cause__ = exc\n                    exc = new_exc\n\n                future.set_exception(exc)\n            else:\n                future.set_result(result)\n\n    def run(self) -> None:\n        with claim_worker_thread(AsyncIOBackend, self.loop):\n            while True:\n                item = self.queue.get()\n                if item is None:\n                    # Shutdown command received\n                    return\n\n                context, func, args, future, cancel_scope = item\n                if not future.cancelled():\n                    result = None\n                    exception: BaseException | None = None\n                    threadlocals.current_cancel_scope = cancel_scope\n                    try:\n                        result = context.run(func, *args)\n                    except BaseException as exc:\n                        exception = exc\n                    finally:\n                        del threadlocals.current_cancel_scope\n\n                    if not self.loop.is_closed():\n                        self.loop.call_soon_threadsafe(\n                            self._report_result, future, result, exception\n                        )\n\n                self.queue.task_done()\n\n    def stop(self, f: asyncio.Task | None = None) -> None:\n        self.stopping = True\n        self.queue.put_nowait(None)\n        self.workers.discard(self)\n        try:\n            self.idle_workers.remove(self)\n        except ValueError:\n            pass\n\n\n_threadpool_idle_workers: RunVar[deque[WorkerThread]] = RunVar(\n    \"_threadpool_idle_workers\"\n)\n_threadpool_workers: RunVar[set[WorkerThread]] = RunVar(\"_threadpool_workers\")\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> BlockingPortal:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._loop = get_running_loop()\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future[T_Retval],\n    ) -> None:\n        AsyncIOBackend.run_sync_from_thread(\n            partial(self._task_group.start_soon, name=name),\n            (self._call_func, func, args, kwargs, future),\n            self._loop,\n        )\n\n\n#\n# Subprocesses\n#\n\n\n@dataclass(eq=False)\nclass StreamReaderWrapper(abc.ByteReceiveStream):\n    _stream: asyncio.StreamReader\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._stream.read(max_bytes)\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        self._stream.feed_eof()\n        await AsyncIOBackend.checkpoint()\n\n\n@dataclass(eq=False)\nclass StreamWriterWrapper(abc.ByteSendStream):\n    _stream: asyncio.StreamWriter\n\n    async def send(self, item: bytes) -> None:\n        self._stream.write(item)\n        await self._stream.drain()\n\n    async def aclose(self) -> None:\n        self._stream.close()\n        await AsyncIOBackend.checkpoint()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: asyncio.subprocess.Process\n    _stdin: StreamWriterWrapper | None\n    _stdout: StreamReaderWrapper | None\n    _stderr: StreamReaderWrapper | None\n\n    async def aclose(self) -> None:\n        with CancelScope(shield=True):\n            if self._stdin:\n                await self._stdin.aclose()\n            if self._stdout:\n                await self._stdout.aclose()\n            if self._stderr:\n                await self._stderr.aclose()\n\n        try:\n            await self.wait()\n        except BaseException:\n            self.kill()\n            with CancelScope(shield=True):\n                await self.wait()\n\n            raise\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: int) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> int | None:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> abc.ByteSendStream | None:\n        return self._stdin\n\n    @property\n    def stdout(self) -> abc.ByteReceiveStream | None:\n        return self._stdout\n\n    @property\n    def stderr(self) -> abc.ByteReceiveStream | None:\n        return self._stderr\n\n\ndef _forcibly_shutdown_process_pool_on_exit(\n    workers: set[Process], _task: object\n) -> None:\n    \"\"\"\n    Forcibly shuts down worker processes belonging to this event loop.\"\"\"\n    child_watcher: asyncio.AbstractChildWatcher | None = None\n    if sys.version_info < (3, 12):\n        try:\n            child_watcher = asyncio.get_event_loop_policy().get_child_watcher()\n        except NotImplementedError:\n            pass\n\n    # Close as much as possible (w/o async/await) to avoid warnings\n    for process in workers:\n        if process.returncode is None:\n            continue\n\n        process._stdin._stream._transport.close()  # type: ignore[union-attr]\n        process._stdout._stream._transport.close()  # type: ignore[union-attr]\n        process._stderr._stream._transport.close()  # type: ignore[union-attr]\n        process.kill()\n        if child_watcher:\n            child_watcher.remove_child_handler(process.pid)\n\n\nasync def _shutdown_process_pool_on_exit(workers: set[abc.Process]) -> None:\n    \"\"\"\n    Shuts down worker processes belonging to this event loop.\n\n    NOTE: this only works when the event loop was started using asyncio.run() or\n    anyio.run().\n\n    \"\"\"\n    process: abc.Process\n    try:\n        await sleep(math.inf)\n    except asyncio.CancelledError:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        for process in workers:\n            await process.aclose()\n\n\n#\n# Sockets and networking\n#\n\n\nclass StreamProtocol(asyncio.Protocol):\n    read_queue: deque[bytes]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Exception | None = None\n    is_at_eof: bool = False\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque()\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n        cast(asyncio.Transport, transport).set_write_buffer_limits(0)\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        if exc:\n            self.exception = BrokenResourceError()\n            self.exception.__cause__ = exc\n\n        self.read_event.set()\n        self.write_event.set()\n\n    def data_received(self, data: bytes) -> None:\n        self.read_queue.append(data)\n        self.read_event.set()\n\n    def eof_received(self) -> bool | None:\n        self.is_at_eof = True\n        self.read_event.set()\n        return True\n\n    def pause_writing(self) -> None:\n        self.write_event = asyncio.Event()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass DatagramProtocol(asyncio.DatagramProtocol):\n    read_queue: deque[tuple[bytes, IPSockAddrType]]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Exception | None = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque(maxlen=100)  # arbitrary value\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        self.read_event.set()\n        self.write_event.set()\n\n    def datagram_received(self, data: bytes, addr: IPSockAddrType) -> None:\n        addr = convert_ipv6_sockaddr(addr)\n        self.read_queue.append((data, addr))\n        self.read_event.set()\n\n    def error_received(self, exc: Exception) -> None:\n        self.exception = exc\n\n    def pause_writing(self) -> None:\n        self.write_event.clear()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass SocketStream(abc.SocketStream):\n    def __init__(self, transport: asyncio.Transport, protocol: StreamProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            if (\n                not self._protocol.read_event.is_set()\n                and not self._transport.is_closing()\n                and not self._protocol.is_at_eof\n            ):\n                self._transport.resume_reading()\n                await self._protocol.read_event.wait()\n                self._transport.pause_reading()\n            else:\n                await AsyncIOBackend.checkpoint()\n\n            try:\n                chunk = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                elif self._protocol.exception:\n                    raise self._protocol.exception from None\n                else:\n                    raise EndOfStream from None\n\n            if len(chunk) > max_bytes:\n                # Split the oversized chunk\n                chunk, leftover = chunk[:max_bytes], chunk[max_bytes:]\n                self._protocol.read_queue.appendleft(leftover)\n\n            # If the read queue is empty, clear the flag so that the next call will\n            # block until data is available\n            if not self._protocol.read_queue:\n                self._protocol.read_event.clear()\n\n        return chunk\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n\n            if self._closed:\n                raise ClosedResourceError\n            elif self._protocol.exception is not None:\n                raise self._protocol.exception\n\n            try:\n                self._transport.write(item)\n            except RuntimeError as exc:\n                if self._transport.is_closing():\n                    raise BrokenResourceError from exc\n                else:\n                    raise\n\n            await self._protocol.write_event.wait()\n\n    async def send_eof(self) -> None:\n        try:\n            self._transport.write_eof()\n        except OSError:\n            pass\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            try:\n                self._transport.write_eof()\n            except OSError:\n                pass\n\n            self._transport.close()\n            await sleep(0)\n            self._transport.abort()\n\n\nclass _RawSocketMixin:\n    _receive_future: asyncio.Future | None = None\n    _send_future: asyncio.Future | None = None\n    _closing = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    def _wait_until_readable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._receive_future\n            loop.remove_reader(self.__raw_socket)\n\n        f = self._receive_future = asyncio.Future()\n        loop.add_reader(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    def _wait_until_writable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._send_future\n            loop.remove_writer(self.__raw_socket)\n\n        f = self._send_future = asyncio.Future()\n        loop.add_writer(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    async def aclose(self) -> None:\n        if not self._closing:\n            self._closing = True\n            if self.__raw_socket.fileno() != -1:\n                self.__raw_socket.close()\n\n            if self._receive_future:\n                self._receive_future.set_result(None)\n            if self._send_future:\n                self._send_future.set_result(None)\n\n\nclass UNIXSocketStream(_RawSocketMixin, abc.UNIXSocketStream):\n    async def send_eof(self) -> None:\n        with self._send_guard:\n            self._raw_socket.shutdown(socket.SHUT_WR)\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recv(max_bytes)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not data:\n                        raise EndOfStream\n\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = self._raw_socket.send(view)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    view = view[bytes_sent:]\n\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError(\"msglen must be a non-negative integer\")\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError(\"maxfds must be a positive integer\")\n\n        loop = get_running_loop()\n        fds = array.array(\"i\")\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = self._raw_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize)\n                    )\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(\n                    f\"Received unexpected ancillary data; message = {message!r}, \"\n                    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n                )\n\n            fds.frombytes(cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        if not message:\n            raise ValueError(\"message must not be empty\")\n        if not fds:\n            raise ValueError(\"fds must not be empty\")\n\n        loop = get_running_loop()\n        filenos: list[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    # The ignore can be removed after mypy picks up\n                    # https://github.com/python/typeshed/pull/5545\n                    self._raw_socket.sendmsg(\n                        [message], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fdarray)]\n                    )\n                    break\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n\nclass TCPSocketListener(abc.SocketListener):\n    _accept_scope: CancelScope | None = None\n    _closed = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = cast(asyncio.BaseEventLoop, get_running_loop())\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    async def accept(self) -> abc.SocketStream:\n        if self._closed:\n            raise ClosedResourceError\n\n        with self._accept_guard:\n            await AsyncIOBackend.checkpoint()\n            with CancelScope() as self._accept_scope:\n                try:\n                    client_sock, _addr = await self._loop.sock_accept(self._raw_socket)\n                except asyncio.CancelledError:\n                    # Workaround for https://bugs.python.org/issue41317\n                    try:\n                        self._loop.remove_reader(self._raw_socket)\n                    except (ValueError, NotImplementedError):\n                        pass\n\n                    if self._closed:\n                        raise ClosedResourceError from None\n\n                    raise\n                finally:\n                    self._accept_scope = None\n\n        client_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        transport, protocol = await self._loop.connect_accepted_socket(\n            StreamProtocol, client_sock\n        )\n        return SocketStream(transport, protocol)\n\n    async def aclose(self) -> None:\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._accept_scope:\n            # Workaround for https://bugs.python.org/issue41317\n            try:\n                self._loop.remove_reader(self._raw_socket)\n            except (ValueError, NotImplementedError):\n                pass\n\n            self._accept_scope.cancel()\n            await sleep(0)\n\n        self._raw_socket.close()\n\n\nclass UNIXSocketListener(abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n        self._closed = False\n\n    async def accept(self) -> abc.SocketStream:\n        await AsyncIOBackend.checkpoint()\n        with self._accept_guard:\n            while True:\n                try:\n                    client_sock, _ = self.__raw_socket.accept()\n                    client_sock.setblocking(False)\n                    return UNIXSocketStream(client_sock)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    self._loop.add_reader(self.__raw_socket, f.set_result, None)\n                    f.add_done_callback(\n                        lambda _: self._loop.remove_reader(self.__raw_socket)\n                    )\n                    await f\n                except OSError as exc:\n                    if self._closed:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        self._closed = True\n        self.__raw_socket.close()\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n\nclass UDPSocket(abc.UDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                return self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(*item)\n\n\nclass ConnectedUDPSocket(abc.ConnectedUDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                packet = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n            return packet[0]\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(item)\n\n\nclass UNIXDatagramSocket(_RawSocketMixin, abc.UNIXDatagramSocket):\n    async def receive(self) -> UNIXDatagramPacketType:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recvfrom(65536)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return data\n\n    async def send(self, item: UNIXDatagramPacketType) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    self._raw_socket.sendto(*item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return\n\n\nclass ConnectedUNIXDatagramSocket(_RawSocketMixin, abc.ConnectedUNIXDatagramSocket):\n    async def receive(self) -> bytes:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recv(65536)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    self._raw_socket.send(item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return\n\n\n_read_events: RunVar[dict[Any, asyncio.Event]] = RunVar(\"read_events\")\n_write_events: RunVar[dict[Any, asyncio.Event]] = RunVar(\"write_events\")\n\n\n#\n# Synchronization\n#\n\n\nclass Event(BaseEvent):\n    def __new__(cls) -> Event:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self._event = asyncio.Event()\n\n    def set(self) -> None:\n        self._event.set()\n\n    def is_set(self) -> bool:\n        return self._event.is_set()\n\n    async def wait(self) -> None:\n        if self.is_set():\n            await AsyncIOBackend.checkpoint()\n        else:\n            await self._event.wait()\n\n    def statistics(self) -> EventStatistics:\n        return EventStatistics(len(self._event._waiters))\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    _total_tokens: float = 0\n\n    def __new__(cls, total_tokens: float) -> CapacityLimiter:\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float):\n        self._borrowers: set[Any] = set()\n        self._wait_queue: OrderedDict[Any, asyncio.Event] = OrderedDict()\n        self.total_tokens = total_tokens\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    @property\n    def total_tokens(self) -> float:\n        return self._total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and not math.isinf(value):\n            raise TypeError(\"total_tokens must be an int or math.inf\")\n        if value < 1:\n            raise ValueError(\"total_tokens must be >= 1\")\n\n        waiters_to_notify = max(value - self._total_tokens, 0)\n        self._total_tokens = value\n\n        # Notify waiting tasks that they have acquired the limiter\n        while self._wait_queue and waiters_to_notify:\n            event = self._wait_queue.popitem(last=False)[1]\n            event.set()\n            waiters_to_notify -= 1\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return len(self._borrowers)\n\n    @property\n    def available_tokens(self) -> float:\n        return self._total_tokens - len(self._borrowers)\n\n    def acquire_nowait(self) -> None:\n        self.acquire_on_behalf_of_nowait(current_task())\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        if borrower in self._borrowers:\n            raise RuntimeError(\n                \"this borrower is already holding one of this CapacityLimiter's \"\n                \"tokens\"\n            )\n\n        if self._wait_queue or len(self._borrowers) >= self._total_tokens:\n            raise WouldBlock\n\n        self._borrowers.add(borrower)\n\n    async def acquire(self) -> None:\n        return await self.acquire_on_behalf_of(current_task())\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await AsyncIOBackend.checkpoint_if_cancelled()\n        try:\n            self.acquire_on_behalf_of_nowait(borrower)\n        except WouldBlock:\n            event = asyncio.Event()\n            self._wait_queue[borrower] = event\n            try:\n                await event.wait()\n            except BaseException:\n                self._wait_queue.pop(borrower, None)\n                raise\n\n            self._borrowers.add(borrower)\n        else:\n            try:\n                await AsyncIOBackend.cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def release(self) -> None:\n        self.release_on_behalf_of(current_task())\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        try:\n            self._borrowers.remove(borrower)\n        except KeyError:\n            raise RuntimeError(\n                \"this borrower isn't holding any of this CapacityLimiter's tokens\"\n            ) from None\n\n        # Notify the next task in line if this limiter has free capacity now\n        if self._wait_queue and len(self._borrowers) < self._total_tokens:\n            event = self._wait_queue.popitem(last=False)[1]\n            event.set()\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        return CapacityLimiterStatistics(\n            self.borrowed_tokens,\n            self.total_tokens,\n            tuple(self._borrowers),\n            len(self._wait_queue),\n        )\n\n\n_default_thread_limiter: RunVar[CapacityLimiter] = RunVar(\"_default_thread_limiter\")\n\n\n#\n# Operating system signals\n#\n\n\nclass _SignalReceiver:\n    def __init__(self, signals: tuple[Signals, ...]):\n        self._signals = signals\n        self._loop = get_running_loop()\n        self._signal_queue: deque[Signals] = deque()\n        self._future: asyncio.Future = asyncio.Future()\n        self._handled_signals: set[Signals] = set()\n\n    def _deliver(self, signum: Signals) -> None:\n        self._signal_queue.append(signum)\n        if not self._future.done():\n            self._future.set_result(None)\n\n    def __enter__(self) -> _SignalReceiver:\n        for sig in set(self._signals):\n            self._loop.add_signal_handler(sig, self._deliver, sig)\n            self._handled_signals.add(sig)\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        for sig in self._handled_signals:\n            self._loop.remove_signal_handler(sig)\n        return None\n\n    def __aiter__(self) -> _SignalReceiver:\n        return self\n\n    async def __anext__(self) -> Signals:\n        await AsyncIOBackend.checkpoint()\n        if not self._signal_queue:\n            self._future = asyncio.Future()\n            await self._future\n\n        return self._signal_queue.popleft()\n\n\n#\n# Testing and debugging\n#\n\n\nclass AsyncIOTaskInfo(TaskInfo):\n    def __init__(self, task: asyncio.Task):\n        task_state = _task_states.get(task)\n        if task_state is None:\n            parent_id = None\n        else:\n            parent_id = task_state.parent_id\n\n        super().__init__(id(task), parent_id, task.get_name(), task.get_coro())\n        self._task = weakref.ref(task)\n\n    def has_pending_cancellation(self) -> bool:\n        if not (task := self._task()):\n            # If the task isn't around anymore, it won't have a pending cancellation\n            return False\n\n        if sys.version_info >= (3, 11):\n            if task.cancelling():\n                return True\n        elif (\n            isinstance(task._fut_waiter, asyncio.Future)\n            and task._fut_waiter.cancelled()\n        ):\n            return True\n\n        if task_state := _task_states.get(task):\n            if cancel_scope := task_state.cancel_scope:\n                return cancel_scope.cancel_called or cancel_scope._parent_cancelled()\n\n        return False\n\n\nclass TestRunner(abc.TestRunner):\n    _send_stream: MemoryObjectSendStream[tuple[Awaitable[Any], asyncio.Future[Any]]]\n\n    def __init__(\n        self,\n        *,\n        debug: bool | None = None,\n        use_uvloop: bool = False,\n        loop_factory: Callable[[], AbstractEventLoop] | None = None,\n    ) -> None:\n        if use_uvloop and loop_factory is None:\n            import uvloop\n\n            loop_factory = uvloop.new_event_loop\n\n        self._runner = Runner(debug=debug, loop_factory=loop_factory)\n        self._exceptions: list[BaseException] = []\n        self._runner_task: asyncio.Task | None = None\n\n    def __enter__(self) -> TestRunner:\n        self._runner.__enter__()\n        self.get_loop().set_exception_handler(self._exception_handler)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self._runner.__exit__(exc_type, exc_val, exc_tb)\n\n    def get_loop(self) -> AbstractEventLoop:\n        return self._runner.get_loop()\n\n    def _exception_handler(\n        self, loop: asyncio.AbstractEventLoop, context: dict[str, Any]\n    ) -> None:\n        if isinstance(context.get(\"exception\"), Exception):\n            self._exceptions.append(context[\"exception\"])\n        else:\n            loop.default_exception_handler(context)\n\n    def _raise_async_exceptions(self) -> None:\n        # Re-raise any exceptions raised in asynchronous callbacks\n        if self._exceptions:\n            exceptions, self._exceptions = self._exceptions, []\n            if len(exceptions) == 1:\n                raise exceptions[0]\n            elif exceptions:\n                raise BaseExceptionGroup(\n                    \"Multiple exceptions occurred in asynchronous callbacks\", exceptions\n                )\n\n    async def _run_tests_and_fixtures(\n        self,\n        receive_stream: MemoryObjectReceiveStream[\n            tuple[Awaitable[T_Retval], asyncio.Future[T_Retval]]\n        ],\n    ) -> None:\n        with receive_stream, self._send_stream:\n            async for coro, future in receive_stream:\n                try:\n                    retval = await coro\n                except BaseException as exc:\n                    if not future.cancelled():\n                        future.set_exception(exc)\n                else:\n                    if not future.cancelled():\n                        future.set_result(retval)\n\n    async def _call_in_runner_task(\n        self,\n        func: Callable[P, Awaitable[T_Retval]],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> T_Retval:\n        if not self._runner_task:\n            self._send_stream, receive_stream = create_memory_object_stream[\n                Tuple[Awaitable[Any], asyncio.Future]\n            ](1)\n            self._runner_task = self.get_loop().create_task(\n                self._run_tests_and_fixtures(receive_stream)\n            )\n\n        coro = func(*args, **kwargs)\n        future: asyncio.Future[T_Retval] = self.get_loop().create_future()\n        self._send_stream.send_nowait((coro, future))\n        return await future\n\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[T_Retval, Any]],\n        kwargs: dict[str, Any],\n    ) -> Iterable[T_Retval]:\n        asyncgen = fixture_func(**kwargs)\n        fixturevalue: T_Retval = self.get_loop().run_until_complete(\n            self._call_in_runner_task(asyncgen.asend, None)\n        )\n        self._raise_async_exceptions()\n\n        yield fixturevalue\n\n        try:\n            self.get_loop().run_until_complete(\n                self._call_in_runner_task(asyncgen.asend, None)\n            )\n        except StopAsyncIteration:\n            self._raise_async_exceptions()\n        else:\n            self.get_loop().run_until_complete(asyncgen.aclose())\n            raise RuntimeError(\"Async generator fixture did not stop\")\n\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        kwargs: dict[str, Any],\n    ) -> T_Retval:\n        retval = self.get_loop().run_until_complete(\n            self._call_in_runner_task(fixture_func, **kwargs)\n        )\n        self._raise_async_exceptions()\n        return retval\n\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: dict[str, Any]\n    ) -> None:\n        try:\n            self.get_loop().run_until_complete(\n                self._call_in_runner_task(test_func, **kwargs)\n            )\n        except Exception as exc:\n            self._exceptions.append(exc)\n\n        self._raise_async_exceptions()\n\n\nclass AsyncIOBackend(AsyncBackend):\n    @classmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        @wraps(func)\n        async def wrapper() -> T_Retval:\n            task = cast(asyncio.Task, current_task())\n            task.set_name(get_callable_name(func))\n            _task_states[task] = TaskState(None, None)\n\n            try:\n                return await func(*args)\n            finally:\n                del _task_states[task]\n\n        debug = options.get(\"debug\", None)\n        loop_factory = options.get(\"loop_factory\", None)\n        if loop_factory is None and options.get(\"use_uvloop\", False):\n            import uvloop\n\n            loop_factory = uvloop.new_event_loop\n\n        with Runner(debug=debug, loop_factory=loop_factory) as runner:\n            return runner.run(wrapper())\n\n    @classmethod\n    def current_token(cls) -> object:\n        return get_running_loop()\n\n    @classmethod\n    def current_time(cls) -> float:\n        return get_running_loop().time()\n\n    @classmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        return CancelledError\n\n    @classmethod\n    async def checkpoint(cls) -> None:\n        await sleep(0)\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        task = current_task()\n        if task is None:\n            return\n\n        try:\n            cancel_scope = _task_states[task].cancel_scope\n        except KeyError:\n            return\n\n        while cancel_scope:\n            if cancel_scope.cancel_called:\n                await sleep(0)\n            elif cancel_scope.shield:\n                break\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        with CancelScope(shield=True):\n            await sleep(0)\n\n    @classmethod\n    async def sleep(cls, delay: float) -> None:\n        await sleep(delay)\n\n    @classmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return CancelScope(deadline=deadline, shield=shield)\n\n    @classmethod\n    def current_effective_deadline(cls) -> float:\n        try:\n            cancel_scope = _task_states[\n                current_task()  # type: ignore[index]\n            ].cancel_scope\n        except KeyError:\n            return math.inf\n\n        deadline = math.inf\n        while cancel_scope:\n            deadline = min(deadline, cancel_scope.deadline)\n            if cancel_scope._cancel_called:\n                deadline = -math.inf\n                break\n            elif cancel_scope.shield:\n                break\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return deadline\n\n    @classmethod\n    def create_task_group(cls) -> abc.TaskGroup:\n        return TaskGroup()\n\n    @classmethod\n    def create_event(cls) -> abc.Event:\n        return Event()\n\n    @classmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> abc.CapacityLimiter:\n        return CapacityLimiter(total_tokens)\n\n    @classmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        abandon_on_cancel: bool = False,\n        limiter: abc.CapacityLimiter | None = None,\n    ) -> T_Retval:\n        await cls.checkpoint()\n\n        # If this is the first run in this event loop thread, set up the necessary\n        # variables\n        try:\n            idle_workers = _threadpool_idle_workers.get()\n            workers = _threadpool_workers.get()\n        except LookupError:\n            idle_workers = deque()\n            workers = set()\n            _threadpool_idle_workers.set(idle_workers)\n            _threadpool_workers.set(workers)\n\n        async with limiter or cls.current_default_thread_limiter():\n            with CancelScope(shield=not abandon_on_cancel) as scope:\n                future: asyncio.Future = asyncio.Future()\n                root_task = find_root_task()\n                if not idle_workers:\n                    worker = WorkerThread(root_task, workers, idle_workers)\n                    worker.start()\n                    workers.add(worker)\n                    root_task.add_done_callback(worker.stop)\n                else:\n                    worker = idle_workers.pop()\n\n                    # Prune any other workers that have been idle for MAX_IDLE_TIME\n                    # seconds or longer\n                    now = cls.current_time()\n                    while idle_workers:\n                        if (\n                            now - idle_workers[0].idle_since\n                            < WorkerThread.MAX_IDLE_TIME\n                        ):\n                            break\n\n                        expired_worker = idle_workers.popleft()\n                        expired_worker.root_task.remove_done_callback(\n                            expired_worker.stop\n                        )\n                        expired_worker.stop()\n\n                context = copy_context()\n                context.run(sniffio.current_async_library_cvar.set, None)\n                if abandon_on_cancel or scope._parent_scope is None:\n                    worker_scope = scope\n                else:\n                    worker_scope = scope._parent_scope\n\n                worker.queue.put_nowait((context, func, args, future, worker_scope))\n                return await future\n\n    @classmethod\n    def check_cancelled(cls) -> None:\n        scope: CancelScope | None = threadlocals.current_cancel_scope\n        while scope is not None:\n            if scope.cancel_called:\n                raise CancelledError(f\"Cancelled by cancel scope {id(scope):x}\")\n\n            if scope.shield:\n                return\n\n            scope = scope._parent_scope\n\n    @classmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        async def task_wrapper(scope: CancelScope) -> T_Retval:\n            __tracebackhide__ = True\n            task = cast(asyncio.Task, current_task())\n            _task_states[task] = TaskState(None, scope)\n            scope._tasks.add(task)\n            try:\n                return await func(*args)\n            except CancelledError as exc:\n                raise concurrent.futures.CancelledError(str(exc)) from None\n            finally:\n                scope._tasks.discard(task)\n\n        loop = cast(AbstractEventLoop, token)\n        context = copy_context()\n        context.run(sniffio.current_async_library_cvar.set, \"asyncio\")\n        wrapper = task_wrapper(threadlocals.current_cancel_scope)\n        f: concurrent.futures.Future[T_Retval] = context.run(\n            asyncio.run_coroutine_threadsafe, wrapper, loop\n        )\n        return f.result()\n\n    @classmethod\n    def run_sync_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        @wraps(func)\n        def wrapper() -> None:\n            try:\n                sniffio.current_async_library_cvar.set(\"asyncio\")\n                f.set_result(func(*args))\n            except BaseException as exc:\n                f.set_exception(exc)\n                if not isinstance(exc, Exception):\n                    raise\n\n        f: concurrent.futures.Future[T_Retval] = Future()\n        loop = cast(AbstractEventLoop, token)\n        loop.call_soon_threadsafe(wrapper)\n        return f.result()\n\n    @classmethod\n    def create_blocking_portal(cls) -> abc.BlockingPortal:\n        return BlockingPortal()\n\n    @classmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        await cls.checkpoint()\n        if shell:\n            process = await asyncio.create_subprocess_shell(\n                cast(\"str | bytes\", command),\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                cwd=cwd,\n                env=env,\n                start_new_session=start_new_session,\n            )\n        else:\n            process = await asyncio.create_subprocess_exec(\n                *command,\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                cwd=cwd,\n                env=env,\n                start_new_session=start_new_session,\n            )\n\n        stdin_stream = StreamWriterWrapper(process.stdin) if process.stdin else None\n        stdout_stream = StreamReaderWrapper(process.stdout) if process.stdout else None\n        stderr_stream = StreamReaderWrapper(process.stderr) if process.stderr else None\n        return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n    @classmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[abc.Process]) -> None:\n        create_task(\n            _shutdown_process_pool_on_exit(workers),\n            name=\"AnyIO process pool shutdown task\",\n        )\n        find_root_task().add_done_callback(\n            partial(_forcibly_shutdown_process_pool_on_exit, workers)\n        )\n\n    @classmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> abc.SocketStream:\n        transport, protocol = cast(\n            Tuple[asyncio.Transport, StreamProtocol],\n            await get_running_loop().create_connection(\n                StreamProtocol, host, port, local_addr=local_address\n            ),\n        )\n        transport.pause_reading()\n        return SocketStream(transport, protocol)\n\n    @classmethod\n    async def connect_unix(cls, path: str | bytes) -> abc.UNIXSocketStream:\n        await cls.checkpoint()\n        loop = get_running_loop()\n        raw_socket = socket.socket(socket.AF_UNIX)\n        raw_socket.setblocking(False)\n        while True:\n            try:\n                raw_socket.connect(path)\n            except BlockingIOError:\n                f: asyncio.Future = asyncio.Future()\n                loop.add_writer(raw_socket, f.set_result, None)\n                f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n                await f\n            except BaseException:\n                raw_socket.close()\n                raise\n            else:\n                return UNIXSocketStream(raw_socket)\n\n    @classmethod\n    def create_tcp_listener(cls, sock: socket.socket) -> SocketListener:\n        return TCPSocketListener(sock)\n\n    @classmethod\n    def create_unix_listener(cls, sock: socket.socket) -> SocketListener:\n        return UNIXSocketListener(sock)\n\n    @classmethod\n    async def create_udp_socket(\n        cls,\n        family: AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        transport, protocol = await get_running_loop().create_datagram_endpoint(\n            DatagramProtocol,\n            local_addr=local_address,\n            remote_addr=remote_address,\n            family=family,\n            reuse_port=reuse_port,\n        )\n        if protocol.exception:\n            transport.close()\n            raise protocol.exception\n\n        if not remote_address:\n            return UDPSocket(transport, protocol)\n        else:\n            return ConnectedUDPSocket(transport, protocol)\n\n    @classmethod\n    async def create_unix_datagram_socket(  # type: ignore[override]\n        cls, raw_socket: socket.socket, remote_path: str | bytes | None\n    ) -> abc.UNIXDatagramSocket | abc.ConnectedUNIXDatagramSocket:\n        await cls.checkpoint()\n        loop = get_running_loop()\n\n        if remote_path:\n            while True:\n                try:\n                    raw_socket.connect(remote_path)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    loop.add_writer(raw_socket, f.set_result, None)\n                    f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n                    await f\n                except BaseException:\n                    raw_socket.close()\n                    raise\n                else:\n                    return ConnectedUNIXDatagramSocket(raw_socket)\n        else:\n            return UNIXDatagramSocket(raw_socket)\n\n    @classmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        return await get_running_loop().getaddrinfo(\n            host, port, family=family, type=type, proto=proto, flags=flags\n        )\n\n    @classmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        return await get_running_loop().getnameinfo(sockaddr, flags)\n\n    @classmethod\n    async def wait_socket_readable(cls, sock: socket.socket) -> None:\n        await cls.checkpoint()\n        try:\n            read_events = _read_events.get()\n        except LookupError:\n            read_events = {}\n            _read_events.set(read_events)\n\n        if read_events.get(sock):\n            raise BusyResourceError(\"reading from\") from None\n\n        loop = get_running_loop()\n        event = read_events[sock] = asyncio.Event()\n        loop.add_reader(sock, event.set)\n        try:\n            await event.wait()\n        finally:\n            if read_events.pop(sock, None) is not None:\n                loop.remove_reader(sock)\n                readable = True\n            else:\n                readable = False\n\n        if not readable:\n            raise ClosedResourceError\n\n    @classmethod\n    async def wait_socket_writable(cls, sock: socket.socket) -> None:\n        await cls.checkpoint()\n        try:\n            write_events = _write_events.get()\n        except LookupError:\n            write_events = {}\n            _write_events.set(write_events)\n\n        if write_events.get(sock):\n            raise BusyResourceError(\"writing to\") from None\n\n        loop = get_running_loop()\n        event = write_events[sock] = asyncio.Event()\n        loop.add_writer(sock.fileno(), event.set)\n        try:\n            await event.wait()\n        finally:\n            if write_events.pop(sock, None) is not None:\n                loop.remove_writer(sock)\n                writable = True\n            else:\n                writable = False\n\n        if not writable:\n            raise ClosedResourceError\n\n    @classmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        try:\n            return _default_thread_limiter.get()\n        except LookupError:\n            limiter = CapacityLimiter(40)\n            _default_thread_limiter.set(limiter)\n            return limiter\n\n    @classmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        return _SignalReceiver(signals)\n\n    @classmethod\n    def get_current_task(cls) -> TaskInfo:\n        return AsyncIOTaskInfo(current_task())  # type: ignore[arg-type]\n\n    @classmethod\n    def get_running_tasks(cls) -> Sequence[TaskInfo]:\n        return [AsyncIOTaskInfo(task) for task in all_tasks() if not task.done()]\n\n    @classmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        await cls.checkpoint()\n        this_task = current_task()\n        while True:\n            for task in all_tasks():\n                if task is this_task:\n                    continue\n\n                waiter = task._fut_waiter  # type: ignore[attr-defined]\n                if waiter is None or waiter.done():\n                    await sleep(0.1)\n                    break\n            else:\n                return\n\n    @classmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        return TestRunner(**options)\n\n\nbackend_class = AsyncIOBackend\n", "src/anyio/_backends/_trio.py": "from __future__ import annotations\n\nimport array\nimport math\nimport socket\nimport sys\nimport types\nimport weakref\nfrom collections.abc import AsyncIterator, Iterable\nfrom concurrent.futures import Future\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom io import IOBase\nfrom os import PathLike\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Collection,\n    ContextManager,\n    Coroutine,\n    Generic,\n    Mapping,\n    NoReturn,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport trio.from_thread\nimport trio.lowlevel\nfrom outcome import Error, Outcome, Value\nfrom trio.lowlevel import (\n    current_root_task,\n    current_task,\n    wait_readable,\n    wait_writable,\n)\nfrom trio.socket import SocketType as TrioSocketType\nfrom trio.to_thread import run_sync\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._eventloop import claim_worker_thread\nfrom .._core._exceptions import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n)\nfrom .._core._sockets import convert_ipv6_sockaddr\nfrom .._core._streams import create_memory_object_stream\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import IPSockAddrType, UDPPacketType, UNIXDatagramPacketType\nfrom ..abc._eventloop import AsyncBackend\nfrom ..streams.memory import MemoryObjectSendStream\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from exceptiongroup import BaseExceptionGroup\n    from typing_extensions import TypeVarTuple, Unpack\n\nT = TypeVar(\"T\")\nT_Retval = TypeVar(\"T_Retval\")\nT_SockAddr = TypeVar(\"T_SockAddr\", str, IPSockAddrType)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\nP = ParamSpec(\"P\")\n\n\n#\n# Event loop\n#\n\nRunVar = trio.lowlevel.RunVar\n\n\n#\n# Timeouts and cancellation\n#\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(\n        cls, original: trio.CancelScope | None = None, **kwargs: object\n    ) -> CancelScope:\n        return object.__new__(cls)\n\n    def __init__(self, original: trio.CancelScope | None = None, **kwargs: Any) -> None:\n        self.__original = original or trio.CancelScope(**kwargs)\n\n    def __enter__(self) -> CancelScope:\n        self.__original.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        # https://github.com/python-trio/trio-typing/pull/79\n        return self.__original.__exit__(exc_type, exc_val, exc_tb)\n\n    def cancel(self) -> None:\n        self.__original.cancel()\n\n    @property\n    def deadline(self) -> float:\n        return self.__original.deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self.__original.deadline = value\n\n    @property\n    def cancel_called(self) -> bool:\n        return self.__original.cancel_called\n\n    @property\n    def cancelled_caught(self) -> bool:\n        return self.__original.cancelled_caught\n\n    @property\n    def shield(self) -> bool:\n        return self.__original.shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        self.__original.shield = value\n\n\n#\n# Task groups\n#\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self._active = False\n        self._nursery_manager = trio.open_nursery(strict_exception_groups=True)\n        self.cancel_scope = None  # type: ignore[assignment]\n\n    async def __aenter__(self) -> TaskGroup:\n        self._active = True\n        self._nursery = await self._nursery_manager.__aenter__()\n        self.cancel_scope = CancelScope(self._nursery.cancel_scope)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        try:\n            return await self._nursery_manager.__aexit__(exc_type, exc_val, exc_tb)\n        except BaseExceptionGroup as exc:\n            _, rest = exc.split(trio.Cancelled)\n            if not rest:\n                cancelled_exc = trio.Cancelled._create()\n                raise cancelled_exc from exc\n\n            raise\n        finally:\n            self._active = False\n\n    def start_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> None:\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        self._nursery.start_soon(func, *args, name=name)\n\n    async def start(\n        self, func: Callable[..., Awaitable[Any]], *args: object, name: object = None\n    ) -> Any:\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        return await self._nursery.start(func, *args, name=name)\n\n\n#\n# Threads\n#\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> BlockingPortal:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._token = trio.lowlevel.current_trio_token()\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future[T_Retval],\n    ) -> None:\n        trio.from_thread.run_sync(\n            partial(self._task_group.start_soon, name=name),\n            self._call_func,\n            func,\n            args,\n            kwargs,\n            future,\n            trio_token=self._token,\n        )\n\n\n#\n# Subprocesses\n#\n\n\n@dataclass(eq=False)\nclass ReceiveStreamWrapper(abc.ByteReceiveStream):\n    _stream: trio.abc.ReceiveStream\n\n    async def receive(self, max_bytes: int | None = None) -> bytes:\n        try:\n            data = await self._stream.receive_some(max_bytes)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError from exc.__cause__\n        except trio.BrokenResourceError as exc:\n            raise BrokenResourceError from exc.__cause__\n\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        await self._stream.aclose()\n\n\n@dataclass(eq=False)\nclass SendStreamWrapper(abc.ByteSendStream):\n    _stream: trio.abc.SendStream\n\n    async def send(self, item: bytes) -> None:\n        try:\n            await self._stream.send_all(item)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError from exc.__cause__\n        except trio.BrokenResourceError as exc:\n            raise BrokenResourceError from exc.__cause__\n\n    async def aclose(self) -> None:\n        await self._stream.aclose()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: trio.Process\n    _stdin: abc.ByteSendStream | None\n    _stdout: abc.ByteReceiveStream | None\n    _stderr: abc.ByteReceiveStream | None\n\n    async def aclose(self) -> None:\n        with CancelScope(shield=True):\n            if self._stdin:\n                await self._stdin.aclose()\n            if self._stdout:\n                await self._stdout.aclose()\n            if self._stderr:\n                await self._stderr.aclose()\n\n        try:\n            await self.wait()\n        except BaseException:\n            self.kill()\n            with CancelScope(shield=True):\n                await self.wait()\n            raise\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: Signals) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> int | None:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> abc.ByteSendStream | None:\n        return self._stdin\n\n    @property\n    def stdout(self) -> abc.ByteReceiveStream | None:\n        return self._stdout\n\n    @property\n    def stderr(self) -> abc.ByteReceiveStream | None:\n        return self._stderr\n\n\nclass _ProcessPoolShutdownInstrument(trio.abc.Instrument):\n    def after_run(self) -> None:\n        super().after_run()\n\n\ncurrent_default_worker_process_limiter: trio.lowlevel.RunVar = RunVar(\n    \"current_default_worker_process_limiter\"\n)\n\n\nasync def _shutdown_process_pool(workers: set[abc.Process]) -> None:\n    try:\n        await trio.sleep(math.inf)\n    except trio.Cancelled:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        with CancelScope(shield=True):\n            for process in workers:\n                await process.aclose()\n\n\n#\n# Sockets and networking\n#\n\n\nclass _TrioSocketMixin(Generic[T_SockAddr]):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        self._trio_socket = trio_socket\n        self._closed = False\n\n    def _check_closed(self) -> None:\n        if self._closed:\n            raise ClosedResourceError\n        if self._trio_socket.fileno() < 0:\n            raise BrokenResourceError\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._trio_socket._sock  # type: ignore[attr-defined]\n\n    async def aclose(self) -> None:\n        if self._trio_socket.fileno() >= 0:\n            self._closed = True\n            self._trio_socket.close()\n\n    def _convert_socket_error(self, exc: BaseException) -> NoReturn:\n        if isinstance(exc, trio.ClosedResourceError):\n            raise ClosedResourceError from exc\n        elif self._trio_socket.fileno() < 0 and self._closed:\n            raise ClosedResourceError from None\n        elif isinstance(exc, OSError):\n            raise BrokenResourceError from exc\n        else:\n            raise exc\n\n\nclass SocketStream(_TrioSocketMixin, abc.SocketStream):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            try:\n                data = await self._trio_socket.recv(max_bytes)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n            if data:\n                return data\n            else:\n                raise EndOfStream\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = await self._trio_socket.send(view)\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n\n                view = view[bytes_sent:]\n\n    async def send_eof(self) -> None:\n        self._trio_socket.shutdown(socket.SHUT_WR)\n\n\nclass UNIXSocketStream(SocketStream, abc.UNIXSocketStream):\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError(\"msglen must be a non-negative integer\")\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError(\"maxfds must be a positive integer\")\n\n        fds = array.array(\"i\")\n        await trio.lowlevel.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = await self._trio_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize)\n                    )\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(\n                    f\"Received unexpected ancillary data; message = {message!r}, \"\n                    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n                )\n\n            fds.frombytes(cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        if not message:\n            raise ValueError(\"message must not be empty\")\n        if not fds:\n            raise ValueError(\"fds must not be empty\")\n\n        filenos: list[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await trio.lowlevel.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    await self._trio_socket.sendmsg(\n                        [message],\n                        [\n                            (\n                                socket.SOL_SOCKET,\n                                socket.SCM_RIGHTS,\n                                fdarray,\n                            )\n                        ],\n                    )\n                    break\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n\n\nclass TCPSocketListener(_TrioSocketMixin, abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    async def accept(self) -> SocketStream:\n        with self._accept_guard:\n            try:\n                trio_socket, _addr = await self._trio_socket.accept()\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n        trio_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        return SocketStream(trio_socket)\n\n\nclass UNIXSocketListener(_TrioSocketMixin, abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    async def accept(self) -> UNIXSocketStream:\n        with self._accept_guard:\n            try:\n                trio_socket, _addr = await self._trio_socket.accept()\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n        return UNIXSocketStream(trio_socket)\n\n\nclass UDPSocket(_TrioSocketMixin[IPSockAddrType], abc.UDPSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            try:\n                data, addr = await self._trio_socket.recvfrom(65536)\n                return data, convert_ipv6_sockaddr(addr)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.sendto(*item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass ConnectedUDPSocket(_TrioSocketMixin[IPSockAddrType], abc.ConnectedUDPSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            try:\n                return await self._trio_socket.recv(65536)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.send(item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass UNIXDatagramSocket(_TrioSocketMixin[str], abc.UNIXDatagramSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> UNIXDatagramPacketType:\n        with self._receive_guard:\n            try:\n                data, addr = await self._trio_socket.recvfrom(65536)\n                return data, addr\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: UNIXDatagramPacketType) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.sendto(*item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass ConnectedUNIXDatagramSocket(\n    _TrioSocketMixin[str], abc.ConnectedUNIXDatagramSocket\n):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            try:\n                return await self._trio_socket.recv(65536)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.send(item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\n#\n# Synchronization\n#\n\n\nclass Event(BaseEvent):\n    def __new__(cls) -> Event:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self.__original = trio.Event()\n\n    def is_set(self) -> bool:\n        return self.__original.is_set()\n\n    async def wait(self) -> None:\n        return await self.__original.wait()\n\n    def statistics(self) -> EventStatistics:\n        orig_statistics = self.__original.statistics()\n        return EventStatistics(tasks_waiting=orig_statistics.tasks_waiting)\n\n    def set(self) -> None:\n        self.__original.set()\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    def __new__(\n        cls,\n        total_tokens: float | None = None,\n        *,\n        original: trio.CapacityLimiter | None = None,\n    ) -> CapacityLimiter:\n        return object.__new__(cls)\n\n    def __init__(\n        self,\n        total_tokens: float | None = None,\n        *,\n        original: trio.CapacityLimiter | None = None,\n    ) -> None:\n        if original is not None:\n            self.__original = original\n        else:\n            assert total_tokens is not None\n            self.__original = trio.CapacityLimiter(total_tokens)\n\n    async def __aenter__(self) -> None:\n        return await self.__original.__aenter__()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.__original.__aexit__(exc_type, exc_val, exc_tb)\n\n    @property\n    def total_tokens(self) -> float:\n        return self.__original.total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        self.__original.total_tokens = value\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return self.__original.borrowed_tokens\n\n    @property\n    def available_tokens(self) -> float:\n        return self.__original.available_tokens\n\n    def acquire_nowait(self) -> None:\n        self.__original.acquire_nowait()\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        self.__original.acquire_on_behalf_of_nowait(borrower)\n\n    async def acquire(self) -> None:\n        await self.__original.acquire()\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await self.__original.acquire_on_behalf_of(borrower)\n\n    def release(self) -> None:\n        return self.__original.release()\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        return self.__original.release_on_behalf_of(borrower)\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        orig = self.__original.statistics()\n        return CapacityLimiterStatistics(\n            borrowed_tokens=orig.borrowed_tokens,\n            total_tokens=orig.total_tokens,\n            borrowers=tuple(orig.borrowers),\n            tasks_waiting=orig.tasks_waiting,\n        )\n\n\n_capacity_limiter_wrapper: trio.lowlevel.RunVar = RunVar(\"_capacity_limiter_wrapper\")\n\n\n#\n# Signal handling\n#\n\n\nclass _SignalReceiver:\n    _iterator: AsyncIterator[int]\n\n    def __init__(self, signals: tuple[Signals, ...]):\n        self._signals = signals\n\n    def __enter__(self) -> _SignalReceiver:\n        self._cm = trio.open_signal_receiver(*self._signals)\n        self._iterator = self._cm.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        return self._cm.__exit__(exc_type, exc_val, exc_tb)\n\n    def __aiter__(self) -> _SignalReceiver:\n        return self\n\n    async def __anext__(self) -> Signals:\n        signum = await self._iterator.__anext__()\n        return Signals(signum)\n\n\n#\n# Testing and debugging\n#\n\n\nclass TestRunner(abc.TestRunner):\n    def __init__(self, **options: Any) -> None:\n        from queue import Queue\n\n        self._call_queue: Queue[Callable[[], object]] = Queue()\n        self._send_stream: MemoryObjectSendStream | None = None\n        self._options = options\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -> None:\n        if self._send_stream:\n            self._send_stream.close()\n            while self._send_stream is not None:\n                self._call_queue.get()()\n\n    async def _run_tests_and_fixtures(self) -> None:\n        self._send_stream, receive_stream = create_memory_object_stream(1)\n        with receive_stream:\n            async for coro, outcome_holder in receive_stream:\n                try:\n                    retval = await coro\n                except BaseException as exc:\n                    outcome_holder.append(Error(exc))\n                else:\n                    outcome_holder.append(Value(retval))\n\n    def _main_task_finished(self, outcome: object) -> None:\n        self._send_stream = None\n\n    def _call_in_runner_task(\n        self,\n        func: Callable[P, Awaitable[T_Retval]],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> T_Retval:\n        if self._send_stream is None:\n            trio.lowlevel.start_guest_run(\n                self._run_tests_and_fixtures,\n                run_sync_soon_threadsafe=self._call_queue.put,\n                done_callback=self._main_task_finished,\n                **self._options,\n            )\n            while self._send_stream is None:\n                self._call_queue.get()()\n\n        outcome_holder: list[Outcome] = []\n        self._send_stream.send_nowait((func(*args, **kwargs), outcome_holder))\n        while not outcome_holder:\n            self._call_queue.get()()\n\n        return outcome_holder[0].unwrap()\n\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[T_Retval, Any]],\n        kwargs: dict[str, Any],\n    ) -> Iterable[T_Retval]:\n        asyncgen = fixture_func(**kwargs)\n        fixturevalue: T_Retval = self._call_in_runner_task(asyncgen.asend, None)\n\n        yield fixturevalue\n\n        try:\n            self._call_in_runner_task(asyncgen.asend, None)\n        except StopAsyncIteration:\n            pass\n        else:\n            self._call_in_runner_task(asyncgen.aclose)\n            raise RuntimeError(\"Async generator fixture did not stop\")\n\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        kwargs: dict[str, Any],\n    ) -> T_Retval:\n        return self._call_in_runner_task(fixture_func, **kwargs)\n\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: dict[str, Any]\n    ) -> None:\n        self._call_in_runner_task(test_func, **kwargs)\n\n\nclass TrioTaskInfo(TaskInfo):\n    def __init__(self, task: trio.lowlevel.Task):\n        parent_id = None\n        if task.parent_nursery and task.parent_nursery.parent_task:\n            parent_id = id(task.parent_nursery.parent_task)\n\n        super().__init__(id(task), parent_id, task.name, task.coro)\n        self._task = weakref.proxy(task)\n\n    def has_pending_cancellation(self) -> bool:\n        try:\n            return self._task._cancel_status.effectively_cancelled\n        except ReferenceError:\n            # If the task is no longer around, it surely doesn't have a cancellation\n            # pending\n            return False\n\n\nclass TrioBackend(AsyncBackend):\n    @classmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        return trio.run(func, *args)\n\n    @classmethod\n    def current_token(cls) -> object:\n        return trio.lowlevel.current_trio_token()\n\n    @classmethod\n    def current_time(cls) -> float:\n        return trio.current_time()\n\n    @classmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        return trio.Cancelled\n\n    @classmethod\n    async def checkpoint(cls) -> None:\n        await trio.lowlevel.checkpoint()\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        await trio.lowlevel.checkpoint_if_cancelled()\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        await trio.lowlevel.cancel_shielded_checkpoint()\n\n    @classmethod\n    async def sleep(cls, delay: float) -> None:\n        await trio.sleep(delay)\n\n    @classmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> abc.CancelScope:\n        return CancelScope(deadline=deadline, shield=shield)\n\n    @classmethod\n    def current_effective_deadline(cls) -> float:\n        return trio.current_effective_deadline()\n\n    @classmethod\n    def create_task_group(cls) -> abc.TaskGroup:\n        return TaskGroup()\n\n    @classmethod\n    def create_event(cls) -> abc.Event:\n        return Event()\n\n    @classmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> CapacityLimiter:\n        return CapacityLimiter(total_tokens)\n\n    @classmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        abandon_on_cancel: bool = False,\n        limiter: abc.CapacityLimiter | None = None,\n    ) -> T_Retval:\n        def wrapper() -> T_Retval:\n            with claim_worker_thread(TrioBackend, token):\n                return func(*args)\n\n        token = TrioBackend.current_token()\n        return await run_sync(\n            wrapper,\n            abandon_on_cancel=abandon_on_cancel,\n            limiter=cast(trio.CapacityLimiter, limiter),\n        )\n\n    @classmethod\n    def check_cancelled(cls) -> None:\n        trio.from_thread.check_cancelled()\n\n    @classmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        return trio.from_thread.run(func, *args)\n\n    @classmethod\n    def run_sync_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        return trio.from_thread.run_sync(func, *args)\n\n    @classmethod\n    def create_blocking_portal(cls) -> abc.BlockingPortal:\n        return BlockingPortal()\n\n    @classmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        process = await trio.lowlevel.open_process(  # type: ignore[misc]\n            command,  # type: ignore[arg-type]\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            shell=shell,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n        stdin_stream = SendStreamWrapper(process.stdin) if process.stdin else None\n        stdout_stream = ReceiveStreamWrapper(process.stdout) if process.stdout else None\n        stderr_stream = ReceiveStreamWrapper(process.stderr) if process.stderr else None\n        return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n    @classmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[abc.Process]) -> None:\n        trio.lowlevel.spawn_system_task(_shutdown_process_pool, workers)\n\n    @classmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> SocketStream:\n        family = socket.AF_INET6 if \":\" in host else socket.AF_INET\n        trio_socket = trio.socket.socket(family)\n        trio_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        if local_address:\n            await trio_socket.bind(local_address)\n\n        try:\n            await trio_socket.connect((host, port))\n        except BaseException:\n            trio_socket.close()\n            raise\n\n        return SocketStream(trio_socket)\n\n    @classmethod\n    async def connect_unix(cls, path: str | bytes) -> abc.UNIXSocketStream:\n        trio_socket = trio.socket.socket(socket.AF_UNIX)\n        try:\n            await trio_socket.connect(path)\n        except BaseException:\n            trio_socket.close()\n            raise\n\n        return UNIXSocketStream(trio_socket)\n\n    @classmethod\n    def create_tcp_listener(cls, sock: socket.socket) -> abc.SocketListener:\n        return TCPSocketListener(sock)\n\n    @classmethod\n    def create_unix_listener(cls, sock: socket.socket) -> abc.SocketListener:\n        return UNIXSocketListener(sock)\n\n    @classmethod\n    async def create_udp_socket(\n        cls,\n        family: socket.AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        trio_socket = trio.socket.socket(family=family, type=socket.SOCK_DGRAM)\n\n        if reuse_port:\n            trio_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n\n        if local_address:\n            await trio_socket.bind(local_address)\n\n        if remote_address:\n            await trio_socket.connect(remote_address)\n            return ConnectedUDPSocket(trio_socket)\n        else:\n            return UDPSocket(trio_socket)\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket.socket, remote_path: None\n    ) -> abc.UNIXDatagramSocket: ...\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket.socket, remote_path: str | bytes\n    ) -> abc.ConnectedUNIXDatagramSocket: ...\n\n    @classmethod\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket.socket, remote_path: str | bytes | None\n    ) -> abc.UNIXDatagramSocket | abc.ConnectedUNIXDatagramSocket:\n        trio_socket = trio.socket.from_stdlib_socket(raw_socket)\n\n        if remote_path:\n            await trio_socket.connect(remote_path)\n            return ConnectedUNIXDatagramSocket(trio_socket)\n        else:\n            return UNIXDatagramSocket(trio_socket)\n\n    @classmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        return await trio.socket.getaddrinfo(host, port, family, type, proto, flags)\n\n    @classmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        return await trio.socket.getnameinfo(sockaddr, flags)\n\n    @classmethod\n    async def wait_socket_readable(cls, sock: socket.socket) -> None:\n        try:\n            await wait_readable(sock)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError().with_traceback(exc.__traceback__) from None\n        except trio.BusyResourceError:\n            raise BusyResourceError(\"reading from\") from None\n\n    @classmethod\n    async def wait_socket_writable(cls, sock: socket.socket) -> None:\n        try:\n            await wait_writable(sock)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError().with_traceback(exc.__traceback__) from None\n        except trio.BusyResourceError:\n            raise BusyResourceError(\"writing to\") from None\n\n    @classmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        try:\n            return _capacity_limiter_wrapper.get()\n        except LookupError:\n            limiter = CapacityLimiter(\n                original=trio.to_thread.current_default_thread_limiter()\n            )\n            _capacity_limiter_wrapper.set(limiter)\n            return limiter\n\n    @classmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        return _SignalReceiver(signals)\n\n    @classmethod\n    def get_current_task(cls) -> TaskInfo:\n        task = current_task()\n        return TrioTaskInfo(task)\n\n    @classmethod\n    def get_running_tasks(cls) -> Sequence[TaskInfo]:\n        root_task = current_root_task()\n        assert root_task\n        task_infos = [TrioTaskInfo(root_task)]\n        nurseries = root_task.child_nurseries\n        while nurseries:\n            new_nurseries: list[trio.Nursery] = []\n            for nursery in nurseries:\n                for task in nursery.child_tasks:\n                    task_infos.append(TrioTaskInfo(task))\n                    new_nurseries.extend(task.child_nurseries)\n\n            nurseries = new_nurseries\n\n        return task_infos\n\n    @classmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        from trio.testing import wait_all_tasks_blocked\n\n        await wait_all_tasks_blocked()\n\n    @classmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        return TestRunner(**options)\n\n\nbackend_class = TrioBackend\n", "src/anyio/_backends/__init__.py": "", "src/anyio/abc/_tasks.py": "from __future__ import annotations\n\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom collections.abc import Awaitable, Callable\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Any, Protocol, TypeVar, overload\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nif TYPE_CHECKING:\n    from .._core._tasks import CancelScope\n\nT_Retval = TypeVar(\"T_Retval\")\nT_contra = TypeVar(\"T_contra\", contravariant=True)\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\nclass TaskStatus(Protocol[T_contra]):\n    @overload\n    def started(self: TaskStatus[None]) -> None: ...\n\n    @overload\n    def started(self, value: T_contra) -> None: ...\n\n    def started(self, value: T_contra | None = None) -> None:\n        \"\"\"\n        Signal that the task has started.\n\n        :param value: object passed back to the starter of the task\n        \"\"\"\n\n\nclass TaskGroup(metaclass=ABCMeta):\n    \"\"\"\n    Groups several asynchronous tasks together.\n\n    :ivar cancel_scope: the cancel scope inherited by all child tasks\n    :vartype cancel_scope: CancelScope\n    \"\"\"\n\n    cancel_scope: CancelScope\n\n    @abstractmethod\n    def start_soon(\n        self,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],\n        *args: Unpack[PosArgsT],\n        name: object = None,\n    ) -> None:\n        \"\"\"\n        Start a new task in this task group.\n\n        :param func: a coroutine function\n        :param args: positional arguments to call the function with\n        :param name: name of the task, for the purposes of introspection and debugging\n\n        .. versionadded:: 3.0\n        \"\"\"\n\n    @abstractmethod\n    async def start(\n        self,\n        func: Callable[..., Awaitable[Any]],\n        *args: object,\n        name: object = None,\n    ) -> Any:\n        \"\"\"\n        Start a new task and wait until it signals for readiness.\n\n        :param func: a coroutine function\n        :param args: positional arguments to call the function with\n        :param name: name of the task, for the purposes of introspection and debugging\n        :return: the value passed to ``task_status.started()``\n        :raises RuntimeError: if the task finishes without calling\n            ``task_status.started()``\n\n        .. versionadded:: 3.0\n        \"\"\"\n\n    @abstractmethod\n    async def __aenter__(self) -> TaskGroup:\n        \"\"\"Enter the task group context and allow starting new tasks.\"\"\"\n\n    @abstractmethod\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        \"\"\"Exit the task group context waiting for all tasks to finish.\"\"\"\n", "src/anyio/abc/_sockets.py": "from __future__ import annotations\n\nimport socket\nfrom abc import abstractmethod\nfrom collections.abc import Callable, Collection, Mapping\nfrom contextlib import AsyncExitStack\nfrom io import IOBase\nfrom ipaddress import IPv4Address, IPv6Address\nfrom socket import AddressFamily\nfrom types import TracebackType\nfrom typing import Any, Tuple, TypeVar, Union\n\nfrom .._core._typedattr import (\n    TypedAttributeProvider,\n    TypedAttributeSet,\n    typed_attribute,\n)\nfrom ._streams import ByteStream, Listener, UnreliableObjectStream\nfrom ._tasks import TaskGroup\n\nIPAddressType = Union[str, IPv4Address, IPv6Address]\nIPSockAddrType = Tuple[str, int]\nSockAddrType = Union[IPSockAddrType, str]\nUDPPacketType = Tuple[bytes, IPSockAddrType]\nUNIXDatagramPacketType = Tuple[bytes, str]\nT_Retval = TypeVar(\"T_Retval\")\n\n\nclass _NullAsyncContextManager:\n    async def __aenter__(self) -> None:\n        pass\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        return None\n\n\nclass SocketAttribute(TypedAttributeSet):\n    #: the address family of the underlying socket\n    family: AddressFamily = typed_attribute()\n    #: the local socket address of the underlying socket\n    local_address: SockAddrType = typed_attribute()\n    #: for IP addresses, the local port the underlying socket is bound to\n    local_port: int = typed_attribute()\n    #: the underlying stdlib socket object\n    raw_socket: socket.socket = typed_attribute()\n    #: the remote address the underlying socket is connected to\n    remote_address: SockAddrType = typed_attribute()\n    #: for IP addresses, the remote port the underlying socket is connected to\n    remote_port: int = typed_attribute()\n\n\nclass _SocketProvider(TypedAttributeProvider):\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        from .._core._sockets import convert_ipv6_sockaddr as convert\n\n        attributes: dict[Any, Callable[[], Any]] = {\n            SocketAttribute.family: lambda: self._raw_socket.family,\n            SocketAttribute.local_address: lambda: convert(\n                self._raw_socket.getsockname()\n            ),\n            SocketAttribute.raw_socket: lambda: self._raw_socket,\n        }\n        try:\n            peername: tuple[str, int] | None = convert(self._raw_socket.getpeername())\n        except OSError:\n            peername = None\n\n        # Provide the remote address for connected sockets\n        if peername is not None:\n            attributes[SocketAttribute.remote_address] = lambda: peername\n\n        # Provide local and remote ports for IP based sockets\n        if self._raw_socket.family in (AddressFamily.AF_INET, AddressFamily.AF_INET6):\n            attributes[SocketAttribute.local_port] = (\n                lambda: self._raw_socket.getsockname()[1]\n            )\n            if peername is not None:\n                remote_port = peername[1]\n                attributes[SocketAttribute.remote_port] = lambda: remote_port\n\n        return attributes\n\n    @property\n    @abstractmethod\n    def _raw_socket(self) -> socket.socket:\n        pass\n\n\nclass SocketStream(ByteStream, _SocketProvider):\n    \"\"\"\n    Transports bytes over a socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n\nclass UNIXSocketStream(SocketStream):\n    @abstractmethod\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        \"\"\"\n        Send file descriptors along with a message to the peer.\n\n        :param message: a non-empty bytestring\n        :param fds: a collection of files (either numeric file descriptors or open file\n            or socket objects)\n        \"\"\"\n\n    @abstractmethod\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        \"\"\"\n        Receive file descriptors along with a message from the peer.\n\n        :param msglen: length of the message to expect from the peer\n        :param maxfds: maximum number of file descriptors to expect from the peer\n        :return: a tuple of (message, file descriptors)\n        \"\"\"\n\n\nclass SocketListener(Listener[SocketStream], _SocketProvider):\n    \"\"\"\n    Listens to incoming socket connections.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    @abstractmethod\n    async def accept(self) -> SocketStream:\n        \"\"\"Accept an incoming connection.\"\"\"\n\n    async def serve(\n        self,\n        handler: Callable[[SocketStream], Any],\n        task_group: TaskGroup | None = None,\n    ) -> None:\n        from .. import create_task_group\n\n        async with AsyncExitStack() as stack:\n            if task_group is None:\n                task_group = await stack.enter_async_context(create_task_group())\n\n            while True:\n                stream = await self.accept()\n                task_group.start_soon(handler, stream)\n\n\nclass UDPSocket(UnreliableObjectStream[UDPPacketType], _SocketProvider):\n    \"\"\"\n    Represents an unconnected UDP socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    async def sendto(self, data: bytes, host: str, port: int) -> None:\n        \"\"\"\n        Alias for :meth:`~.UnreliableObjectSendStream.send` ((data, (host, port))).\n\n        \"\"\"\n        return await self.send((data, (host, port)))\n\n\nclass ConnectedUDPSocket(UnreliableObjectStream[bytes], _SocketProvider):\n    \"\"\"\n    Represents an connected UDP socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n\nclass UNIXDatagramSocket(\n    UnreliableObjectStream[UNIXDatagramPacketType], _SocketProvider\n):\n    \"\"\"\n    Represents an unconnected Unix datagram socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    async def sendto(self, data: bytes, path: str) -> None:\n        \"\"\"Alias for :meth:`~.UnreliableObjectSendStream.send` ((data, path)).\"\"\"\n        return await self.send((data, path))\n\n\nclass ConnectedUNIXDatagramSocket(UnreliableObjectStream[bytes], _SocketProvider):\n    \"\"\"\n    Represents a connected Unix datagram socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n", "src/anyio/abc/_resources.py": "from __future__ import annotations\n\nfrom abc import ABCMeta, abstractmethod\nfrom types import TracebackType\nfrom typing import TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass AsyncResource(metaclass=ABCMeta):\n    \"\"\"\n    Abstract base class for all closeable asynchronous resources.\n\n    Works as an asynchronous context manager which returns the instance itself on enter,\n    and calls :meth:`aclose` on exit.\n    \"\"\"\n\n    __slots__ = ()\n\n    async def __aenter__(self: T) -> T:\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.aclose()\n\n    @abstractmethod\n    async def aclose(self) -> None:\n        \"\"\"Close the resource.\"\"\"\n", "src/anyio/abc/_eventloop.py": "from __future__ import annotations\n\nimport math\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom collections.abc import AsyncIterator, Awaitable, Mapping\nfrom os import PathLike\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind, socket\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ContextManager,\n    Sequence,\n    TypeVar,\n    overload,\n)\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from .._core._synchronization import CapacityLimiter, Event\n    from .._core._tasks import CancelScope\n    from .._core._testing import TaskInfo\n    from ..from_thread import BlockingPortal\n    from ._sockets import (\n        ConnectedUDPSocket,\n        ConnectedUNIXDatagramSocket,\n        IPSockAddrType,\n        SocketListener,\n        SocketStream,\n        UDPSocket,\n        UNIXDatagramSocket,\n        UNIXSocketStream,\n    )\n    from ._subprocesses import Process\n    from ._tasks import TaskGroup\n    from ._testing import TestRunner\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\n\nclass AsyncBackend(metaclass=ABCMeta):\n    @classmethod\n    @abstractmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        \"\"\"\n        Run the given coroutine function in an asynchronous event loop.\n\n        The current thread must not be already running an event loop.\n\n        :param func: a coroutine function\n        :param args: positional arguments to ``func``\n        :param kwargs: positional arguments to ``func``\n        :param options: keyword arguments to call the backend ``run()`` implementation\n            with\n        :return: the return value of the coroutine function\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def current_token(cls) -> object:\n        \"\"\"\n\n        :return:\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def current_time(cls) -> float:\n        \"\"\"\n        Return the current value of the event loop's internal clock.\n\n        :return: the clock value (seconds)\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        \"\"\"Return the exception class that is raised in a task if it's cancelled.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    async def checkpoint(cls) -> None:\n        \"\"\"\n        Check if the task has been cancelled, and allow rescheduling of other tasks.\n\n        This is effectively the same as running :meth:`checkpoint_if_cancelled` and then\n        :meth:`cancel_shielded_checkpoint`.\n        \"\"\"\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        \"\"\"\n        Check if the current task group has been cancelled.\n\n        This will check if the task has been cancelled, but will not allow other tasks\n        to be scheduled if not.\n\n        \"\"\"\n        if cls.current_effective_deadline() == -math.inf:\n            await cls.checkpoint()\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        \"\"\"\n        Allow the rescheduling of other tasks.\n\n        This will give other tasks the opportunity to run, but without checking if the\n        current task group has been cancelled, unlike with :meth:`checkpoint`.\n\n        \"\"\"\n        with cls.create_cancel_scope(shield=True):\n            await cls.sleep(0)\n\n    @classmethod\n    @abstractmethod\n    async def sleep(cls, delay: float) -> None:\n        \"\"\"\n        Pause the current task for the specified duration.\n\n        :param delay: the duration, in seconds\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def current_effective_deadline(cls) -> float:\n        \"\"\"\n        Return the nearest deadline among all the cancel scopes effective for the\n        current task.\n\n        :return:\n            - a clock value from the event loop's internal clock\n            - ``inf`` if there is no deadline in effect\n            - ``-inf`` if the current scope has been cancelled\n        :rtype: float\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def create_task_group(cls) -> TaskGroup:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_event(cls) -> Event:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> CapacityLimiter:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        abandon_on_cancel: bool = False,\n        limiter: CapacityLimiter | None = None,\n    ) -> T_Retval:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def check_cancelled(cls) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def run_sync_from_thread(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], T_Retval],\n        args: tuple[Unpack[PosArgsT]],\n        token: object,\n    ) -> T_Retval:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_blocking_portal(cls) -> BlockingPortal:\n        pass\n\n    @classmethod\n    @overload\n    async def open_process(\n        cls,\n        command: str | bytes,\n        *,\n        shell: Literal[True],\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike[str] | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        pass\n\n    @classmethod\n    @overload\n    async def open_process(\n        cls,\n        command: Sequence[str | bytes],\n        *,\n        shell: Literal[False],\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike[str] | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike[str] | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[Process]) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> SocketStream:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def connect_unix(cls, path: str | bytes) -> UNIXSocketStream:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_tcp_listener(cls, sock: socket) -> SocketListener:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_unix_listener(cls, sock: socket) -> SocketListener:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def create_udp_socket(\n        cls,\n        family: AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        pass\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket, remote_path: None\n    ) -> UNIXDatagramSocket: ...\n\n    @classmethod\n    @overload\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket, remote_path: str | bytes\n    ) -> ConnectedUNIXDatagramSocket: ...\n\n    @classmethod\n    @abstractmethod\n    async def create_unix_datagram_socket(\n        cls, raw_socket: socket, remote_path: str | bytes | None\n    ) -> UNIXDatagramSocket | ConnectedUNIXDatagramSocket:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def wait_socket_readable(cls, sock: socket) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def wait_socket_writable(cls, sock: socket) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def get_current_task(cls) -> TaskInfo:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def get_running_tasks(cls) -> Sequence[TaskInfo]:\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        pass\n", "src/anyio/abc/_streams.py": "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom collections.abc import Callable\nfrom typing import Any, Generic, TypeVar, Union\n\nfrom .._core._exceptions import EndOfStream\nfrom .._core._typedattr import TypedAttributeProvider\nfrom ._resources import AsyncResource\nfrom ._tasks import TaskGroup\n\nT_Item = TypeVar(\"T_Item\")\nT_co = TypeVar(\"T_co\", covariant=True)\nT_contra = TypeVar(\"T_contra\", contravariant=True)\n\n\nclass UnreliableObjectReceiveStream(\n    Generic[T_co], AsyncResource, TypedAttributeProvider\n):\n    \"\"\"\n    An interface for receiving objects.\n\n    This interface makes no guarantees that the received messages arrive in the order in\n    which they were sent, or that no messages are missed.\n\n    Asynchronously iterating over objects of this type will yield objects matching the\n    given type parameter.\n    \"\"\"\n\n    def __aiter__(self) -> UnreliableObjectReceiveStream[T_co]:\n        return self\n\n    async def __anext__(self) -> T_co:\n        try:\n            return await self.receive()\n        except EndOfStream:\n            raise StopAsyncIteration\n\n    @abstractmethod\n    async def receive(self) -> T_co:\n        \"\"\"\n        Receive the next item.\n\n        :raises ~anyio.ClosedResourceError: if the receive stream has been explicitly\n            closed\n        :raises ~anyio.EndOfStream: if this stream has been closed from the other end\n        :raises ~anyio.BrokenResourceError: if this stream has been rendered unusable\n            due to external causes\n        \"\"\"\n\n\nclass UnreliableObjectSendStream(\n    Generic[T_contra], AsyncResource, TypedAttributeProvider\n):\n    \"\"\"\n    An interface for sending objects.\n\n    This interface makes no guarantees that the messages sent will reach the\n    recipient(s) in the same order in which they were sent, or at all.\n    \"\"\"\n\n    @abstractmethod\n    async def send(self, item: T_contra) -> None:\n        \"\"\"\n        Send an item to the peer(s).\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if the send stream has been explicitly\n            closed\n        :raises ~anyio.BrokenResourceError: if this stream has been rendered unusable\n            due to external causes\n        \"\"\"\n\n\nclass UnreliableObjectStream(\n    UnreliableObjectReceiveStream[T_Item], UnreliableObjectSendStream[T_Item]\n):\n    \"\"\"\n    A bidirectional message stream which does not guarantee the order or reliability of\n    message delivery.\n    \"\"\"\n\n\nclass ObjectReceiveStream(UnreliableObjectReceiveStream[T_co]):\n    \"\"\"\n    A receive message stream which guarantees that messages are received in the same\n    order in which they were sent, and that no messages are missed.\n    \"\"\"\n\n\nclass ObjectSendStream(UnreliableObjectSendStream[T_contra]):\n    \"\"\"\n    A send message stream which guarantees that messages are delivered in the same order\n    in which they were sent, without missing any messages in the middle.\n    \"\"\"\n\n\nclass ObjectStream(\n    ObjectReceiveStream[T_Item],\n    ObjectSendStream[T_Item],\n    UnreliableObjectStream[T_Item],\n):\n    \"\"\"\n    A bidirectional message stream which guarantees the order and reliability of message\n    delivery.\n    \"\"\"\n\n    @abstractmethod\n    async def send_eof(self) -> None:\n        \"\"\"\n        Send an end-of-file indication to the peer.\n\n        You should not try to send any further data to this stream after calling this\n        method. This method is idempotent (does nothing on successive calls).\n        \"\"\"\n\n\nclass ByteReceiveStream(AsyncResource, TypedAttributeProvider):\n    \"\"\"\n    An interface for receiving bytes from a single peer.\n\n    Iterating this byte stream will yield a byte string of arbitrary length, but no more\n    than 65536 bytes.\n    \"\"\"\n\n    def __aiter__(self) -> ByteReceiveStream:\n        return self\n\n    async def __anext__(self) -> bytes:\n        try:\n            return await self.receive()\n        except EndOfStream:\n            raise StopAsyncIteration\n\n    @abstractmethod\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        \"\"\"\n        Receive at most ``max_bytes`` bytes from the peer.\n\n        .. note:: Implementors of this interface should not return an empty\n            :class:`bytes` object, and users should ignore them.\n\n        :param max_bytes: maximum number of bytes to receive\n        :return: the received bytes\n        :raises ~anyio.EndOfStream: if this stream has been closed from the other end\n        \"\"\"\n\n\nclass ByteSendStream(AsyncResource, TypedAttributeProvider):\n    \"\"\"An interface for sending bytes to a single peer.\"\"\"\n\n    @abstractmethod\n    async def send(self, item: bytes) -> None:\n        \"\"\"\n        Send the given bytes to the peer.\n\n        :param item: the bytes to send\n        \"\"\"\n\n\nclass ByteStream(ByteReceiveStream, ByteSendStream):\n    \"\"\"A bidirectional byte stream.\"\"\"\n\n    @abstractmethod\n    async def send_eof(self) -> None:\n        \"\"\"\n        Send an end-of-file indication to the peer.\n\n        You should not try to send any further data to this stream after calling this\n        method. This method is idempotent (does nothing on successive calls).\n        \"\"\"\n\n\n#: Type alias for all unreliable bytes-oriented receive streams.\nAnyUnreliableByteReceiveStream = Union[\n    UnreliableObjectReceiveStream[bytes], ByteReceiveStream\n]\n#: Type alias for all unreliable bytes-oriented send streams.\nAnyUnreliableByteSendStream = Union[UnreliableObjectSendStream[bytes], ByteSendStream]\n#: Type alias for all unreliable bytes-oriented streams.\nAnyUnreliableByteStream = Union[UnreliableObjectStream[bytes], ByteStream]\n#: Type alias for all bytes-oriented receive streams.\nAnyByteReceiveStream = Union[ObjectReceiveStream[bytes], ByteReceiveStream]\n#: Type alias for all bytes-oriented send streams.\nAnyByteSendStream = Union[ObjectSendStream[bytes], ByteSendStream]\n#: Type alias for all bytes-oriented streams.\nAnyByteStream = Union[ObjectStream[bytes], ByteStream]\n\n\nclass Listener(Generic[T_co], AsyncResource, TypedAttributeProvider):\n    \"\"\"An interface for objects that let you accept incoming connections.\"\"\"\n\n    @abstractmethod\n    async def serve(\n        self, handler: Callable[[T_co], Any], task_group: TaskGroup | None = None\n    ) -> None:\n        \"\"\"\n        Accept incoming connections as they come in and start tasks to handle them.\n\n        :param handler: a callable that will be used to handle each accepted connection\n        :param task_group: the task group that will be used to start tasks for handling\n            each accepted connection (if omitted, an ad-hoc task group will be created)\n        \"\"\"\n", "src/anyio/abc/_subprocesses.py": "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom signal import Signals\n\nfrom ._resources import AsyncResource\nfrom ._streams import ByteReceiveStream, ByteSendStream\n\n\nclass Process(AsyncResource):\n    \"\"\"An asynchronous version of :class:`subprocess.Popen`.\"\"\"\n\n    @abstractmethod\n    async def wait(self) -> int:\n        \"\"\"\n        Wait until the process exits.\n\n        :return: the exit code of the process\n        \"\"\"\n\n    @abstractmethod\n    def terminate(self) -> None:\n        \"\"\"\n        Terminates the process, gracefully if possible.\n\n        On Windows, this calls ``TerminateProcess()``.\n        On POSIX systems, this sends ``SIGTERM`` to the process.\n\n        .. seealso:: :meth:`subprocess.Popen.terminate`\n        \"\"\"\n\n    @abstractmethod\n    def kill(self) -> None:\n        \"\"\"\n        Kills the process.\n\n        On Windows, this calls ``TerminateProcess()``.\n        On POSIX systems, this sends ``SIGKILL`` to the process.\n\n        .. seealso:: :meth:`subprocess.Popen.kill`\n        \"\"\"\n\n    @abstractmethod\n    def send_signal(self, signal: Signals) -> None:\n        \"\"\"\n        Send a signal to the subprocess.\n\n        .. seealso:: :meth:`subprocess.Popen.send_signal`\n\n        :param signal: the signal number (e.g. :data:`signal.SIGHUP`)\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def pid(self) -> int:\n        \"\"\"The process ID of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def returncode(self) -> int | None:\n        \"\"\"\n        The return code of the process. If the process has not yet terminated, this will\n        be ``None``.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def stdin(self) -> ByteSendStream | None:\n        \"\"\"The stream for the standard input of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def stdout(self) -> ByteReceiveStream | None:\n        \"\"\"The stream for the standard output of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def stderr(self) -> ByteReceiveStream | None:\n        \"\"\"The stream for the standard error output of the process.\"\"\"\n", "src/anyio/abc/__init__.py": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom ._eventloop import AsyncBackend as AsyncBackend\nfrom ._resources import AsyncResource as AsyncResource\nfrom ._sockets import ConnectedUDPSocket as ConnectedUDPSocket\nfrom ._sockets import ConnectedUNIXDatagramSocket as ConnectedUNIXDatagramSocket\nfrom ._sockets import IPAddressType as IPAddressType\nfrom ._sockets import IPSockAddrType as IPSockAddrType\nfrom ._sockets import SocketAttribute as SocketAttribute\nfrom ._sockets import SocketListener as SocketListener\nfrom ._sockets import SocketStream as SocketStream\nfrom ._sockets import UDPPacketType as UDPPacketType\nfrom ._sockets import UDPSocket as UDPSocket\nfrom ._sockets import UNIXDatagramPacketType as UNIXDatagramPacketType\nfrom ._sockets import UNIXDatagramSocket as UNIXDatagramSocket\nfrom ._sockets import UNIXSocketStream as UNIXSocketStream\nfrom ._streams import AnyByteReceiveStream as AnyByteReceiveStream\nfrom ._streams import AnyByteSendStream as AnyByteSendStream\nfrom ._streams import AnyByteStream as AnyByteStream\nfrom ._streams import AnyUnreliableByteReceiveStream as AnyUnreliableByteReceiveStream\nfrom ._streams import AnyUnreliableByteSendStream as AnyUnreliableByteSendStream\nfrom ._streams import AnyUnreliableByteStream as AnyUnreliableByteStream\nfrom ._streams import ByteReceiveStream as ByteReceiveStream\nfrom ._streams import ByteSendStream as ByteSendStream\nfrom ._streams import ByteStream as ByteStream\nfrom ._streams import Listener as Listener\nfrom ._streams import ObjectReceiveStream as ObjectReceiveStream\nfrom ._streams import ObjectSendStream as ObjectSendStream\nfrom ._streams import ObjectStream as ObjectStream\nfrom ._streams import UnreliableObjectReceiveStream as UnreliableObjectReceiveStream\nfrom ._streams import UnreliableObjectSendStream as UnreliableObjectSendStream\nfrom ._streams import UnreliableObjectStream as UnreliableObjectStream\nfrom ._subprocesses import Process as Process\nfrom ._tasks import TaskGroup as TaskGroup\nfrom ._tasks import TaskStatus as TaskStatus\nfrom ._testing import TestRunner as TestRunner\n\n# Re-exported here, for backwards compatibility\n# isort: off\nfrom .._core._synchronization import (\n    CapacityLimiter as CapacityLimiter,\n    Condition as Condition,\n    Event as Event,\n    Lock as Lock,\n    Semaphore as Semaphore,\n)\nfrom .._core._tasks import CancelScope as CancelScope\nfrom ..from_thread import BlockingPortal as BlockingPortal\n\n# Re-export imports so they look like they live directly in this package\nkey: str\nvalue: Any\nfor key, value in list(locals().items()):\n    if getattr(value, \"__module__\", \"\").startswith(\"anyio.abc.\"):\n        value.__module__ = __name__\n", "src/anyio/_core/_tasks.py": "from __future__ import annotations\n\nimport math\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom types import TracebackType\n\nfrom ..abc._tasks import TaskGroup, TaskStatus\nfrom ._eventloop import get_async_backend\n\n\nclass _IgnoredTaskStatus(TaskStatus[object]):\n    def started(self, value: object = None) -> None:\n        pass\n\n\nTASK_STATUS_IGNORED = _IgnoredTaskStatus()\n\n\nclass CancelScope:\n    \"\"\"\n    Wraps a unit of work that can be made separately cancellable.\n\n    :param deadline: The time (clock value) when this scope is cancelled automatically\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    \"\"\"\n\n    def __new__(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return get_async_backend().create_cancel_scope(shield=shield, deadline=deadline)\n\n    def cancel(self) -> None:\n        \"\"\"Cancel this scope immediately.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def deadline(self) -> float:\n        \"\"\"\n        The time (clock value) when this scope is cancelled automatically.\n\n        Will be ``float('inf')`` if no timeout has been set.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        raise NotImplementedError\n\n    @property\n    def cancel_called(self) -> bool:\n        \"\"\"``True`` if :meth:`cancel` has been called.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def cancelled_caught(self) -> bool:\n        \"\"\"\n        ``True`` if this scope suppressed a cancellation exception it itself raised.\n\n        This is typically used to check if any work was interrupted, or to see if the\n        scope was cancelled due to its deadline being reached. The value will, however,\n        only be ``True`` if the cancellation was triggered by the scope itself (and not\n        an outer scope).\n\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def shield(self) -> bool:\n        \"\"\"\n        ``True`` if this scope is shielded from external cancellation.\n\n        While a scope is shielded, it will not receive cancellations from outside.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        raise NotImplementedError\n\n    def __enter__(self) -> CancelScope:\n        raise NotImplementedError\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        raise NotImplementedError\n\n\n@contextmanager\ndef fail_after(\n    delay: float | None, shield: bool = False\n) -> Generator[CancelScope, None, None]:\n    \"\"\"\n    Create a context manager which raises a :class:`TimeoutError` if does not finish in\n    time.\n\n    :param delay: maximum allowed time (in seconds) before raising the exception, or\n        ``None`` to disable the timeout\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a context manager that yields a cancel scope\n    :rtype: :class:`~typing.ContextManager`\\\\[:class:`~anyio.CancelScope`\\\\]\n\n    \"\"\"\n    current_time = get_async_backend().current_time\n    deadline = (current_time() + delay) if delay is not None else math.inf\n    with get_async_backend().create_cancel_scope(\n        deadline=deadline, shield=shield\n    ) as cancel_scope:\n        yield cancel_scope\n\n    if cancel_scope.cancelled_caught and current_time() >= cancel_scope.deadline:\n        raise TimeoutError\n\n\ndef move_on_after(delay: float | None, shield: bool = False) -> CancelScope:\n    \"\"\"\n    Create a cancel scope with a deadline that expires after the given delay.\n\n    :param delay: maximum allowed time (in seconds) before exiting the context block, or\n        ``None`` to disable the timeout\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a cancel scope\n\n    \"\"\"\n    deadline = (\n        (get_async_backend().current_time() + delay) if delay is not None else math.inf\n    )\n    return get_async_backend().create_cancel_scope(deadline=deadline, shield=shield)\n\n\ndef current_effective_deadline() -> float:\n    \"\"\"\n    Return the nearest deadline among all the cancel scopes effective for the current\n    task.\n\n    :return: a clock value from the event loop's internal clock (or ``float('inf')`` if\n        there is no deadline in effect, or ``float('-inf')`` if the current scope has\n        been cancelled)\n    :rtype: float\n\n    \"\"\"\n    return get_async_backend().current_effective_deadline()\n\n\ndef create_task_group() -> TaskGroup:\n    \"\"\"\n    Create a task group.\n\n    :return: a task group\n\n    \"\"\"\n    return get_async_backend().create_task_group()\n", "src/anyio/_core/_sockets.py": "from __future__ import annotations\n\nimport errno\nimport os\nimport socket\nimport ssl\nimport stat\nimport sys\nfrom collections.abc import Awaitable\nfrom ipaddress import IPv6Address, ip_address\nfrom os import PathLike, chmod\nfrom socket import AddressFamily, SocketKind\nfrom typing import Any, Literal, cast, overload\n\nfrom .. import to_thread\nfrom ..abc import (\n    ConnectedUDPSocket,\n    ConnectedUNIXDatagramSocket,\n    IPAddressType,\n    IPSockAddrType,\n    SocketListener,\n    SocketStream,\n    UDPSocket,\n    UNIXDatagramSocket,\n    UNIXSocketStream,\n)\nfrom ..streams.stapled import MultiListener\nfrom ..streams.tls import TLSStream\nfrom ._eventloop import get_async_backend\nfrom ._resources import aclose_forcefully\nfrom ._synchronization import Event\nfrom ._tasks import create_task_group, move_on_after\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import ExceptionGroup\n\nIPPROTO_IPV6 = getattr(socket, \"IPPROTO_IPV6\", 41)  # https://bugs.python.org/issue29515\n\nAnyIPAddressFamily = Literal[\n    AddressFamily.AF_UNSPEC, AddressFamily.AF_INET, AddressFamily.AF_INET6\n]\nIPAddressFamily = Literal[AddressFamily.AF_INET, AddressFamily.AF_INET6]\n\n\n# tls_hostname given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# ssl_context given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    ssl_context: ssl.SSLContext,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# tls=True\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    tls: Literal[True],\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# tls=False\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    tls: Literal[False],\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> SocketStream: ...\n\n\n# No TLS arguments\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> SocketStream: ...\n\n\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = None,\n    tls: bool = False,\n    ssl_context: ssl.SSLContext | None = None,\n    tls_standard_compatible: bool = True,\n    tls_hostname: str | None = None,\n    happy_eyeballs_delay: float = 0.25,\n) -> SocketStream | TLSStream:\n    \"\"\"\n    Connect to a host using the TCP protocol.\n\n    This function implements the stateless version of the Happy Eyeballs algorithm (RFC\n    6555). If ``remote_host`` is a host name that resolves to multiple IP addresses,\n    each one is tried until one connection attempt succeeds. If the first attempt does\n    not connected within 250 milliseconds, a second attempt is started using the next\n    address in the list, and so on. On IPv6 enabled systems, an IPv6 address (if\n    available) is tried first.\n\n    When the connection has been established, a TLS handshake will be done if either\n    ``ssl_context`` or ``tls_hostname`` is not ``None``, or if ``tls`` is ``True``.\n\n    :param remote_host: the IP address or host name to connect to\n    :param remote_port: port on the target host to connect to\n    :param local_host: the interface address or name to bind the socket to before\n        connecting\n    :param tls: ``True`` to do a TLS handshake with the connected stream and return a\n        :class:`~anyio.streams.tls.TLSStream` instead\n    :param ssl_context: the SSL context object to use (if omitted, a default context is\n        created)\n    :param tls_standard_compatible: If ``True``, performs the TLS shutdown handshake\n        before closing the stream and requires that the server does this as well.\n        Otherwise, :exc:`~ssl.SSLEOFError` may be raised during reads from the stream.\n        Some protocols, such as HTTP, require this option to be ``False``.\n        See :meth:`~ssl.SSLContext.wrap_socket` for details.\n    :param tls_hostname: host name to check the server certificate against (defaults to\n        the value of ``remote_host``)\n    :param happy_eyeballs_delay: delay (in seconds) before starting the next connection\n        attempt\n    :return: a socket stream object if no TLS handshake was done, otherwise a TLS stream\n    :raises OSError: if the connection attempt fails\n\n    \"\"\"\n    # Placed here due to https://github.com/python/mypy/issues/7057\n    connected_stream: SocketStream | None = None\n\n    async def try_connect(remote_host: str, event: Event) -> None:\n        nonlocal connected_stream\n        try:\n            stream = await asynclib.connect_tcp(remote_host, remote_port, local_address)\n        except OSError as exc:\n            oserrors.append(exc)\n            return\n        else:\n            if connected_stream is None:\n                connected_stream = stream\n                tg.cancel_scope.cancel()\n            else:\n                await stream.aclose()\n        finally:\n            event.set()\n\n    asynclib = get_async_backend()\n    local_address: IPSockAddrType | None = None\n    family = socket.AF_UNSPEC\n    if local_host:\n        gai_res = await getaddrinfo(str(local_host), None)\n        family, *_, local_address = gai_res[0]\n\n    target_host = str(remote_host)\n    try:\n        addr_obj = ip_address(remote_host)\n    except ValueError:\n        # getaddrinfo() will raise an exception if name resolution fails\n        gai_res = await getaddrinfo(\n            target_host, remote_port, family=family, type=socket.SOCK_STREAM\n        )\n\n        # Organize the list so that the first address is an IPv6 address (if available)\n        # and the second one is an IPv4 addresses. The rest can be in whatever order.\n        v6_found = v4_found = False\n        target_addrs: list[tuple[socket.AddressFamily, str]] = []\n        for af, *rest, sa in gai_res:\n            if af == socket.AF_INET6 and not v6_found:\n                v6_found = True\n                target_addrs.insert(0, (af, sa[0]))\n            elif af == socket.AF_INET and not v4_found and v6_found:\n                v4_found = True\n                target_addrs.insert(1, (af, sa[0]))\n            else:\n                target_addrs.append((af, sa[0]))\n    else:\n        if isinstance(addr_obj, IPv6Address):\n            target_addrs = [(socket.AF_INET6, addr_obj.compressed)]\n        else:\n            target_addrs = [(socket.AF_INET, addr_obj.compressed)]\n\n    oserrors: list[OSError] = []\n    async with create_task_group() as tg:\n        for i, (af, addr) in enumerate(target_addrs):\n            event = Event()\n            tg.start_soon(try_connect, addr, event)\n            with move_on_after(happy_eyeballs_delay):\n                await event.wait()\n\n    if connected_stream is None:\n        cause = (\n            oserrors[0]\n            if len(oserrors) == 1\n            else ExceptionGroup(\"multiple connection attempts failed\", oserrors)\n        )\n        raise OSError(\"All connection attempts failed\") from cause\n\n    if tls or tls_hostname or ssl_context:\n        try:\n            return await TLSStream.wrap(\n                connected_stream,\n                server_side=False,\n                hostname=tls_hostname or str(remote_host),\n                ssl_context=ssl_context,\n                standard_compatible=tls_standard_compatible,\n            )\n        except BaseException:\n            await aclose_forcefully(connected_stream)\n            raise\n\n    return connected_stream\n\n\nasync def connect_unix(path: str | bytes | PathLike[Any]) -> UNIXSocketStream:\n    \"\"\"\n    Connect to the given UNIX socket.\n\n    Not available on Windows.\n\n    :param path: path to the socket\n    :return: a socket stream object\n\n    \"\"\"\n    path = os.fspath(path)\n    return await get_async_backend().connect_unix(path)\n\n\nasync def create_tcp_listener(\n    *,\n    local_host: IPAddressType | None = None,\n    local_port: int = 0,\n    family: AnyIPAddressFamily = socket.AddressFamily.AF_UNSPEC,\n    backlog: int = 65536,\n    reuse_port: bool = False,\n) -> MultiListener[SocketStream]:\n    \"\"\"\n    Create a TCP socket listener.\n\n    :param local_port: port number to listen on\n    :param local_host: IP address of the interface to listen on. If omitted, listen on\n        all IPv4 and IPv6 interfaces. To listen on all interfaces on a specific address\n        family, use ``0.0.0.0`` for IPv4 or ``::`` for IPv6.\n    :param family: address family (used if ``local_host`` was omitted)\n    :param backlog: maximum number of queued incoming connections (up to a maximum of\n        2**16, or 65536)\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same\n        address/port (not supported on Windows)\n    :return: a list of listener objects\n\n    \"\"\"\n    asynclib = get_async_backend()\n    backlog = min(backlog, 65536)\n    local_host = str(local_host) if local_host is not None else None\n    gai_res = await getaddrinfo(\n        local_host,\n        local_port,\n        family=family,\n        type=socket.SocketKind.SOCK_STREAM if sys.platform == \"win32\" else 0,\n        flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG,\n    )\n    listeners: list[SocketListener] = []\n    try:\n        # The set() is here to work around a glibc bug:\n        # https://sourceware.org/bugzilla/show_bug.cgi?id=14969\n        sockaddr: tuple[str, int] | tuple[str, int, int, int]\n        for fam, kind, *_, sockaddr in sorted(set(gai_res)):\n            # Workaround for an uvloop bug where we don't get the correct scope ID for\n            # IPv6 link-local addresses when passing type=socket.SOCK_STREAM to\n            # getaddrinfo(): https://github.com/MagicStack/uvloop/issues/539\n            if sys.platform != \"win32\" and kind is not SocketKind.SOCK_STREAM:\n                continue\n\n            raw_socket = socket.socket(fam)\n            raw_socket.setblocking(False)\n\n            # For Windows, enable exclusive address use. For others, enable address\n            # reuse.\n            if sys.platform == \"win32\":\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_EXCLUSIVEADDRUSE, 1)\n            else:\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n            if reuse_port:\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n\n            # If only IPv6 was requested, disable dual stack operation\n            if fam == socket.AF_INET6:\n                raw_socket.setsockopt(IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)\n\n                # Workaround for #554\n                if \"%\" in sockaddr[0]:\n                    addr, scope_id = sockaddr[0].split(\"%\", 1)\n                    sockaddr = (addr, sockaddr[1], 0, int(scope_id))\n\n            raw_socket.bind(sockaddr)\n            raw_socket.listen(backlog)\n            listener = asynclib.create_tcp_listener(raw_socket)\n            listeners.append(listener)\n    except BaseException:\n        for listener in listeners:\n            await listener.aclose()\n\n        raise\n\n    return MultiListener(listeners)\n\n\nasync def create_unix_listener(\n    path: str | bytes | PathLike[Any],\n    *,\n    mode: int | None = None,\n    backlog: int = 65536,\n) -> SocketListener:\n    \"\"\"\n    Create a UNIX socket listener.\n\n    Not available on Windows.\n\n    :param path: path of the socket\n    :param mode: permissions to set on the socket\n    :param backlog: maximum number of queued incoming connections (up to a maximum of\n        2**16, or 65536)\n    :return: a listener object\n\n    .. versionchanged:: 3.0\n        If a socket already exists on the file system in the given path, it will be\n        removed first.\n\n    \"\"\"\n    backlog = min(backlog, 65536)\n    raw_socket = await setup_unix_local_socket(path, mode, socket.SOCK_STREAM)\n    try:\n        raw_socket.listen(backlog)\n        return get_async_backend().create_unix_listener(raw_socket)\n    except BaseException:\n        raw_socket.close()\n        raise\n\n\nasync def create_udp_socket(\n    family: AnyIPAddressFamily = AddressFamily.AF_UNSPEC,\n    *,\n    local_host: IPAddressType | None = None,\n    local_port: int = 0,\n    reuse_port: bool = False,\n) -> UDPSocket:\n    \"\"\"\n    Create a UDP socket.\n\n    If ``port`` has been given, the socket will be bound to this port on the local\n    machine, making this socket suitable for providing UDP based services.\n\n    :param family: address family (``AF_INET`` or ``AF_INET6``) \u2013 automatically\n        determined from ``local_host`` if omitted\n    :param local_host: IP address or host name of the local interface to bind to\n    :param local_port: local port to bind to\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same\n        address/port (not supported on Windows)\n    :return: a UDP socket\n\n    \"\"\"\n    if family is AddressFamily.AF_UNSPEC and not local_host:\n        raise ValueError('Either \"family\" or \"local_host\" must be given')\n\n    if local_host:\n        gai_res = await getaddrinfo(\n            str(local_host),\n            local_port,\n            family=family,\n            type=socket.SOCK_DGRAM,\n            flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG,\n        )\n        family = cast(AnyIPAddressFamily, gai_res[0][0])\n        local_address = gai_res[0][-1]\n    elif family is AddressFamily.AF_INET6:\n        local_address = (\"::\", 0)\n    else:\n        local_address = (\"0.0.0.0\", 0)\n\n    sock = await get_async_backend().create_udp_socket(\n        family, local_address, None, reuse_port\n    )\n    return cast(UDPSocket, sock)\n\n\nasync def create_connected_udp_socket(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    family: AnyIPAddressFamily = AddressFamily.AF_UNSPEC,\n    local_host: IPAddressType | None = None,\n    local_port: int = 0,\n    reuse_port: bool = False,\n) -> ConnectedUDPSocket:\n    \"\"\"\n    Create a connected UDP socket.\n\n    Connected UDP sockets can only communicate with the specified remote host/port, an\n    any packets sent from other sources are dropped.\n\n    :param remote_host: remote host to set as the default target\n    :param remote_port: port on the remote host to set as the default target\n    :param family: address family (``AF_INET`` or ``AF_INET6``) \u2013 automatically\n        determined from ``local_host`` or ``remote_host`` if omitted\n    :param local_host: IP address or host name of the local interface to bind to\n    :param local_port: local port to bind to\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same\n        address/port (not supported on Windows)\n    :return: a connected UDP socket\n\n    \"\"\"\n    local_address = None\n    if local_host:\n        gai_res = await getaddrinfo(\n            str(local_host),\n            local_port,\n            family=family,\n            type=socket.SOCK_DGRAM,\n            flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG,\n        )\n        family = cast(AnyIPAddressFamily, gai_res[0][0])\n        local_address = gai_res[0][-1]\n\n    gai_res = await getaddrinfo(\n        str(remote_host), remote_port, family=family, type=socket.SOCK_DGRAM\n    )\n    family = cast(AnyIPAddressFamily, gai_res[0][0])\n    remote_address = gai_res[0][-1]\n\n    sock = await get_async_backend().create_udp_socket(\n        family, local_address, remote_address, reuse_port\n    )\n    return cast(ConnectedUDPSocket, sock)\n\n\nasync def create_unix_datagram_socket(\n    *,\n    local_path: None | str | bytes | PathLike[Any] = None,\n    local_mode: int | None = None,\n) -> UNIXDatagramSocket:\n    \"\"\"\n    Create a UNIX datagram socket.\n\n    Not available on Windows.\n\n    If ``local_path`` has been given, the socket will be bound to this path, making this\n    socket suitable for receiving datagrams from other processes. Other processes can\n    send datagrams to this socket only if ``local_path`` is set.\n\n    If a socket already exists on the file system in the ``local_path``, it will be\n    removed first.\n\n    :param local_path: the path on which to bind to\n    :param local_mode: permissions to set on the local socket\n    :return: a UNIX datagram socket\n\n    \"\"\"\n    raw_socket = await setup_unix_local_socket(\n        local_path, local_mode, socket.SOCK_DGRAM\n    )\n    return await get_async_backend().create_unix_datagram_socket(raw_socket, None)\n\n\nasync def create_connected_unix_datagram_socket(\n    remote_path: str | bytes | PathLike[Any],\n    *,\n    local_path: None | str | bytes | PathLike[Any] = None,\n    local_mode: int | None = None,\n) -> ConnectedUNIXDatagramSocket:\n    \"\"\"\n    Create a connected UNIX datagram socket.\n\n    Connected datagram sockets can only communicate with the specified remote path.\n\n    If ``local_path`` has been given, the socket will be bound to this path, making\n    this socket suitable for receiving datagrams from other processes. Other processes\n    can send datagrams to this socket only if ``local_path`` is set.\n\n    If a socket already exists on the file system in the ``local_path``, it will be\n    removed first.\n\n    :param remote_path: the path to set as the default target\n    :param local_path: the path on which to bind to\n    :param local_mode: permissions to set on the local socket\n    :return: a connected UNIX datagram socket\n\n    \"\"\"\n    remote_path = os.fspath(remote_path)\n    raw_socket = await setup_unix_local_socket(\n        local_path, local_mode, socket.SOCK_DGRAM\n    )\n    return await get_async_backend().create_unix_datagram_socket(\n        raw_socket, remote_path\n    )\n\n\nasync def getaddrinfo(\n    host: bytes | str | None,\n    port: str | int | None,\n    *,\n    family: int | AddressFamily = 0,\n    type: int | SocketKind = 0,\n    proto: int = 0,\n    flags: int = 0,\n) -> list[tuple[AddressFamily, SocketKind, int, str, tuple[str, int]]]:\n    \"\"\"\n    Look up a numeric IP address given a host name.\n\n    Internationalized domain names are translated according to the (non-transitional)\n    IDNA 2008 standard.\n\n    .. note:: 4-tuple IPv6 socket addresses are automatically converted to 2-tuples of\n        (host, port), unlike what :func:`socket.getaddrinfo` does.\n\n    :param host: host name\n    :param port: port number\n    :param family: socket family (`'AF_INET``, ...)\n    :param type: socket type (``SOCK_STREAM``, ...)\n    :param proto: protocol number\n    :param flags: flags to pass to upstream ``getaddrinfo()``\n    :return: list of tuples containing (family, type, proto, canonname, sockaddr)\n\n    .. seealso:: :func:`socket.getaddrinfo`\n\n    \"\"\"\n    # Handle unicode hostnames\n    if isinstance(host, str):\n        try:\n            encoded_host: bytes | None = host.encode(\"ascii\")\n        except UnicodeEncodeError:\n            import idna\n\n            encoded_host = idna.encode(host, uts46=True)\n    else:\n        encoded_host = host\n\n    gai_res = await get_async_backend().getaddrinfo(\n        encoded_host, port, family=family, type=type, proto=proto, flags=flags\n    )\n    return [\n        (family, type, proto, canonname, convert_ipv6_sockaddr(sockaddr))\n        for family, type, proto, canonname, sockaddr in gai_res\n    ]\n\n\ndef getnameinfo(sockaddr: IPSockAddrType, flags: int = 0) -> Awaitable[tuple[str, str]]:\n    \"\"\"\n    Look up the host name of an IP address.\n\n    :param sockaddr: socket address (e.g. (ipaddress, port) for IPv4)\n    :param flags: flags to pass to upstream ``getnameinfo()``\n    :return: a tuple of (host name, service name)\n\n    .. seealso:: :func:`socket.getnameinfo`\n\n    \"\"\"\n    return get_async_backend().getnameinfo(sockaddr, flags)\n\n\ndef wait_socket_readable(sock: socket.socket) -> Awaitable[None]:\n    \"\"\"\n    Wait until the given socket has data to be read.\n\n    This does **NOT** work on Windows when using the asyncio backend with a proactor\n    event loop (default on py3.8+).\n\n    .. warning:: Only use this on raw sockets that have not been wrapped by any higher\n        level constructs like socket streams!\n\n    :param sock: a socket object\n    :raises ~anyio.ClosedResourceError: if the socket was closed while waiting for the\n        socket to become readable\n    :raises ~anyio.BusyResourceError: if another task is already waiting for the socket\n        to become readable\n\n    \"\"\"\n    return get_async_backend().wait_socket_readable(sock)\n\n\ndef wait_socket_writable(sock: socket.socket) -> Awaitable[None]:\n    \"\"\"\n    Wait until the given socket can be written to.\n\n    This does **NOT** work on Windows when using the asyncio backend with a proactor\n    event loop (default on py3.8+).\n\n    .. warning:: Only use this on raw sockets that have not been wrapped by any higher\n        level constructs like socket streams!\n\n    :param sock: a socket object\n    :raises ~anyio.ClosedResourceError: if the socket was closed while waiting for the\n        socket to become writable\n    :raises ~anyio.BusyResourceError: if another task is already waiting for the socket\n        to become writable\n\n    \"\"\"\n    return get_async_backend().wait_socket_writable(sock)\n\n\n#\n# Private API\n#\n\n\ndef convert_ipv6_sockaddr(\n    sockaddr: tuple[str, int, int, int] | tuple[str, int],\n) -> tuple[str, int]:\n    \"\"\"\n    Convert a 4-tuple IPv6 socket address to a 2-tuple (address, port) format.\n\n    If the scope ID is nonzero, it is added to the address, separated with ``%``.\n    Otherwise the flow id and scope id are simply cut off from the tuple.\n    Any other kinds of socket addresses are returned as-is.\n\n    :param sockaddr: the result of :meth:`~socket.socket.getsockname`\n    :return: the converted socket address\n\n    \"\"\"\n    # This is more complicated than it should be because of MyPy\n    if isinstance(sockaddr, tuple) and len(sockaddr) == 4:\n        host, port, flowinfo, scope_id = sockaddr\n        if scope_id:\n            # PyPy (as of v7.3.11) leaves the interface name in the result, so\n            # we discard it and only get the scope ID from the end\n            # (https://foss.heptapod.net/pypy/pypy/-/issues/3938)\n            host = host.split(\"%\")[0]\n\n            # Add scope_id to the address\n            return f\"{host}%{scope_id}\", port\n        else:\n            return host, port\n    else:\n        return sockaddr\n\n\nasync def setup_unix_local_socket(\n    path: None | str | bytes | PathLike[Any],\n    mode: int | None,\n    socktype: int,\n) -> socket.socket:\n    \"\"\"\n    Create a UNIX local socket object, deleting the socket at the given path if it\n    exists.\n\n    Not available on Windows.\n\n    :param path: path of the socket\n    :param mode: permissions to set on the socket\n    :param socktype: socket.SOCK_STREAM or socket.SOCK_DGRAM\n\n    \"\"\"\n    path_str: str | bytes | None\n    if path is not None:\n        path_str = os.fspath(path)\n\n        # Copied from pathlib...\n        try:\n            stat_result = os.stat(path)\n        except OSError as e:\n            if e.errno not in (errno.ENOENT, errno.ENOTDIR, errno.EBADF, errno.ELOOP):\n                raise\n        else:\n            if stat.S_ISSOCK(stat_result.st_mode):\n                os.unlink(path)\n    else:\n        path_str = None\n\n    raw_socket = socket.socket(socket.AF_UNIX, socktype)\n    raw_socket.setblocking(False)\n\n    if path_str is not None:\n        try:\n            await to_thread.run_sync(raw_socket.bind, path_str, abandon_on_cancel=True)\n            if mode is not None:\n                await to_thread.run_sync(chmod, path_str, mode, abandon_on_cancel=True)\n        except BaseException:\n            raw_socket.close()\n            raise\n\n    return raw_socket\n", "src/anyio/_core/_exceptions.py": "from __future__ import annotations\n\n\nclass BrokenResourceError(Exception):\n    \"\"\"\n    Raised when trying to use a resource that has been rendered unusable due to external\n    causes (e.g. a send stream whose peer has disconnected).\n    \"\"\"\n\n\nclass BrokenWorkerProcess(Exception):\n    \"\"\"\n    Raised by :func:`run_sync_in_process` if the worker process terminates abruptly or\n    otherwise misbehaves.\n    \"\"\"\n\n\nclass BusyResourceError(Exception):\n    \"\"\"\n    Raised when two tasks are trying to read from or write to the same resource\n    concurrently.\n    \"\"\"\n\n    def __init__(self, action: str):\n        super().__init__(f\"Another task is already {action} this resource\")\n\n\nclass ClosedResourceError(Exception):\n    \"\"\"Raised when trying to use a resource that has been closed.\"\"\"\n\n\nclass DelimiterNotFound(Exception):\n    \"\"\"\n    Raised during\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the\n    maximum number of bytes has been read without the delimiter being found.\n    \"\"\"\n\n    def __init__(self, max_bytes: int) -> None:\n        super().__init__(\n            f\"The delimiter was not found among the first {max_bytes} bytes\"\n        )\n\n\nclass EndOfStream(Exception):\n    \"\"\"\n    Raised when trying to read from a stream that has been closed from the other end.\n    \"\"\"\n\n\nclass IncompleteRead(Exception):\n    \"\"\"\n    Raised during\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_exactly` or\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the\n    connection is closed before the requested amount of bytes has been read.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(\n            \"The stream was closed before the read operation could be completed\"\n        )\n\n\nclass TypedAttributeLookupError(LookupError):\n    \"\"\"\n    Raised by :meth:`~anyio.TypedAttributeProvider.extra` when the given typed attribute\n    is not found and no default value has been given.\n    \"\"\"\n\n\nclass WouldBlock(Exception):\n    \"\"\"Raised by ``X_nowait`` functions if ``X()`` would block.\"\"\"\n", "src/anyio/_core/_signals.py": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterator\nfrom signal import Signals\nfrom typing import ContextManager\n\nfrom ._eventloop import get_async_backend\n\n\ndef open_signal_receiver(*signals: Signals) -> ContextManager[AsyncIterator[Signals]]:\n    \"\"\"\n    Start receiving operating system signals.\n\n    :param signals: signals to receive (e.g. ``signal.SIGINT``)\n    :return: an asynchronous context manager for an asynchronous iterator which yields\n        signal numbers\n\n    .. warning:: Windows does not support signals natively so it is best to avoid\n        relying on this in cross-platform applications.\n\n    .. warning:: On asyncio, this permanently replaces any previous signal handler for\n        the given signals, as set via :meth:`~asyncio.loop.add_signal_handler`.\n\n    \"\"\"\n    return get_async_backend().open_signal_receiver(*signals)\n", "src/anyio/_core/_resources.py": "from __future__ import annotations\n\nfrom ..abc import AsyncResource\nfrom ._tasks import CancelScope\n\n\nasync def aclose_forcefully(resource: AsyncResource) -> None:\n    \"\"\"\n    Close an asynchronous resource in a cancelled scope.\n\n    Doing this closes the resource without waiting on anything.\n\n    :param resource: the resource to close\n\n    \"\"\"\n    with CancelScope() as scope:\n        scope.cancel()\n        await resource.aclose()\n", "src/anyio/_core/_eventloop.py": "from __future__ import annotations\n\nimport math\nimport sys\nimport threading\nfrom collections.abc import Awaitable, Callable, Generator\nfrom contextlib import contextmanager\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nimport sniffio\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nif TYPE_CHECKING:\n    from ..abc import AsyncBackend\n\n# This must be updated when new backends are introduced\nBACKENDS = \"asyncio\", \"trio\"\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n\nthreadlocals = threading.local()\nloaded_backends: dict[str, type[AsyncBackend]] = {}\n\n\ndef run(\n    func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n    *args: Unpack[PosArgsT],\n    backend: str = \"asyncio\",\n    backend_options: dict[str, Any] | None = None,\n) -> T_Retval:\n    \"\"\"\n    Run the given coroutine function in an asynchronous event loop.\n\n    The current thread must not be already running an event loop.\n\n    :param func: a coroutine function\n    :param args: positional arguments to ``func``\n    :param backend: name of the asynchronous event loop implementation \u2013 currently\n        either ``asyncio`` or ``trio``\n    :param backend_options: keyword arguments to call the backend ``run()``\n        implementation with (documented :ref:`here <backend options>`)\n    :return: the return value of the coroutine function\n    :raises RuntimeError: if an asynchronous event loop is already running in this\n        thread\n    :raises LookupError: if the named backend is not found\n\n    \"\"\"\n    try:\n        asynclib_name = sniffio.current_async_library()\n    except sniffio.AsyncLibraryNotFoundError:\n        pass\n    else:\n        raise RuntimeError(f\"Already running {asynclib_name} in this thread\")\n\n    try:\n        async_backend = get_async_backend(backend)\n    except ImportError as exc:\n        raise LookupError(f\"No such backend: {backend}\") from exc\n\n    token = None\n    if sniffio.current_async_library_cvar.get(None) is None:\n        # Since we're in control of the event loop, we can cache the name of the async\n        # library\n        token = sniffio.current_async_library_cvar.set(backend)\n\n    try:\n        backend_options = backend_options or {}\n        return async_backend.run(func, args, {}, backend_options)\n    finally:\n        if token:\n            sniffio.current_async_library_cvar.reset(token)\n\n\nasync def sleep(delay: float) -> None:\n    \"\"\"\n    Pause the current task for the specified duration.\n\n    :param delay: the duration, in seconds\n\n    \"\"\"\n    return await get_async_backend().sleep(delay)\n\n\nasync def sleep_forever() -> None:\n    \"\"\"\n    Pause the current task until it's cancelled.\n\n    This is a shortcut for ``sleep(math.inf)``.\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    await sleep(math.inf)\n\n\nasync def sleep_until(deadline: float) -> None:\n    \"\"\"\n    Pause the current task until the given time.\n\n    :param deadline: the absolute time to wake up at (according to the internal\n        monotonic clock of the event loop)\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    now = current_time()\n    await sleep(max(deadline - now, 0))\n\n\ndef current_time() -> float:\n    \"\"\"\n    Return the current value of the event loop's internal clock.\n\n    :return: the clock value (seconds)\n\n    \"\"\"\n    return get_async_backend().current_time()\n\n\ndef get_all_backends() -> tuple[str, ...]:\n    \"\"\"Return a tuple of the names of all built-in backends.\"\"\"\n    return BACKENDS\n\n\ndef get_cancelled_exc_class() -> type[BaseException]:\n    \"\"\"Return the current async library's cancellation exception class.\"\"\"\n    return get_async_backend().cancelled_exception_class()\n\n\n#\n# Private API\n#\n\n\n@contextmanager\ndef claim_worker_thread(\n    backend_class: type[AsyncBackend], token: object\n) -> Generator[Any, None, None]:\n    threadlocals.current_async_backend = backend_class\n    threadlocals.current_token = token\n    try:\n        yield\n    finally:\n        del threadlocals.current_async_backend\n        del threadlocals.current_token\n\n\ndef get_async_backend(asynclib_name: str | None = None) -> type[AsyncBackend]:\n    if asynclib_name is None:\n        asynclib_name = sniffio.current_async_library()\n\n    # We use our own dict instead of sys.modules to get the already imported back-end\n    # class because the appropriate modules in sys.modules could potentially be only\n    # partially initialized\n    try:\n        return loaded_backends[asynclib_name]\n    except KeyError:\n        module = import_module(f\"anyio._backends._{asynclib_name}\")\n        loaded_backends[asynclib_name] = module.backend_class\n        return module.backend_class\n", "src/anyio/_core/_typedattr.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nfrom typing import Any, TypeVar, final, overload\n\nfrom ._exceptions import TypedAttributeLookupError\n\nT_Attr = TypeVar(\"T_Attr\")\nT_Default = TypeVar(\"T_Default\")\nundefined = object()\n\n\ndef typed_attribute() -> Any:\n    \"\"\"Return a unique object, used to mark typed attributes.\"\"\"\n    return object()\n\n\nclass TypedAttributeSet:\n    \"\"\"\n    Superclass for typed attribute collections.\n\n    Checks that every public attribute of every subclass has a type annotation.\n    \"\"\"\n\n    def __init_subclass__(cls) -> None:\n        annotations: dict[str, Any] = getattr(cls, \"__annotations__\", {})\n        for attrname in dir(cls):\n            if not attrname.startswith(\"_\") and attrname not in annotations:\n                raise TypeError(\n                    f\"Attribute {attrname!r} is missing its type annotation\"\n                )\n\n        super().__init_subclass__()\n\n\nclass TypedAttributeProvider:\n    \"\"\"Base class for classes that wish to provide typed extra attributes.\"\"\"\n\n    @property\n    def extra_attributes(self) -> Mapping[T_Attr, Callable[[], T_Attr]]:\n        \"\"\"\n        A mapping of the extra attributes to callables that return the corresponding\n        values.\n\n        If the provider wraps another provider, the attributes from that wrapper should\n        also be included in the returned mapping (but the wrapper may override the\n        callables from the wrapped instance).\n\n        \"\"\"\n        return {}\n\n    @overload\n    def extra(self, attribute: T_Attr) -> T_Attr: ...\n\n    @overload\n    def extra(self, attribute: T_Attr, default: T_Default) -> T_Attr | T_Default: ...\n\n    @final\n    def extra(self, attribute: Any, default: object = undefined) -> object:\n        \"\"\"\n        extra(attribute, default=undefined)\n\n        Return the value of the given typed extra attribute.\n\n        :param attribute: the attribute (member of a :class:`~TypedAttributeSet`) to\n            look for\n        :param default: the value that should be returned if no value is found for the\n            attribute\n        :raises ~anyio.TypedAttributeLookupError: if the search failed and no default\n            value was given\n\n        \"\"\"\n        try:\n            getter = self.extra_attributes[attribute]\n        except KeyError:\n            if default is undefined:\n                raise TypedAttributeLookupError(\"Attribute not found\") from None\n            else:\n                return default\n\n        return getter()\n", "src/anyio/_core/_streams.py": "from __future__ import annotations\n\nimport math\nfrom typing import Tuple, TypeVar\nfrom warnings import warn\n\nfrom ..streams.memory import (\n    MemoryObjectReceiveStream,\n    MemoryObjectSendStream,\n    MemoryObjectStreamState,\n)\n\nT_Item = TypeVar(\"T_Item\")\n\n\nclass create_memory_object_stream(\n    Tuple[MemoryObjectSendStream[T_Item], MemoryObjectReceiveStream[T_Item]],\n):\n    \"\"\"\n    Create a memory object stream.\n\n    The stream's item type can be annotated like\n    :func:`create_memory_object_stream[T_Item]`.\n\n    :param max_buffer_size: number of items held in the buffer until ``send()`` starts\n        blocking\n    :param item_type: old way of marking the streams with the right generic type for\n        static typing (does nothing on AnyIO 4)\n\n        .. deprecated:: 4.0\n          Use ``create_memory_object_stream[YourItemType](...)`` instead.\n    :return: a tuple of (send stream, receive stream)\n\n    \"\"\"\n\n    def __new__(  # type: ignore[misc]\n        cls, max_buffer_size: float = 0, item_type: object = None\n    ) -> tuple[MemoryObjectSendStream[T_Item], MemoryObjectReceiveStream[T_Item]]:\n        if max_buffer_size != math.inf and not isinstance(max_buffer_size, int):\n            raise ValueError(\"max_buffer_size must be either an integer or math.inf\")\n        if max_buffer_size < 0:\n            raise ValueError(\"max_buffer_size cannot be negative\")\n        if item_type is not None:\n            warn(\n                \"The item_type argument has been deprecated in AnyIO 4.0. \"\n                \"Use create_memory_object_stream[YourItemType](...) instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        state = MemoryObjectStreamState[T_Item](max_buffer_size)\n        return (MemoryObjectSendStream(state), MemoryObjectReceiveStream(state))\n", "src/anyio/_core/_subprocesses.py": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterable, Mapping, Sequence\nfrom io import BytesIO\nfrom os import PathLike\nfrom subprocess import DEVNULL, PIPE, CalledProcessError, CompletedProcess\nfrom typing import IO, Any, cast\n\nfrom ..abc import Process\nfrom ._eventloop import get_async_backend\nfrom ._tasks import create_task_group\n\n\nasync def run_process(\n    command: str | bytes | Sequence[str | bytes],\n    *,\n    input: bytes | None = None,\n    stdout: int | IO[Any] | None = PIPE,\n    stderr: int | IO[Any] | None = PIPE,\n    check: bool = True,\n    cwd: str | bytes | PathLike[str] | None = None,\n    env: Mapping[str, str] | None = None,\n    start_new_session: bool = False,\n) -> CompletedProcess[bytes]:\n    \"\"\"\n    Run an external command in a subprocess and wait until it completes.\n\n    .. seealso:: :func:`subprocess.run`\n\n    :param command: either a string to pass to the shell, or an iterable of strings\n        containing the executable name or path and its arguments\n    :param input: bytes passed to the standard input of the subprocess\n    :param stdout: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        a file-like object, or `None`\n    :param stderr: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        :data:`subprocess.STDOUT`, a file-like object, or `None`\n    :param check: if ``True``, raise :exc:`~subprocess.CalledProcessError` if the\n        process terminates with a return code other than 0\n    :param cwd: If not ``None``, change the working directory to this before running the\n        command\n    :param env: if not ``None``, this mapping replaces the inherited environment\n        variables from the parent process\n    :param start_new_session: if ``true`` the setsid() system call will be made in the\n        child process prior to the execution of the subprocess. (POSIX only)\n    :return: an object representing the completed process\n    :raises ~subprocess.CalledProcessError: if ``check`` is ``True`` and the process\n        exits with a nonzero return code\n\n    \"\"\"\n\n    async def drain_stream(stream: AsyncIterable[bytes], index: int) -> None:\n        buffer = BytesIO()\n        async for chunk in stream:\n            buffer.write(chunk)\n\n        stream_contents[index] = buffer.getvalue()\n\n    async with await open_process(\n        command,\n        stdin=PIPE if input else DEVNULL,\n        stdout=stdout,\n        stderr=stderr,\n        cwd=cwd,\n        env=env,\n        start_new_session=start_new_session,\n    ) as process:\n        stream_contents: list[bytes | None] = [None, None]\n        async with create_task_group() as tg:\n            if process.stdout:\n                tg.start_soon(drain_stream, process.stdout, 0)\n\n            if process.stderr:\n                tg.start_soon(drain_stream, process.stderr, 1)\n\n            if process.stdin and input:\n                await process.stdin.send(input)\n                await process.stdin.aclose()\n\n            await process.wait()\n\n    output, errors = stream_contents\n    if check and process.returncode != 0:\n        raise CalledProcessError(cast(int, process.returncode), command, output, errors)\n\n    return CompletedProcess(command, cast(int, process.returncode), output, errors)\n\n\nasync def open_process(\n    command: str | bytes | Sequence[str | bytes],\n    *,\n    stdin: int | IO[Any] | None = PIPE,\n    stdout: int | IO[Any] | None = PIPE,\n    stderr: int | IO[Any] | None = PIPE,\n    cwd: str | bytes | PathLike[str] | None = None,\n    env: Mapping[str, str] | None = None,\n    start_new_session: bool = False,\n) -> Process:\n    \"\"\"\n    Start an external command in a subprocess.\n\n    .. seealso:: :class:`subprocess.Popen`\n\n    :param command: either a string to pass to the shell, or an iterable of strings\n        containing the executable name or path and its arguments\n    :param stdin: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`, a\n        file-like object, or ``None``\n    :param stdout: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        a file-like object, or ``None``\n    :param stderr: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL`,\n        :data:`subprocess.STDOUT`, a file-like object, or ``None``\n    :param cwd: If not ``None``, the working directory is changed before executing\n    :param env: If env is not ``None``, it must be a mapping that defines the\n        environment variables for the new process\n    :param start_new_session: if ``true`` the setsid() system call will be made in the\n        child process prior to the execution of the subprocess. (POSIX only)\n    :return: an asynchronous process object\n\n    \"\"\"\n    if isinstance(command, (str, bytes)):\n        return await get_async_backend().open_process(\n            command,\n            shell=True,\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n    else:\n        return await get_async_backend().open_process(\n            command,\n            shell=False,\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n", "src/anyio/_core/_synchronization.py": "from __future__ import annotations\n\nimport math\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom types import TracebackType\n\nfrom sniffio import AsyncLibraryNotFoundError\n\nfrom ..lowlevel import cancel_shielded_checkpoint, checkpoint, checkpoint_if_cancelled\nfrom ._eventloop import get_async_backend\nfrom ._exceptions import BusyResourceError, WouldBlock\nfrom ._tasks import CancelScope\nfrom ._testing import TaskInfo, get_current_task\n\n\n@dataclass(frozen=True)\nclass EventStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Event.wait`\n    \"\"\"\n\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass CapacityLimiterStatistics:\n    \"\"\"\n    :ivar int borrowed_tokens: number of tokens currently borrowed by tasks\n    :ivar float total_tokens: total number of available tokens\n    :ivar tuple borrowers: tasks or other objects currently holding tokens borrowed from\n        this limiter\n    :ivar int tasks_waiting: number of tasks waiting on\n        :meth:`~.CapacityLimiter.acquire` or\n        :meth:`~.CapacityLimiter.acquire_on_behalf_of`\n    \"\"\"\n\n    borrowed_tokens: int\n    total_tokens: float\n    borrowers: tuple[object, ...]\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass LockStatistics:\n    \"\"\"\n    :ivar bool locked: flag indicating if this lock is locked or not\n    :ivar ~anyio.TaskInfo owner: task currently holding the lock (or ``None`` if the\n        lock is not held by any task)\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Lock.acquire`\n    \"\"\"\n\n    locked: bool\n    owner: TaskInfo | None\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass ConditionStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks blocked on :meth:`~.Condition.wait`\n    :ivar ~anyio.LockStatistics lock_statistics: statistics of the underlying\n        :class:`~.Lock`\n    \"\"\"\n\n    tasks_waiting: int\n    lock_statistics: LockStatistics\n\n\n@dataclass(frozen=True)\nclass SemaphoreStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Semaphore.acquire`\n\n    \"\"\"\n\n    tasks_waiting: int\n\n\nclass Event:\n    def __new__(cls) -> Event:\n        try:\n            return get_async_backend().create_event()\n        except AsyncLibraryNotFoundError:\n            return EventAdapter()\n\n    def set(self) -> None:\n        \"\"\"Set the flag, notifying all listeners.\"\"\"\n        raise NotImplementedError\n\n    def is_set(self) -> bool:\n        \"\"\"Return ``True`` if the flag is set, ``False`` if not.\"\"\"\n        raise NotImplementedError\n\n    async def wait(self) -> None:\n        \"\"\"\n        Wait until the flag has been set.\n\n        If the flag has already been set when this method is called, it returns\n        immediately.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def statistics(self) -> EventStatistics:\n        \"\"\"Return statistics about the current state of this event.\"\"\"\n        raise NotImplementedError\n\n\nclass EventAdapter(Event):\n    _internal_event: Event | None = None\n\n    def __new__(cls) -> EventAdapter:\n        return object.__new__(cls)\n\n    @property\n    def _event(self) -> Event:\n        if self._internal_event is None:\n            self._internal_event = get_async_backend().create_event()\n\n        return self._internal_event\n\n    def set(self) -> None:\n        self._event.set()\n\n    def is_set(self) -> bool:\n        return self._internal_event is not None and self._internal_event.is_set()\n\n    async def wait(self) -> None:\n        await self._event.wait()\n\n    def statistics(self) -> EventStatistics:\n        if self._internal_event is None:\n            return EventStatistics(tasks_waiting=0)\n\n        return self._internal_event.statistics()\n\n\nclass Lock:\n    _owner_task: TaskInfo | None = None\n\n    def __init__(self) -> None:\n        self._waiters: deque[tuple[TaskInfo, Event]] = deque()\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire the lock.\"\"\"\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_nowait()\n        except WouldBlock:\n            task = get_current_task()\n            event = Event()\n            token = task, event\n            self._waiters.append(token)\n            try:\n                await event.wait()\n            except BaseException:\n                if not event.is_set():\n                    self._waiters.remove(token)\n                elif self._owner_task == task:\n                    self.release()\n\n                raise\n\n            assert self._owner_task == task\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the lock, without blocking.\n\n        :raises ~anyio.WouldBlock: if the operation would block\n\n        \"\"\"\n        task = get_current_task()\n        if self._owner_task == task:\n            raise RuntimeError(\"Attempted to acquire an already held Lock\")\n\n        if self._owner_task is not None:\n            raise WouldBlock\n\n        self._owner_task = task\n\n    def release(self) -> None:\n        \"\"\"Release the lock.\"\"\"\n        if self._owner_task != get_current_task():\n            raise RuntimeError(\"The current task is not holding this lock\")\n\n        if self._waiters:\n            self._owner_task, event = self._waiters.popleft()\n            event.set()\n        else:\n            del self._owner_task\n\n    def locked(self) -> bool:\n        \"\"\"Return True if the lock is currently held.\"\"\"\n        return self._owner_task is not None\n\n    def statistics(self) -> LockStatistics:\n        \"\"\"\n        Return statistics about the current state of this lock.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return LockStatistics(self.locked(), self._owner_task, len(self._waiters))\n\n\nclass Condition:\n    _owner_task: TaskInfo | None = None\n\n    def __init__(self, lock: Lock | None = None):\n        self._lock = lock or Lock()\n        self._waiters: deque[Event] = deque()\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    def _check_acquired(self) -> None:\n        if self._owner_task != get_current_task():\n            raise RuntimeError(\"The current task is not holding the underlying lock\")\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire the underlying lock.\"\"\"\n        await self._lock.acquire()\n        self._owner_task = get_current_task()\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the underlying lock, without blocking.\n\n        :raises ~anyio.WouldBlock: if the operation would block\n\n        \"\"\"\n        self._lock.acquire_nowait()\n        self._owner_task = get_current_task()\n\n    def release(self) -> None:\n        \"\"\"Release the underlying lock.\"\"\"\n        self._lock.release()\n\n    def locked(self) -> bool:\n        \"\"\"Return True if the lock is set.\"\"\"\n        return self._lock.locked()\n\n    def notify(self, n: int = 1) -> None:\n        \"\"\"Notify exactly n listeners.\"\"\"\n        self._check_acquired()\n        for _ in range(n):\n            try:\n                event = self._waiters.popleft()\n            except IndexError:\n                break\n\n            event.set()\n\n    def notify_all(self) -> None:\n        \"\"\"Notify all the listeners.\"\"\"\n        self._check_acquired()\n        for event in self._waiters:\n            event.set()\n\n        self._waiters.clear()\n\n    async def wait(self) -> None:\n        \"\"\"Wait for a notification.\"\"\"\n        await checkpoint()\n        event = Event()\n        self._waiters.append(event)\n        self.release()\n        try:\n            await event.wait()\n        except BaseException:\n            if not event.is_set():\n                self._waiters.remove(event)\n\n            raise\n        finally:\n            with CancelScope(shield=True):\n                await self.acquire()\n\n    def statistics(self) -> ConditionStatistics:\n        \"\"\"\n        Return statistics about the current state of this condition.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return ConditionStatistics(len(self._waiters), self._lock.statistics())\n\n\nclass Semaphore:\n    def __init__(self, initial_value: int, *, max_value: int | None = None):\n        if not isinstance(initial_value, int):\n            raise TypeError(\"initial_value must be an integer\")\n        if initial_value < 0:\n            raise ValueError(\"initial_value must be >= 0\")\n        if max_value is not None:\n            if not isinstance(max_value, int):\n                raise TypeError(\"max_value must be an integer or None\")\n            if max_value < initial_value:\n                raise ValueError(\n                    \"max_value must be equal to or higher than initial_value\"\n                )\n\n        self._value = initial_value\n        self._max_value = max_value\n        self._waiters: deque[Event] = deque()\n\n    async def __aenter__(self) -> Semaphore:\n        await self.acquire()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    async def acquire(self) -> None:\n        \"\"\"Decrement the semaphore value, blocking if necessary.\"\"\"\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_nowait()\n        except WouldBlock:\n            event = Event()\n            self._waiters.append(event)\n            try:\n                await event.wait()\n            except BaseException:\n                if not event.is_set():\n                    self._waiters.remove(event)\n                else:\n                    self.release()\n\n                raise\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the underlying lock, without blocking.\n\n        :raises ~anyio.WouldBlock: if the operation would block\n\n        \"\"\"\n        if self._value == 0:\n            raise WouldBlock\n\n        self._value -= 1\n\n    def release(self) -> None:\n        \"\"\"Increment the semaphore value.\"\"\"\n        if self._max_value is not None and self._value == self._max_value:\n            raise ValueError(\"semaphore released too many times\")\n\n        if self._waiters:\n            self._waiters.popleft().set()\n        else:\n            self._value += 1\n\n    @property\n    def value(self) -> int:\n        \"\"\"The current value of the semaphore.\"\"\"\n        return self._value\n\n    @property\n    def max_value(self) -> int | None:\n        \"\"\"The maximum value of the semaphore.\"\"\"\n        return self._max_value\n\n    def statistics(self) -> SemaphoreStatistics:\n        \"\"\"\n        Return statistics about the current state of this semaphore.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return SemaphoreStatistics(len(self._waiters))\n\n\nclass CapacityLimiter:\n    def __new__(cls, total_tokens: float) -> CapacityLimiter:\n        try:\n            return get_async_backend().create_capacity_limiter(total_tokens)\n        except AsyncLibraryNotFoundError:\n            return CapacityLimiterAdapter(total_tokens)\n\n    async def __aenter__(self) -> None:\n        raise NotImplementedError\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        raise NotImplementedError\n\n    @property\n    def total_tokens(self) -> float:\n        \"\"\"\n        The total number of tokens available for borrowing.\n\n        This is a read-write property. If the total number of tokens is increased, the\n        proportionate number of tasks waiting on this limiter will be granted their\n        tokens.\n\n        .. versionchanged:: 3.0\n            The property is now writable.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        raise NotImplementedError\n\n    @property\n    def borrowed_tokens(self) -> int:\n        \"\"\"The number of tokens that have currently been borrowed.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def available_tokens(self) -> float:\n        \"\"\"The number of tokens currently available to be borrowed\"\"\"\n        raise NotImplementedError\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire a token for the current task without waiting for one to become\n        available.\n\n        :raises ~anyio.WouldBlock: if there are no tokens available for borrowing\n\n        \"\"\"\n        raise NotImplementedError\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        \"\"\"\n        Acquire a token without waiting for one to become available.\n\n        :param borrower: the entity borrowing a token\n        :raises ~anyio.WouldBlock: if there are no tokens available for borrowing\n\n        \"\"\"\n        raise NotImplementedError\n\n    async def acquire(self) -> None:\n        \"\"\"\n        Acquire a token for the current task, waiting if necessary for one to become\n        available.\n\n        \"\"\"\n        raise NotImplementedError\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        \"\"\"\n        Acquire a token, waiting if necessary for one to become available.\n\n        :param borrower: the entity borrowing a token\n\n        \"\"\"\n        raise NotImplementedError\n\n    def release(self) -> None:\n        \"\"\"\n        Release the token held by the current task.\n\n        :raises RuntimeError: if the current task has not borrowed a token from this\n            limiter.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        \"\"\"\n        Release the token held by the given borrower.\n\n        :raises RuntimeError: if the borrower has not borrowed a token from this\n            limiter.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        \"\"\"\n        Return statistics about the current state of this limiter.\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass CapacityLimiterAdapter(CapacityLimiter):\n    _internal_limiter: CapacityLimiter | None = None\n\n    def __new__(cls, total_tokens: float) -> CapacityLimiterAdapter:\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float) -> None:\n        self.total_tokens = total_tokens\n\n    @property\n    def _limiter(self) -> CapacityLimiter:\n        if self._internal_limiter is None:\n            self._internal_limiter = get_async_backend().create_capacity_limiter(\n                self._total_tokens\n            )\n\n        return self._internal_limiter\n\n    async def __aenter__(self) -> None:\n        await self._limiter.__aenter__()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        return await self._limiter.__aexit__(exc_type, exc_val, exc_tb)\n\n    @property\n    def total_tokens(self) -> float:\n        if self._internal_limiter is None:\n            return self._total_tokens\n\n        return self._internal_limiter.total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and value is not math.inf:\n            raise TypeError(\"total_tokens must be an int or math.inf\")\n        elif value < 1:\n            raise ValueError(\"total_tokens must be >= 1\")\n\n        if self._internal_limiter is None:\n            self._total_tokens = value\n            return\n\n        self._limiter.total_tokens = value\n\n    @property\n    def borrowed_tokens(self) -> int:\n        if self._internal_limiter is None:\n            return 0\n\n        return self._internal_limiter.borrowed_tokens\n\n    @property\n    def available_tokens(self) -> float:\n        if self._internal_limiter is None:\n            return self._total_tokens\n\n        return self._internal_limiter.available_tokens\n\n    def acquire_nowait(self) -> None:\n        self._limiter.acquire_nowait()\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        self._limiter.acquire_on_behalf_of_nowait(borrower)\n\n    async def acquire(self) -> None:\n        await self._limiter.acquire()\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await self._limiter.acquire_on_behalf_of(borrower)\n\n    def release(self) -> None:\n        self._limiter.release()\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        self._limiter.release_on_behalf_of(borrower)\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        if self._internal_limiter is None:\n            return CapacityLimiterStatistics(\n                borrowed_tokens=0,\n                total_tokens=self.total_tokens,\n                borrowers=(),\n                tasks_waiting=0,\n            )\n\n        return self._internal_limiter.statistics()\n\n\nclass ResourceGuard:\n    \"\"\"\n    A context manager for ensuring that a resource is only used by a single task at a\n    time.\n\n    Entering this context manager while the previous has not exited it yet will trigger\n    :exc:`BusyResourceError`.\n\n    :param action: the action to guard against (visible in the :exc:`BusyResourceError`\n        when triggered, e.g. \"Another task is already {action} this resource\")\n\n    .. versionadded:: 4.1\n    \"\"\"\n\n    __slots__ = \"action\", \"_guarded\"\n\n    def __init__(self, action: str = \"using\"):\n        self.action: str = action\n        self._guarded = False\n\n    def __enter__(self) -> None:\n        if self._guarded:\n            raise BusyResourceError(self.action)\n\n        self._guarded = True\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        self._guarded = False\n        return None\n", "src/anyio/_core/__init__.py": "", "src/anyio/_core/_fileio.py": "from __future__ import annotations\n\nimport os\nimport pathlib\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom os import PathLike\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    AsyncIterator,\n    Final,\n    Generic,\n    overload,\n)\n\nfrom .. import to_thread\nfrom ..abc import AsyncResource\n\nif TYPE_CHECKING:\n    from _typeshed import OpenBinaryMode, OpenTextMode, ReadableBuffer, WriteableBuffer\nelse:\n    ReadableBuffer = OpenBinaryMode = OpenTextMode = WriteableBuffer = object\n\n\nclass AsyncFile(AsyncResource, Generic[AnyStr]):\n    \"\"\"\n    An asynchronous file object.\n\n    This class wraps a standard file object and provides async friendly versions of the\n    following blocking methods (where available on the original file object):\n\n    * read\n    * read1\n    * readline\n    * readlines\n    * readinto\n    * readinto1\n    * write\n    * writelines\n    * truncate\n    * seek\n    * tell\n    * flush\n\n    All other methods are directly passed through.\n\n    This class supports the asynchronous context manager protocol which closes the\n    underlying file at the end of the context block.\n\n    This class also supports asynchronous iteration::\n\n        async with await open_file(...) as f:\n            async for line in f:\n                print(line)\n    \"\"\"\n\n    def __init__(self, fp: IO[AnyStr]) -> None:\n        self._fp: Any = fp\n\n    def __getattr__(self, name: str) -> object:\n        return getattr(self._fp, name)\n\n    @property\n    def wrapped(self) -> IO[AnyStr]:\n        \"\"\"The wrapped file object.\"\"\"\n        return self._fp\n\n    async def __aiter__(self) -> AsyncIterator[AnyStr]:\n        while True:\n            line = await self.readline()\n            if line:\n                yield line\n            else:\n                break\n\n    async def aclose(self) -> None:\n        return await to_thread.run_sync(self._fp.close)\n\n    async def read(self, size: int = -1) -> AnyStr:\n        return await to_thread.run_sync(self._fp.read, size)\n\n    async def read1(self: AsyncFile[bytes], size: int = -1) -> bytes:\n        return await to_thread.run_sync(self._fp.read1, size)\n\n    async def readline(self) -> AnyStr:\n        return await to_thread.run_sync(self._fp.readline)\n\n    async def readlines(self) -> list[AnyStr]:\n        return await to_thread.run_sync(self._fp.readlines)\n\n    async def readinto(self: AsyncFile[bytes], b: WriteableBuffer) -> bytes:\n        return await to_thread.run_sync(self._fp.readinto, b)\n\n    async def readinto1(self: AsyncFile[bytes], b: WriteableBuffer) -> bytes:\n        return await to_thread.run_sync(self._fp.readinto1, b)\n\n    @overload\n    async def write(self: AsyncFile[bytes], b: ReadableBuffer) -> int: ...\n\n    @overload\n    async def write(self: AsyncFile[str], b: str) -> int: ...\n\n    async def write(self, b: ReadableBuffer | str) -> int:\n        return await to_thread.run_sync(self._fp.write, b)\n\n    @overload\n    async def writelines(\n        self: AsyncFile[bytes], lines: Iterable[ReadableBuffer]\n    ) -> None: ...\n\n    @overload\n    async def writelines(self: AsyncFile[str], lines: Iterable[str]) -> None: ...\n\n    async def writelines(self, lines: Iterable[ReadableBuffer] | Iterable[str]) -> None:\n        return await to_thread.run_sync(self._fp.writelines, lines)\n\n    async def truncate(self, size: int | None = None) -> int:\n        return await to_thread.run_sync(self._fp.truncate, size)\n\n    async def seek(self, offset: int, whence: int | None = os.SEEK_SET) -> int:\n        return await to_thread.run_sync(self._fp.seek, offset, whence)\n\n    async def tell(self) -> int:\n        return await to_thread.run_sync(self._fp.tell)\n\n    async def flush(self) -> None:\n        return await to_thread.run_sync(self._fp.flush)\n\n\n@overload\nasync def open_file(\n    file: str | PathLike[str] | int,\n    mode: OpenBinaryMode,\n    buffering: int = ...,\n    encoding: str | None = ...,\n    errors: str | None = ...,\n    newline: str | None = ...,\n    closefd: bool = ...,\n    opener: Callable[[str, int], int] | None = ...,\n) -> AsyncFile[bytes]: ...\n\n\n@overload\nasync def open_file(\n    file: str | PathLike[str] | int,\n    mode: OpenTextMode = ...,\n    buffering: int = ...,\n    encoding: str | None = ...,\n    errors: str | None = ...,\n    newline: str | None = ...,\n    closefd: bool = ...,\n    opener: Callable[[str, int], int] | None = ...,\n) -> AsyncFile[str]: ...\n\n\nasync def open_file(\n    file: str | PathLike[str] | int,\n    mode: str = \"r\",\n    buffering: int = -1,\n    encoding: str | None = None,\n    errors: str | None = None,\n    newline: str | None = None,\n    closefd: bool = True,\n    opener: Callable[[str, int], int] | None = None,\n) -> AsyncFile[Any]:\n    \"\"\"\n    Open a file asynchronously.\n\n    The arguments are exactly the same as for the builtin :func:`open`.\n\n    :return: an asynchronous file object\n\n    \"\"\"\n    fp = await to_thread.run_sync(\n        open, file, mode, buffering, encoding, errors, newline, closefd, opener\n    )\n    return AsyncFile(fp)\n\n\ndef wrap_file(file: IO[AnyStr]) -> AsyncFile[AnyStr]:\n    \"\"\"\n    Wrap an existing file as an asynchronous file.\n\n    :param file: an existing file-like object\n    :return: an asynchronous file object\n\n    \"\"\"\n    return AsyncFile(file)\n\n\n@dataclass(eq=False)\nclass _PathIterator(AsyncIterator[\"Path\"]):\n    iterator: Iterator[PathLike[str]]\n\n    async def __anext__(self) -> Path:\n        nextval = await to_thread.run_sync(\n            next, self.iterator, None, abandon_on_cancel=True\n        )\n        if nextval is None:\n            raise StopAsyncIteration from None\n\n        return Path(nextval)\n\n\nclass Path:\n    \"\"\"\n    An asynchronous version of :class:`pathlib.Path`.\n\n    This class cannot be substituted for :class:`pathlib.Path` or\n    :class:`pathlib.PurePath`, but it is compatible with the :class:`os.PathLike`\n    interface.\n\n    It implements the Python 3.10 version of :class:`pathlib.Path` interface, except for\n    the deprecated :meth:`~pathlib.Path.link_to` method.\n\n    Any methods that do disk I/O need to be awaited on. These methods are:\n\n    * :meth:`~pathlib.Path.absolute`\n    * :meth:`~pathlib.Path.chmod`\n    * :meth:`~pathlib.Path.cwd`\n    * :meth:`~pathlib.Path.exists`\n    * :meth:`~pathlib.Path.expanduser`\n    * :meth:`~pathlib.Path.group`\n    * :meth:`~pathlib.Path.hardlink_to`\n    * :meth:`~pathlib.Path.home`\n    * :meth:`~pathlib.Path.is_block_device`\n    * :meth:`~pathlib.Path.is_char_device`\n    * :meth:`~pathlib.Path.is_dir`\n    * :meth:`~pathlib.Path.is_fifo`\n    * :meth:`~pathlib.Path.is_file`\n    * :meth:`~pathlib.Path.is_mount`\n    * :meth:`~pathlib.Path.lchmod`\n    * :meth:`~pathlib.Path.lstat`\n    * :meth:`~pathlib.Path.mkdir`\n    * :meth:`~pathlib.Path.open`\n    * :meth:`~pathlib.Path.owner`\n    * :meth:`~pathlib.Path.read_bytes`\n    * :meth:`~pathlib.Path.read_text`\n    * :meth:`~pathlib.Path.readlink`\n    * :meth:`~pathlib.Path.rename`\n    * :meth:`~pathlib.Path.replace`\n    * :meth:`~pathlib.Path.rmdir`\n    * :meth:`~pathlib.Path.samefile`\n    * :meth:`~pathlib.Path.stat`\n    * :meth:`~pathlib.Path.touch`\n    * :meth:`~pathlib.Path.unlink`\n    * :meth:`~pathlib.Path.write_bytes`\n    * :meth:`~pathlib.Path.write_text`\n\n    Additionally, the following methods return an async iterator yielding\n    :class:`~.Path` objects:\n\n    * :meth:`~pathlib.Path.glob`\n    * :meth:`~pathlib.Path.iterdir`\n    * :meth:`~pathlib.Path.rglob`\n    \"\"\"\n\n    __slots__ = \"_path\", \"__weakref__\"\n\n    __weakref__: Any\n\n    def __init__(self, *args: str | PathLike[str]) -> None:\n        self._path: Final[pathlib.Path] = pathlib.Path(*args)\n\n    def __fspath__(self) -> str:\n        return self._path.__fspath__()\n\n    def __str__(self) -> str:\n        return self._path.__str__()\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.as_posix()!r})\"\n\n    def __bytes__(self) -> bytes:\n        return self._path.__bytes__()\n\n    def __hash__(self) -> int:\n        return self._path.__hash__()\n\n    def __eq__(self, other: object) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__eq__(target)\n\n    def __lt__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__lt__(target)\n\n    def __le__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__le__(target)\n\n    def __gt__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__gt__(target)\n\n    def __ge__(self, other: pathlib.PurePath | Path) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__ge__(target)\n\n    def __truediv__(self, other: str | PathLike[str]) -> Path:\n        return Path(self._path / other)\n\n    def __rtruediv__(self, other: str | PathLike[str]) -> Path:\n        return Path(other) / self\n\n    @property\n    def parts(self) -> tuple[str, ...]:\n        return self._path.parts\n\n    @property\n    def drive(self) -> str:\n        return self._path.drive\n\n    @property\n    def root(self) -> str:\n        return self._path.root\n\n    @property\n    def anchor(self) -> str:\n        return self._path.anchor\n\n    @property\n    def parents(self) -> Sequence[Path]:\n        return tuple(Path(p) for p in self._path.parents)\n\n    @property\n    def parent(self) -> Path:\n        return Path(self._path.parent)\n\n    @property\n    def name(self) -> str:\n        return self._path.name\n\n    @property\n    def suffix(self) -> str:\n        return self._path.suffix\n\n    @property\n    def suffixes(self) -> list[str]:\n        return self._path.suffixes\n\n    @property\n    def stem(self) -> str:\n        return self._path.stem\n\n    async def absolute(self) -> Path:\n        path = await to_thread.run_sync(self._path.absolute)\n        return Path(path)\n\n    def as_posix(self) -> str:\n        return self._path.as_posix()\n\n    def as_uri(self) -> str:\n        return self._path.as_uri()\n\n    if sys.version_info >= (3, 13):\n        parser = pathlib.Path.parser  # type: ignore[attr-defined]\n\n        @classmethod\n        def from_uri(cls, uri: str) -> Path:\n            return Path(pathlib.Path.from_uri(uri))  # type: ignore[attr-defined]\n\n        def full_match(\n            self, path_pattern: str, *, case_sensitive: bool | None = None\n        ) -> bool:\n            return self._path.full_match(  # type: ignore[attr-defined]\n                path_pattern, case_sensitive=case_sensitive\n            )\n\n        def match(\n            self, path_pattern: str, *, case_sensitive: bool | None = None\n        ) -> bool:\n            return self._path.match(path_pattern, case_sensitive=case_sensitive)\n    else:\n\n        def match(self, path_pattern: str) -> bool:\n            return self._path.match(path_pattern)\n\n    def is_relative_to(self, other: str | PathLike[str]) -> bool:\n        try:\n            self.relative_to(other)\n            return True\n        except ValueError:\n            return False\n\n    async def is_junction(self) -> bool:\n        return await to_thread.run_sync(self._path.is_junction)\n\n    async def chmod(self, mode: int, *, follow_symlinks: bool = True) -> None:\n        func = partial(os.chmod, follow_symlinks=follow_symlinks)\n        return await to_thread.run_sync(func, self._path, mode)\n\n    @classmethod\n    async def cwd(cls) -> Path:\n        path = await to_thread.run_sync(pathlib.Path.cwd)\n        return cls(path)\n\n    async def exists(self) -> bool:\n        return await to_thread.run_sync(self._path.exists, abandon_on_cancel=True)\n\n    async def expanduser(self) -> Path:\n        return Path(\n            await to_thread.run_sync(self._path.expanduser, abandon_on_cancel=True)\n        )\n\n    def glob(self, pattern: str) -> AsyncIterator[Path]:\n        gen = self._path.glob(pattern)\n        return _PathIterator(gen)\n\n    async def group(self) -> str:\n        return await to_thread.run_sync(self._path.group, abandon_on_cancel=True)\n\n    async def hardlink_to(\n        self, target: str | bytes | PathLike[str] | PathLike[bytes]\n    ) -> None:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(os.link, target, self)\n\n    @classmethod\n    async def home(cls) -> Path:\n        home_path = await to_thread.run_sync(pathlib.Path.home)\n        return cls(home_path)\n\n    def is_absolute(self) -> bool:\n        return self._path.is_absolute()\n\n    async def is_block_device(self) -> bool:\n        return await to_thread.run_sync(\n            self._path.is_block_device, abandon_on_cancel=True\n        )\n\n    async def is_char_device(self) -> bool:\n        return await to_thread.run_sync(\n            self._path.is_char_device, abandon_on_cancel=True\n        )\n\n    async def is_dir(self) -> bool:\n        return await to_thread.run_sync(self._path.is_dir, abandon_on_cancel=True)\n\n    async def is_fifo(self) -> bool:\n        return await to_thread.run_sync(self._path.is_fifo, abandon_on_cancel=True)\n\n    async def is_file(self) -> bool:\n        return await to_thread.run_sync(self._path.is_file, abandon_on_cancel=True)\n\n    async def is_mount(self) -> bool:\n        return await to_thread.run_sync(\n            os.path.ismount, self._path, abandon_on_cancel=True\n        )\n\n    def is_reserved(self) -> bool:\n        return self._path.is_reserved()\n\n    async def is_socket(self) -> bool:\n        return await to_thread.run_sync(self._path.is_socket, abandon_on_cancel=True)\n\n    async def is_symlink(self) -> bool:\n        return await to_thread.run_sync(self._path.is_symlink, abandon_on_cancel=True)\n\n    def iterdir(self) -> AsyncIterator[Path]:\n        gen = self._path.iterdir()\n        return _PathIterator(gen)\n\n    def joinpath(self, *args: str | PathLike[str]) -> Path:\n        return Path(self._path.joinpath(*args))\n\n    async def lchmod(self, mode: int) -> None:\n        await to_thread.run_sync(self._path.lchmod, mode)\n\n    async def lstat(self) -> os.stat_result:\n        return await to_thread.run_sync(self._path.lstat, abandon_on_cancel=True)\n\n    async def mkdir(\n        self, mode: int = 0o777, parents: bool = False, exist_ok: bool = False\n    ) -> None:\n        await to_thread.run_sync(self._path.mkdir, mode, parents, exist_ok)\n\n    @overload\n    async def open(\n        self,\n        mode: OpenBinaryMode,\n        buffering: int = ...,\n        encoding: str | None = ...,\n        errors: str | None = ...,\n        newline: str | None = ...,\n    ) -> AsyncFile[bytes]: ...\n\n    @overload\n    async def open(\n        self,\n        mode: OpenTextMode = ...,\n        buffering: int = ...,\n        encoding: str | None = ...,\n        errors: str | None = ...,\n        newline: str | None = ...,\n    ) -> AsyncFile[str]: ...\n\n    async def open(\n        self,\n        mode: str = \"r\",\n        buffering: int = -1,\n        encoding: str | None = None,\n        errors: str | None = None,\n        newline: str | None = None,\n    ) -> AsyncFile[Any]:\n        fp = await to_thread.run_sync(\n            self._path.open, mode, buffering, encoding, errors, newline\n        )\n        return AsyncFile(fp)\n\n    async def owner(self) -> str:\n        return await to_thread.run_sync(self._path.owner, abandon_on_cancel=True)\n\n    async def read_bytes(self) -> bytes:\n        return await to_thread.run_sync(self._path.read_bytes)\n\n    async def read_text(\n        self, encoding: str | None = None, errors: str | None = None\n    ) -> str:\n        return await to_thread.run_sync(self._path.read_text, encoding, errors)\n\n    if sys.version_info >= (3, 12):\n\n        def relative_to(\n            self, *other: str | PathLike[str], walk_up: bool = False\n        ) -> Path:\n            return Path(self._path.relative_to(*other, walk_up=walk_up))\n\n    else:\n\n        def relative_to(self, *other: str | PathLike[str]) -> Path:\n            return Path(self._path.relative_to(*other))\n\n    async def readlink(self) -> Path:\n        target = await to_thread.run_sync(os.readlink, self._path)\n        return Path(target)\n\n    async def rename(self, target: str | pathlib.PurePath | Path) -> Path:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.rename, target)\n        return Path(target)\n\n    async def replace(self, target: str | pathlib.PurePath | Path) -> Path:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.replace, target)\n        return Path(target)\n\n    async def resolve(self, strict: bool = False) -> Path:\n        func = partial(self._path.resolve, strict=strict)\n        return Path(await to_thread.run_sync(func, abandon_on_cancel=True))\n\n    def rglob(self, pattern: str) -> AsyncIterator[Path]:\n        gen = self._path.rglob(pattern)\n        return _PathIterator(gen)\n\n    async def rmdir(self) -> None:\n        await to_thread.run_sync(self._path.rmdir)\n\n    async def samefile(self, other_path: str | PathLike[str]) -> bool:\n        if isinstance(other_path, Path):\n            other_path = other_path._path\n\n        return await to_thread.run_sync(\n            self._path.samefile, other_path, abandon_on_cancel=True\n        )\n\n    async def stat(self, *, follow_symlinks: bool = True) -> os.stat_result:\n        func = partial(os.stat, follow_symlinks=follow_symlinks)\n        return await to_thread.run_sync(func, self._path, abandon_on_cancel=True)\n\n    async def symlink_to(\n        self,\n        target: str | bytes | PathLike[str] | PathLike[bytes],\n        target_is_directory: bool = False,\n    ) -> None:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.symlink_to, target, target_is_directory)\n\n    async def touch(self, mode: int = 0o666, exist_ok: bool = True) -> None:\n        await to_thread.run_sync(self._path.touch, mode, exist_ok)\n\n    async def unlink(self, missing_ok: bool = False) -> None:\n        try:\n            await to_thread.run_sync(self._path.unlink)\n        except FileNotFoundError:\n            if not missing_ok:\n                raise\n\n    if sys.version_info >= (3, 12):\n\n        async def walk(\n            self,\n            top_down: bool = True,\n            on_error: Callable[[OSError], object] | None = None,\n            follow_symlinks: bool = False,\n        ) -> AsyncIterator[tuple[Path, list[str], list[str]]]:\n            def get_next_value() -> tuple[pathlib.Path, list[str], list[str]] | None:\n                try:\n                    return next(gen)\n                except StopIteration:\n                    return None\n\n            gen = self._path.walk(top_down, on_error, follow_symlinks)\n            while True:\n                value = await to_thread.run_sync(get_next_value)\n                if value is None:\n                    return\n\n                root, dirs, paths = value\n                yield Path(root), dirs, paths\n\n    def with_name(self, name: str) -> Path:\n        return Path(self._path.with_name(name))\n\n    def with_stem(self, stem: str) -> Path:\n        return Path(self._path.with_name(stem + self._path.suffix))\n\n    def with_suffix(self, suffix: str) -> Path:\n        return Path(self._path.with_suffix(suffix))\n\n    def with_segments(self, *pathsegments: str | PathLike[str]) -> Path:\n        return Path(*pathsegments)\n\n    async def write_bytes(self, data: bytes) -> int:\n        return await to_thread.run_sync(self._path.write_bytes, data)\n\n    async def write_text(\n        self,\n        data: str,\n        encoding: str | None = None,\n        errors: str | None = None,\n        newline: str | None = None,\n    ) -> int:\n        # Path.write_text() does not support the \"newline\" parameter before Python 3.10\n        def sync_write_text() -> int:\n            with self._path.open(\n                \"w\", encoding=encoding, errors=errors, newline=newline\n            ) as fp:\n                return fp.write(data)\n\n        return await to_thread.run_sync(sync_write_text)\n\n\nPathLike.register(Path)\n", "src/anyio/streams/file.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nfrom io import SEEK_SET, UnsupportedOperation\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Any, BinaryIO, cast\n\nfrom .. import (\n    BrokenResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    TypedAttributeSet,\n    to_thread,\n    typed_attribute,\n)\nfrom ..abc import ByteReceiveStream, ByteSendStream\n\n\nclass FileStreamAttribute(TypedAttributeSet):\n    #: the open file descriptor\n    file: BinaryIO = typed_attribute()\n    #: the path of the file on the file system, if available (file must be a real file)\n    path: Path = typed_attribute()\n    #: the file number, if available (file must be a real file or a TTY)\n    fileno: int = typed_attribute()\n\n\nclass _BaseFileStream:\n    def __init__(self, file: BinaryIO):\n        self._file = file\n\n    async def aclose(self) -> None:\n        await to_thread.run_sync(self._file.close)\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        attributes: dict[Any, Callable[[], Any]] = {\n            FileStreamAttribute.file: lambda: self._file,\n        }\n\n        if hasattr(self._file, \"name\"):\n            attributes[FileStreamAttribute.path] = lambda: Path(self._file.name)\n\n        try:\n            self._file.fileno()\n        except UnsupportedOperation:\n            pass\n        else:\n            attributes[FileStreamAttribute.fileno] = lambda: self._file.fileno()\n\n        return attributes\n\n\nclass FileReadStream(_BaseFileStream, ByteReceiveStream):\n    \"\"\"\n    A byte stream that reads from a file in the file system.\n\n    :param file: a file that has been opened for reading in binary mode\n\n    .. versionadded:: 3.0\n    \"\"\"\n\n    @classmethod\n    async def from_path(cls, path: str | PathLike[str]) -> FileReadStream:\n        \"\"\"\n        Create a file read stream by opening the given file.\n\n        :param path: path of the file to read from\n\n        \"\"\"\n        file = await to_thread.run_sync(Path(path).open, \"rb\")\n        return cls(cast(BinaryIO, file))\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        try:\n            data = await to_thread.run_sync(self._file.read, max_bytes)\n        except ValueError:\n            raise ClosedResourceError from None\n        except OSError as exc:\n            raise BrokenResourceError from exc\n\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def seek(self, position: int, whence: int = SEEK_SET) -> int:\n        \"\"\"\n        Seek the file to the given position.\n\n        .. seealso:: :meth:`io.IOBase.seek`\n\n        .. note:: Not all file descriptors are seekable.\n\n        :param position: position to seek the file to\n        :param whence: controls how ``position`` is interpreted\n        :return: the new absolute position\n        :raises OSError: if the file is not seekable\n\n        \"\"\"\n        return await to_thread.run_sync(self._file.seek, position, whence)\n\n    async def tell(self) -> int:\n        \"\"\"\n        Return the current stream position.\n\n        .. note:: Not all file descriptors are seekable.\n\n        :return: the current absolute position\n        :raises OSError: if the file is not seekable\n\n        \"\"\"\n        return await to_thread.run_sync(self._file.tell)\n\n\nclass FileWriteStream(_BaseFileStream, ByteSendStream):\n    \"\"\"\n    A byte stream that writes to a file in the file system.\n\n    :param file: a file that has been opened for writing in binary mode\n\n    .. versionadded:: 3.0\n    \"\"\"\n\n    @classmethod\n    async def from_path(\n        cls, path: str | PathLike[str], append: bool = False\n    ) -> FileWriteStream:\n        \"\"\"\n        Create a file write stream by opening the given file for writing.\n\n        :param path: path of the file to write to\n        :param append: if ``True``, open the file for appending; if ``False``, any\n            existing file at the given path will be truncated\n\n        \"\"\"\n        mode = \"ab\" if append else \"wb\"\n        file = await to_thread.run_sync(Path(path).open, mode)\n        return cls(cast(BinaryIO, file))\n\n    async def send(self, item: bytes) -> None:\n        try:\n            await to_thread.run_sync(self._file.write, item)\n        except ValueError:\n            raise ClosedResourceError from None\n        except OSError as exc:\n            raise BrokenResourceError from exc\n", "src/anyio/streams/memory.py": "from __future__ import annotations\n\nimport warnings\nfrom collections import OrderedDict, deque\nfrom dataclasses import dataclass, field\nfrom types import TracebackType\nfrom typing import Generic, NamedTuple, TypeVar\n\nfrom .. import (\n    BrokenResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    WouldBlock,\n)\nfrom .._core._testing import TaskInfo, get_current_task\nfrom ..abc import Event, ObjectReceiveStream, ObjectSendStream\nfrom ..lowlevel import checkpoint\n\nT_Item = TypeVar(\"T_Item\")\nT_co = TypeVar(\"T_co\", covariant=True)\nT_contra = TypeVar(\"T_contra\", contravariant=True)\n\n\nclass MemoryObjectStreamStatistics(NamedTuple):\n    current_buffer_used: int  #: number of items stored in the buffer\n    #: maximum number of items that can be stored on this stream (or :data:`math.inf`)\n    max_buffer_size: float\n    open_send_streams: int  #: number of unclosed clones of the send stream\n    open_receive_streams: int  #: number of unclosed clones of the receive stream\n    #: number of tasks blocked on :meth:`MemoryObjectSendStream.send`\n    tasks_waiting_send: int\n    #: number of tasks blocked on :meth:`MemoryObjectReceiveStream.receive`\n    tasks_waiting_receive: int\n\n\n@dataclass(eq=False)\nclass MemoryObjectItemReceiver(Generic[T_Item]):\n    task_info: TaskInfo = field(init=False, default_factory=get_current_task)\n    item: T_Item = field(init=False)\n\n\n@dataclass(eq=False)\nclass MemoryObjectStreamState(Generic[T_Item]):\n    max_buffer_size: float = field()\n    buffer: deque[T_Item] = field(init=False, default_factory=deque)\n    open_send_channels: int = field(init=False, default=0)\n    open_receive_channels: int = field(init=False, default=0)\n    waiting_receivers: OrderedDict[Event, MemoryObjectItemReceiver[T_Item]] = field(\n        init=False, default_factory=OrderedDict\n    )\n    waiting_senders: OrderedDict[Event, T_Item] = field(\n        init=False, default_factory=OrderedDict\n    )\n\n    def statistics(self) -> MemoryObjectStreamStatistics:\n        return MemoryObjectStreamStatistics(\n            len(self.buffer),\n            self.max_buffer_size,\n            self.open_send_channels,\n            self.open_receive_channels,\n            len(self.waiting_senders),\n            len(self.waiting_receivers),\n        )\n\n\n@dataclass(eq=False)\nclass MemoryObjectReceiveStream(Generic[T_co], ObjectReceiveStream[T_co]):\n    _state: MemoryObjectStreamState[T_co]\n    _closed: bool = field(init=False, default=False)\n\n    def __post_init__(self) -> None:\n        self._state.open_receive_channels += 1\n\n    def receive_nowait(self) -> T_co:\n        \"\"\"\n        Receive the next item if it can be done without waiting.\n\n        :return: the received item\n        :raises ~anyio.ClosedResourceError: if this send stream has been closed\n        :raises ~anyio.EndOfStream: if the buffer is empty and this stream has been\n            closed from the sending end\n        :raises ~anyio.WouldBlock: if there are no items in the buffer and no tasks\n            waiting to send\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n\n        if self._state.waiting_senders:\n            # Get the item from the next sender\n            send_event, item = self._state.waiting_senders.popitem(last=False)\n            self._state.buffer.append(item)\n            send_event.set()\n\n        if self._state.buffer:\n            return self._state.buffer.popleft()\n        elif not self._state.open_send_channels:\n            raise EndOfStream\n\n        raise WouldBlock\n\n    async def receive(self) -> T_co:\n        await checkpoint()\n        try:\n            return self.receive_nowait()\n        except WouldBlock:\n            # Add ourselves in the queue\n            receive_event = Event()\n            receiver = MemoryObjectItemReceiver[T_co]()\n            self._state.waiting_receivers[receive_event] = receiver\n\n            try:\n                await receive_event.wait()\n            finally:\n                self._state.waiting_receivers.pop(receive_event, None)\n\n            try:\n                return receiver.item\n            except AttributeError:\n                raise EndOfStream\n\n    def clone(self) -> MemoryObjectReceiveStream[T_co]:\n        \"\"\"\n        Create a clone of this receive stream.\n\n        Each clone can be closed separately. Only when all clones have been closed will\n        the receiving end of the memory stream be considered closed by the sending ends.\n\n        :return: the cloned stream\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n\n        return MemoryObjectReceiveStream(_state=self._state)\n\n    def close(self) -> None:\n        \"\"\"\n        Close the stream.\n\n        This works the exact same way as :meth:`aclose`, but is provided as a special\n        case for the benefit of synchronous callbacks.\n\n        \"\"\"\n        if not self._closed:\n            self._closed = True\n            self._state.open_receive_channels -= 1\n            if self._state.open_receive_channels == 0:\n                send_events = list(self._state.waiting_senders.keys())\n                for event in send_events:\n                    event.set()\n\n    async def aclose(self) -> None:\n        self.close()\n\n    def statistics(self) -> MemoryObjectStreamStatistics:\n        \"\"\"\n        Return statistics about the current state of this stream.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return self._state.statistics()\n\n    def __enter__(self) -> MemoryObjectReceiveStream[T_co]:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.close()\n\n    def __del__(self) -> None:\n        if not self._closed:\n            warnings.warn(\n                f\"Unclosed <{self.__class__.__name__} at {id(self):x}>\",\n                ResourceWarning,\n                source=self,\n            )\n\n\n@dataclass(eq=False)\nclass MemoryObjectSendStream(Generic[T_contra], ObjectSendStream[T_contra]):\n    _state: MemoryObjectStreamState[T_contra]\n    _closed: bool = field(init=False, default=False)\n\n    def __post_init__(self) -> None:\n        self._state.open_send_channels += 1\n\n    def send_nowait(self, item: T_contra) -> None:\n        \"\"\"\n        Send an item immediately if it can be done without waiting.\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if this send stream has been closed\n        :raises ~anyio.BrokenResourceError: if the stream has been closed from the\n            receiving end\n        :raises ~anyio.WouldBlock: if the buffer is full and there are no tasks waiting\n            to receive\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n        if not self._state.open_receive_channels:\n            raise BrokenResourceError\n\n        while self._state.waiting_receivers:\n            receive_event, receiver = self._state.waiting_receivers.popitem(last=False)\n            if not receiver.task_info.has_pending_cancellation():\n                receiver.item = item\n                receive_event.set()\n                return\n\n        if len(self._state.buffer) < self._state.max_buffer_size:\n            self._state.buffer.append(item)\n        else:\n            raise WouldBlock\n\n    async def send(self, item: T_contra) -> None:\n        \"\"\"\n        Send an item to the stream.\n\n        If the buffer is full, this method blocks until there is again room in the\n        buffer or the item can be sent directly to a receiver.\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if this send stream has been closed\n        :raises ~anyio.BrokenResourceError: if the stream has been closed from the\n            receiving end\n\n        \"\"\"\n        await checkpoint()\n        try:\n            self.send_nowait(item)\n        except WouldBlock:\n            # Wait until there's someone on the receiving end\n            send_event = Event()\n            self._state.waiting_senders[send_event] = item\n            try:\n                await send_event.wait()\n            except BaseException:\n                self._state.waiting_senders.pop(send_event, None)\n                raise\n\n            if send_event in self._state.waiting_senders:\n                del self._state.waiting_senders[send_event]\n                raise BrokenResourceError from None\n\n    def clone(self) -> MemoryObjectSendStream[T_contra]:\n        \"\"\"\n        Create a clone of this send stream.\n\n        Each clone can be closed separately. Only when all clones have been closed will\n        the sending end of the memory stream be considered closed by the receiving ends.\n\n        :return: the cloned stream\n\n        \"\"\"\n        if self._closed:\n            raise ClosedResourceError\n\n        return MemoryObjectSendStream(_state=self._state)\n\n    def close(self) -> None:\n        \"\"\"\n        Close the stream.\n\n        This works the exact same way as :meth:`aclose`, but is provided as a special\n        case for the benefit of synchronous callbacks.\n\n        \"\"\"\n        if not self._closed:\n            self._closed = True\n            self._state.open_send_channels -= 1\n            if self._state.open_send_channels == 0:\n                receive_events = list(self._state.waiting_receivers.keys())\n                self._state.waiting_receivers.clear()\n                for event in receive_events:\n                    event.set()\n\n    async def aclose(self) -> None:\n        self.close()\n\n    def statistics(self) -> MemoryObjectStreamStatistics:\n        \"\"\"\n        Return statistics about the current state of this stream.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return self._state.statistics()\n\n    def __enter__(self) -> MemoryObjectSendStream[T_contra]:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.close()\n\n    def __del__(self) -> None:\n        if not self._closed:\n            warnings.warn(\n                f\"Unclosed <{self.__class__.__name__} at {id(self):x}>\",\n                ResourceWarning,\n                source=self,\n            )\n", "src/anyio/streams/tls.py": "from __future__ import annotations\n\nimport logging\nimport re\nimport ssl\nimport sys\nfrom collections.abc import Callable, Mapping\nfrom dataclasses import dataclass\nfrom functools import wraps\nfrom typing import Any, Tuple, TypeVar\n\nfrom .. import (\n    BrokenResourceError,\n    EndOfStream,\n    aclose_forcefully,\n    get_cancelled_exc_class,\n)\nfrom .._core._typedattr import TypedAttributeSet, typed_attribute\nfrom ..abc import AnyByteStream, ByteStream, Listener, TaskGroup\n\nif sys.version_info >= (3, 11):\n    from typing import TypeVarTuple, Unpack\nelse:\n    from typing_extensions import TypeVarTuple, Unpack\n\nT_Retval = TypeVar(\"T_Retval\")\nPosArgsT = TypeVarTuple(\"PosArgsT\")\n_PCTRTT = Tuple[Tuple[str, str], ...]\n_PCTRTTT = Tuple[_PCTRTT, ...]\n\n\nclass TLSAttribute(TypedAttributeSet):\n    \"\"\"Contains Transport Layer Security related attributes.\"\"\"\n\n    #: the selected ALPN protocol\n    alpn_protocol: str | None = typed_attribute()\n    #: the channel binding for type ``tls-unique``\n    channel_binding_tls_unique: bytes = typed_attribute()\n    #: the selected cipher\n    cipher: tuple[str, str, int] = typed_attribute()\n    #: the peer certificate in dictionary form (see :meth:`ssl.SSLSocket.getpeercert`\n    # for more information)\n    peer_certificate: None | (dict[str, str | _PCTRTTT | _PCTRTT]) = typed_attribute()\n    #: the peer certificate in binary form\n    peer_certificate_binary: bytes | None = typed_attribute()\n    #: ``True`` if this is the server side of the connection\n    server_side: bool = typed_attribute()\n    #: ciphers shared by the client during the TLS handshake (``None`` if this is the\n    #: client side)\n    shared_ciphers: list[tuple[str, str, int]] | None = typed_attribute()\n    #: the :class:`~ssl.SSLObject` used for encryption\n    ssl_object: ssl.SSLObject = typed_attribute()\n    #: ``True`` if this stream does (and expects) a closing TLS handshake when the\n    #: stream is being closed\n    standard_compatible: bool = typed_attribute()\n    #: the TLS protocol version (e.g. ``TLSv1.2``)\n    tls_version: str = typed_attribute()\n\n\n@dataclass(eq=False)\nclass TLSStream(ByteStream):\n    \"\"\"\n    A stream wrapper that encrypts all sent data and decrypts received data.\n\n    This class has no public initializer; use :meth:`wrap` instead.\n    All extra attributes from :class:`~TLSAttribute` are supported.\n\n    :var AnyByteStream transport_stream: the wrapped stream\n\n    \"\"\"\n\n    transport_stream: AnyByteStream\n    standard_compatible: bool\n    _ssl_object: ssl.SSLObject\n    _read_bio: ssl.MemoryBIO\n    _write_bio: ssl.MemoryBIO\n\n    @classmethod\n    async def wrap(\n        cls,\n        transport_stream: AnyByteStream,\n        *,\n        server_side: bool | None = None,\n        hostname: str | None = None,\n        ssl_context: ssl.SSLContext | None = None,\n        standard_compatible: bool = True,\n    ) -> TLSStream:\n        \"\"\"\n        Wrap an existing stream with Transport Layer Security.\n\n        This performs a TLS handshake with the peer.\n\n        :param transport_stream: a bytes-transporting stream to wrap\n        :param server_side: ``True`` if this is the server side of the connection,\n            ``False`` if this is the client side (if omitted, will be set to ``False``\n            if ``hostname`` has been provided, ``False`` otherwise). Used only to create\n            a default context when an explicit context has not been provided.\n        :param hostname: host name of the peer (if host name checking is desired)\n        :param ssl_context: the SSLContext object to use (if not provided, a secure\n            default will be created)\n        :param standard_compatible: if ``False``, skip the closing handshake when\n            closing the connection, and don't raise an exception if the peer does the\n            same\n        :raises ~ssl.SSLError: if the TLS handshake fails\n\n        \"\"\"\n        if server_side is None:\n            server_side = not hostname\n\n        if not ssl_context:\n            purpose = (\n                ssl.Purpose.CLIENT_AUTH if server_side else ssl.Purpose.SERVER_AUTH\n            )\n            ssl_context = ssl.create_default_context(purpose)\n\n            # Re-enable detection of unexpected EOFs if it was disabled by Python\n            if hasattr(ssl, \"OP_IGNORE_UNEXPECTED_EOF\"):\n                ssl_context.options &= ~ssl.OP_IGNORE_UNEXPECTED_EOF\n\n        bio_in = ssl.MemoryBIO()\n        bio_out = ssl.MemoryBIO()\n        ssl_object = ssl_context.wrap_bio(\n            bio_in, bio_out, server_side=server_side, server_hostname=hostname\n        )\n        wrapper = cls(\n            transport_stream=transport_stream,\n            standard_compatible=standard_compatible,\n            _ssl_object=ssl_object,\n            _read_bio=bio_in,\n            _write_bio=bio_out,\n        )\n        await wrapper._call_sslobject_method(ssl_object.do_handshake)\n        return wrapper\n\n    async def _call_sslobject_method(\n        self, func: Callable[[Unpack[PosArgsT]], T_Retval], *args: Unpack[PosArgsT]\n    ) -> T_Retval:\n        while True:\n            try:\n                result = func(*args)\n            except ssl.SSLWantReadError:\n                try:\n                    # Flush any pending writes first\n                    if self._write_bio.pending:\n                        await self.transport_stream.send(self._write_bio.read())\n\n                    data = await self.transport_stream.receive()\n                except EndOfStream:\n                    self._read_bio.write_eof()\n                except OSError as exc:\n                    self._read_bio.write_eof()\n                    self._write_bio.write_eof()\n                    raise BrokenResourceError from exc\n                else:\n                    self._read_bio.write(data)\n            except ssl.SSLWantWriteError:\n                await self.transport_stream.send(self._write_bio.read())\n            except ssl.SSLSyscallError as exc:\n                self._read_bio.write_eof()\n                self._write_bio.write_eof()\n                raise BrokenResourceError from exc\n            except ssl.SSLError as exc:\n                self._read_bio.write_eof()\n                self._write_bio.write_eof()\n                if (\n                    isinstance(exc, ssl.SSLEOFError)\n                    or \"UNEXPECTED_EOF_WHILE_READING\" in exc.strerror\n                ):\n                    if self.standard_compatible:\n                        raise BrokenResourceError from exc\n                    else:\n                        raise EndOfStream from None\n\n                raise\n            else:\n                # Flush any pending writes first\n                if self._write_bio.pending:\n                    await self.transport_stream.send(self._write_bio.read())\n\n                return result\n\n    async def unwrap(self) -> tuple[AnyByteStream, bytes]:\n        \"\"\"\n        Does the TLS closing handshake.\n\n        :return: a tuple of (wrapped byte stream, bytes left in the read buffer)\n\n        \"\"\"\n        await self._call_sslobject_method(self._ssl_object.unwrap)\n        self._read_bio.write_eof()\n        self._write_bio.write_eof()\n        return self.transport_stream, self._read_bio.read()\n\n    async def aclose(self) -> None:\n        if self.standard_compatible:\n            try:\n                await self.unwrap()\n            except BaseException:\n                await aclose_forcefully(self.transport_stream)\n                raise\n\n        await self.transport_stream.aclose()\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n        if not data:\n            raise EndOfStream\n\n        return data\n\n    async def send(self, item: bytes) -> None:\n        await self._call_sslobject_method(self._ssl_object.write, item)\n\n    async def send_eof(self) -> None:\n        tls_version = self.extra(TLSAttribute.tls_version)\n        match = re.match(r\"TLSv(\\d+)(?:\\.(\\d+))?\", tls_version)\n        if match:\n            major, minor = int(match.group(1)), int(match.group(2) or 0)\n            if (major, minor) < (1, 3):\n                raise NotImplementedError(\n                    f\"send_eof() requires at least TLSv1.3; current \"\n                    f\"session uses {tls_version}\"\n                )\n\n        raise NotImplementedError(\n            \"send_eof() has not yet been implemented for TLS streams\"\n        )\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self.transport_stream.extra_attributes,\n            TLSAttribute.alpn_protocol: self._ssl_object.selected_alpn_protocol,\n            TLSAttribute.channel_binding_tls_unique: (\n                self._ssl_object.get_channel_binding\n            ),\n            TLSAttribute.cipher: self._ssl_object.cipher,\n            TLSAttribute.peer_certificate: lambda: self._ssl_object.getpeercert(False),\n            TLSAttribute.peer_certificate_binary: lambda: self._ssl_object.getpeercert(\n                True\n            ),\n            TLSAttribute.server_side: lambda: self._ssl_object.server_side,\n            TLSAttribute.shared_ciphers: lambda: self._ssl_object.shared_ciphers()\n            if self._ssl_object.server_side\n            else None,\n            TLSAttribute.standard_compatible: lambda: self.standard_compatible,\n            TLSAttribute.ssl_object: lambda: self._ssl_object,\n            TLSAttribute.tls_version: self._ssl_object.version,\n        }\n\n\n@dataclass(eq=False)\nclass TLSListener(Listener[TLSStream]):\n    \"\"\"\n    A convenience listener that wraps another listener and auto-negotiates a TLS session\n    on every accepted connection.\n\n    If the TLS handshake times out or raises an exception,\n    :meth:`handle_handshake_error` is called to do whatever post-mortem processing is\n    deemed necessary.\n\n    Supports only the :attr:`~TLSAttribute.standard_compatible` extra attribute.\n\n    :param Listener listener: the listener to wrap\n    :param ssl_context: the SSL context object\n    :param standard_compatible: a flag passed through to :meth:`TLSStream.wrap`\n    :param handshake_timeout: time limit for the TLS handshake\n        (passed to :func:`~anyio.fail_after`)\n    \"\"\"\n\n    listener: Listener[Any]\n    ssl_context: ssl.SSLContext\n    standard_compatible: bool = True\n    handshake_timeout: float = 30\n\n    @staticmethod\n    async def handle_handshake_error(exc: BaseException, stream: AnyByteStream) -> None:\n        \"\"\"\n        Handle an exception raised during the TLS handshake.\n\n        This method does 3 things:\n\n        #. Forcefully closes the original stream\n        #. Logs the exception (unless it was a cancellation exception) using the\n           ``anyio.streams.tls`` logger\n        #. Reraises the exception if it was a base exception or a cancellation exception\n\n        :param exc: the exception\n        :param stream: the original stream\n\n        \"\"\"\n        await aclose_forcefully(stream)\n\n        # Log all except cancellation exceptions\n        if not isinstance(exc, get_cancelled_exc_class()):\n            # CPython (as of 3.11.5) returns incorrect `sys.exc_info()` here when using\n            # any asyncio implementation, so we explicitly pass the exception to log\n            # (https://github.com/python/cpython/issues/108668). Trio does not have this\n            # issue because it works around the CPython bug.\n            logging.getLogger(__name__).exception(\n                \"Error during TLS handshake\", exc_info=exc\n            )\n\n        # Only reraise base exceptions and cancellation exceptions\n        if not isinstance(exc, Exception) or isinstance(exc, get_cancelled_exc_class()):\n            raise\n\n    async def serve(\n        self,\n        handler: Callable[[TLSStream], Any],\n        task_group: TaskGroup | None = None,\n    ) -> None:\n        @wraps(handler)\n        async def handler_wrapper(stream: AnyByteStream) -> None:\n            from .. import fail_after\n\n            try:\n                with fail_after(self.handshake_timeout):\n                    wrapped_stream = await TLSStream.wrap(\n                        stream,\n                        ssl_context=self.ssl_context,\n                        standard_compatible=self.standard_compatible,\n                    )\n            except BaseException as exc:\n                await self.handle_handshake_error(exc, stream)\n            else:\n                await handler(wrapped_stream)\n\n        await self.listener.serve(handler_wrapper, task_group)\n\n    async def aclose(self) -> None:\n        await self.listener.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            TLSAttribute.standard_compatible: lambda: self.standard_compatible,\n        }\n", "src/anyio/streams/__init__.py": "", "src/anyio/streams/stapled.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, TypeVar\n\nfrom ..abc import (\n    ByteReceiveStream,\n    ByteSendStream,\n    ByteStream,\n    Listener,\n    ObjectReceiveStream,\n    ObjectSendStream,\n    ObjectStream,\n    TaskGroup,\n)\n\nT_Item = TypeVar(\"T_Item\")\nT_Stream = TypeVar(\"T_Stream\")\n\n\n@dataclass(eq=False)\nclass StapledByteStream(ByteStream):\n    \"\"\"\n    Combines two byte streams into a single, bidirectional byte stream.\n\n    Extra attributes will be provided from both streams, with the receive stream\n    providing the values in case of a conflict.\n\n    :param ByteSendStream send_stream: the sending byte stream\n    :param ByteReceiveStream receive_stream: the receiving byte stream\n    \"\"\"\n\n    send_stream: ByteSendStream\n    receive_stream: ByteReceiveStream\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        return await self.receive_stream.receive(max_bytes)\n\n    async def send(self, item: bytes) -> None:\n        await self.send_stream.send(item)\n\n    async def send_eof(self) -> None:\n        await self.send_stream.aclose()\n\n    async def aclose(self) -> None:\n        await self.send_stream.aclose()\n        await self.receive_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self.send_stream.extra_attributes,\n            **self.receive_stream.extra_attributes,\n        }\n\n\n@dataclass(eq=False)\nclass StapledObjectStream(Generic[T_Item], ObjectStream[T_Item]):\n    \"\"\"\n    Combines two object streams into a single, bidirectional object stream.\n\n    Extra attributes will be provided from both streams, with the receive stream\n    providing the values in case of a conflict.\n\n    :param ObjectSendStream send_stream: the sending object stream\n    :param ObjectReceiveStream receive_stream: the receiving object stream\n    \"\"\"\n\n    send_stream: ObjectSendStream[T_Item]\n    receive_stream: ObjectReceiveStream[T_Item]\n\n    async def receive(self) -> T_Item:\n        return await self.receive_stream.receive()\n\n    async def send(self, item: T_Item) -> None:\n        await self.send_stream.send(item)\n\n    async def send_eof(self) -> None:\n        await self.send_stream.aclose()\n\n    async def aclose(self) -> None:\n        await self.send_stream.aclose()\n        await self.receive_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self.send_stream.extra_attributes,\n            **self.receive_stream.extra_attributes,\n        }\n\n\n@dataclass(eq=False)\nclass MultiListener(Generic[T_Stream], Listener[T_Stream]):\n    \"\"\"\n    Combines multiple listeners into one, serving connections from all of them at once.\n\n    Any MultiListeners in the given collection of listeners will have their listeners\n    moved into this one.\n\n    Extra attributes are provided from each listener, with each successive listener\n    overriding any conflicting attributes from the previous one.\n\n    :param listeners: listeners to serve\n    :type listeners: Sequence[Listener[T_Stream]]\n    \"\"\"\n\n    listeners: Sequence[Listener[T_Stream]]\n\n    def __post_init__(self) -> None:\n        listeners: list[Listener[T_Stream]] = []\n        for listener in self.listeners:\n            if isinstance(listener, MultiListener):\n                listeners.extend(listener.listeners)\n                del listener.listeners[:]  # type: ignore[attr-defined]\n            else:\n                listeners.append(listener)\n\n        self.listeners = listeners\n\n    async def serve(\n        self, handler: Callable[[T_Stream], Any], task_group: TaskGroup | None = None\n    ) -> None:\n        from .. import create_task_group\n\n        async with create_task_group() as tg:\n            for listener in self.listeners:\n                tg.start_soon(listener.serve, handler, task_group)\n\n    async def aclose(self) -> None:\n        for listener in self.listeners:\n            await listener.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        attributes: dict = {}\n        for listener in self.listeners:\n            attributes.update(listener.extra_attributes)\n\n        return attributes\n", "src/anyio/streams/text.py": "from __future__ import annotations\n\nimport codecs\nfrom collections.abc import Callable, Mapping\nfrom dataclasses import InitVar, dataclass, field\nfrom typing import Any\n\nfrom ..abc import (\n    AnyByteReceiveStream,\n    AnyByteSendStream,\n    AnyByteStream,\n    ObjectReceiveStream,\n    ObjectSendStream,\n    ObjectStream,\n)\n\n\n@dataclass(eq=False)\nclass TextReceiveStream(ObjectReceiveStream[str]):\n    \"\"\"\n    Stream wrapper that decodes bytes to strings using the given encoding.\n\n    Decoding is done using :class:`~codecs.IncrementalDecoder` which returns any\n    completely received unicode characters as soon as they come in.\n\n    :param transport_stream: any bytes-based receive stream\n    :param encoding: character encoding to use for decoding bytes to strings (defaults\n        to ``utf-8``)\n    :param errors: handling scheme for decoding errors (defaults to ``strict``; see the\n        `codecs module documentation`_ for a comprehensive list of options)\n\n    .. _codecs module documentation:\n        https://docs.python.org/3/library/codecs.html#codec-objects\n    \"\"\"\n\n    transport_stream: AnyByteReceiveStream\n    encoding: InitVar[str] = \"utf-8\"\n    errors: InitVar[str] = \"strict\"\n    _decoder: codecs.IncrementalDecoder = field(init=False)\n\n    def __post_init__(self, encoding: str, errors: str) -> None:\n        decoder_class = codecs.getincrementaldecoder(encoding)\n        self._decoder = decoder_class(errors=errors)\n\n    async def receive(self) -> str:\n        while True:\n            chunk = await self.transport_stream.receive()\n            decoded = self._decoder.decode(chunk)\n            if decoded:\n                return decoded\n\n    async def aclose(self) -> None:\n        await self.transport_stream.aclose()\n        self._decoder.reset()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return self.transport_stream.extra_attributes\n\n\n@dataclass(eq=False)\nclass TextSendStream(ObjectSendStream[str]):\n    \"\"\"\n    Sends strings to the wrapped stream as bytes using the given encoding.\n\n    :param AnyByteSendStream transport_stream: any bytes-based send stream\n    :param str encoding: character encoding to use for encoding strings to bytes\n        (defaults to ``utf-8``)\n    :param str errors: handling scheme for encoding errors (defaults to ``strict``; see\n        the `codecs module documentation`_ for a comprehensive list of options)\n\n    .. _codecs module documentation:\n        https://docs.python.org/3/library/codecs.html#codec-objects\n    \"\"\"\n\n    transport_stream: AnyByteSendStream\n    encoding: InitVar[str] = \"utf-8\"\n    errors: str = \"strict\"\n    _encoder: Callable[..., tuple[bytes, int]] = field(init=False)\n\n    def __post_init__(self, encoding: str) -> None:\n        self._encoder = codecs.getencoder(encoding)\n\n    async def send(self, item: str) -> None:\n        encoded = self._encoder(item, self.errors)[0]\n        await self.transport_stream.send(encoded)\n\n    async def aclose(self) -> None:\n        await self.transport_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return self.transport_stream.extra_attributes\n\n\n@dataclass(eq=False)\nclass TextStream(ObjectStream[str]):\n    \"\"\"\n    A bidirectional stream that decodes bytes to strings on receive and encodes strings\n    to bytes on send.\n\n    Extra attributes will be provided from both streams, with the receive stream\n    providing the values in case of a conflict.\n\n    :param AnyByteStream transport_stream: any bytes-based stream\n    :param str encoding: character encoding to use for encoding/decoding strings to/from\n        bytes (defaults to ``utf-8``)\n    :param str errors: handling scheme for encoding errors (defaults to ``strict``; see\n        the `codecs module documentation`_ for a comprehensive list of options)\n\n    .. _codecs module documentation:\n        https://docs.python.org/3/library/codecs.html#codec-objects\n    \"\"\"\n\n    transport_stream: AnyByteStream\n    encoding: InitVar[str] = \"utf-8\"\n    errors: InitVar[str] = \"strict\"\n    _receive_stream: TextReceiveStream = field(init=False)\n    _send_stream: TextSendStream = field(init=False)\n\n    def __post_init__(self, encoding: str, errors: str) -> None:\n        self._receive_stream = TextReceiveStream(\n            self.transport_stream, encoding=encoding, errors=errors\n        )\n        self._send_stream = TextSendStream(\n            self.transport_stream, encoding=encoding, errors=errors\n        )\n\n    async def receive(self) -> str:\n        return await self._receive_stream.receive()\n\n    async def send(self, item: str) -> None:\n        await self._send_stream.send(item)\n\n    async def send_eof(self) -> None:\n        await self.transport_stream.send_eof()\n\n    async def aclose(self) -> None:\n        await self._send_stream.aclose()\n        await self._receive_stream.aclose()\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return {\n            **self._send_stream.extra_attributes,\n            **self._receive_stream.extra_attributes,\n        }\n", "src/anyio/streams/buffered.py": "from __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nfrom dataclasses import dataclass, field\nfrom typing import Any\n\nfrom .. import ClosedResourceError, DelimiterNotFound, EndOfStream, IncompleteRead\nfrom ..abc import AnyByteReceiveStream, ByteReceiveStream\n\n\n@dataclass(eq=False)\nclass BufferedByteReceiveStream(ByteReceiveStream):\n    \"\"\"\n    Wraps any bytes-based receive stream and uses a buffer to provide sophisticated\n    receiving capabilities in the form of a byte stream.\n    \"\"\"\n\n    receive_stream: AnyByteReceiveStream\n    _buffer: bytearray = field(init=False, default_factory=bytearray)\n    _closed: bool = field(init=False, default=False)\n\n    async def aclose(self) -> None:\n        await self.receive_stream.aclose()\n        self._closed = True\n\n    @property\n    def buffer(self) -> bytes:\n        \"\"\"The bytes currently in the buffer.\"\"\"\n        return bytes(self._buffer)\n\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        return self.receive_stream.extra_attributes\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        if self._closed:\n            raise ClosedResourceError\n\n        if self._buffer:\n            chunk = bytes(self._buffer[:max_bytes])\n            del self._buffer[:max_bytes]\n            return chunk\n        elif isinstance(self.receive_stream, ByteReceiveStream):\n            return await self.receive_stream.receive(max_bytes)\n        else:\n            # With a bytes-oriented object stream, we need to handle any surplus bytes\n            # we get from the receive() call\n            chunk = await self.receive_stream.receive()\n            if len(chunk) > max_bytes:\n                # Save the surplus bytes in the buffer\n                self._buffer.extend(chunk[max_bytes:])\n                return chunk[:max_bytes]\n            else:\n                return chunk\n\n    async def receive_exactly(self, nbytes: int) -> bytes:\n        \"\"\"\n        Read exactly the given amount of bytes from the stream.\n\n        :param nbytes: the number of bytes to read\n        :return: the bytes read\n        :raises ~anyio.IncompleteRead: if the stream was closed before the requested\n            amount of bytes could be read from the stream\n\n        \"\"\"\n        while True:\n            remaining = nbytes - len(self._buffer)\n            if remaining <= 0:\n                retval = self._buffer[:nbytes]\n                del self._buffer[:nbytes]\n                return bytes(retval)\n\n            try:\n                if isinstance(self.receive_stream, ByteReceiveStream):\n                    chunk = await self.receive_stream.receive(remaining)\n                else:\n                    chunk = await self.receive_stream.receive()\n            except EndOfStream as exc:\n                raise IncompleteRead from exc\n\n            self._buffer.extend(chunk)\n\n    async def receive_until(self, delimiter: bytes, max_bytes: int) -> bytes:\n        \"\"\"\n        Read from the stream until the delimiter is found or max_bytes have been read.\n\n        :param delimiter: the marker to look for in the stream\n        :param max_bytes: maximum number of bytes that will be read before raising\n            :exc:`~anyio.DelimiterNotFound`\n        :return: the bytes read (not including the delimiter)\n        :raises ~anyio.IncompleteRead: if the stream was closed before the delimiter\n            was found\n        :raises ~anyio.DelimiterNotFound: if the delimiter is not found within the\n            bytes read up to the maximum allowed\n\n        \"\"\"\n        delimiter_size = len(delimiter)\n        offset = 0\n        while True:\n            # Check if the delimiter can be found in the current buffer\n            index = self._buffer.find(delimiter, offset)\n            if index >= 0:\n                found = self._buffer[:index]\n                del self._buffer[: index + len(delimiter) :]\n                return bytes(found)\n\n            # Check if the buffer is already at or over the limit\n            if len(self._buffer) >= max_bytes:\n                raise DelimiterNotFound(max_bytes)\n\n            # Read more data into the buffer from the socket\n            try:\n                data = await self.receive_stream.receive()\n            except EndOfStream as exc:\n                raise IncompleteRead from exc\n\n            # Move the offset forward and add the new data to the buffer\n            offset = max(len(self._buffer) - delimiter_size + 1, 0)\n            self._buffer.extend(data)\n"}