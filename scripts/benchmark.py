import json
from tests.abstraction_handling.from_rust_stitch_test import run_compression_for_testing

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")


def output_results(result):
    for i, abstr in enumerate(result.abstractions_python()):
        print("*" * 30 + " " + result.abstractions[i].name + " " + "*" * 30)
        print(abstr)

    with open("out.py", "w") as f:
        f.write("# This file is auto-generated by create.py\n")
        for x in result.rewritten_python(is_pythonm=True):
            f.write("#" + "-" * 80 + "\n")
            f.write(x + "\n\n")


def print_stats(code, result):
    original = sum(len(tokenizer.encode(x)) for x in code)
    rewritten = sum(
        len(tokenizer.encode(x)) for x in result.rewritten_python(is_pythonm=True)
    )
    abstractions = sum(
        len(tokenizer.encode(x)) for x in result.abstractions_python(is_pythonm=True)
    )
    print(f"Original: {original} tokens")
    print(f"Rewritten: {rewritten} tokens ({rewritten/original:.2%} of original)")
    print(
        f"Abstractions: {abstractions} tokens ({abstractions/original:.2%} of original)"
    )
    compression_amount = original - (rewritten + abstractions)
    print(
        f"Overall compression: {compression_amount}/{original} tokens: {compression_amount / original:.2%}"
    )


def run_experiment(path):
    with open(f"data/vlmaterial-set/{path}.json", "r") as f:
        code = json.load(f)

    result = run_compression_for_testing(
        code,
        iterations=100,
        max_arity=4,
        silent=False,
        verbose_best=True,
        use_symvars=False,
    )

    cost_fn = lambda x: len(tokenizer.encode(x))

    result = result.inline_multiline_calls()
    result = result.remove_unhelpful_abstractions(is_pythonm=True, cost_fn=cost_fn)

    output_results(result)
    print_stats(code, result)


run_experiment("human_1000")
# Original: 807057 tokens
# Rewritten: 481650 tokens (59.68% of original)
# Abstractions: 2559 tokens (0.32% of original)
# Overall compression: 322848/807057 tokens: 40.00%
